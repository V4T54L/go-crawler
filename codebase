{
  "Dockerfile": "# Stage 1: Build the application\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Copy go.mod and go.sum files to download dependencies\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy the source code\nCOPY . .\n\n# Build the application\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /app/crawler-service ./cmd/api\n\n# Stage 2: Create the final, minimal image\nFROM alpine:latest\n\nWORKDIR /root/\n\n# Copy the binary from the builder stage\nCOPY --from=builder /app/crawler-service .\n\n# Expose port (will be used by the API later)\nEXPOSE 8080\n\n# Command to run the executable\nCMD [\"./crawler-service\"]\n",
  "Makefile": ".PHONY: build run tidy\n\nbuild:\n\t@echo \"Building binary...\"\n\t@go build -o ./bin/crawler-service ./cmd/api\n\nrun:\n\t@echo \"Running application...\"\n\t@go run ./cmd/api\n\ntidy:\n\t@echo \"Running go mod tidy...\"\n\t@go mod tidy\n",
  "cmd/api/main.go": "package main\n\nimport (\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/user/crawler-service/pkg/logger\"\n)\n\nfunc main() {\n\tlogger.Init(os.Stdout, slog.LevelInfo)\n\tslog.Info(\"Starting crawler service...\")\n\t// TODO: Initialize config, database, redis, and start the server\n}\n",
  "go.mod": "module github.com/user/crawler-service\n\ngo 1.21\n",
  "internal/entity/extracted_data.go": "package entity\n\nimport \"time\"\n\n// ImageInfo represents the structured data for an image extracted from a page.\ntype ImageInfo struct {\n\tSrc     string `json:\"src\"`\n\tAlt     string `json:\"alt\"`\n\tDataSrc string `json:\"data_src,omitempty\"` // For lazy-loaded images\n}\n\n// ExtractedData mirrors the `extracted_data` PostgreSQL table schema.\ntype ExtractedData struct {\n\tID               int64\n\tURL              string\n\tTitle            string\n\tDescription      string\n\tKeywords         []string\n\tH1Tags           []string\n\tContent          string\n\tImages           []ImageInfo // Stored as JSONB in PostgreSQL\n\tCrawlTimestamp   time.Time\n\tHTTPStatusCode   int\n\tResponseTimeMS   int\n}\n",
  "internal/entity/failed_url.go": "package entity\n\nimport \"time\"\n\n// FailedURL mirrors the `failed_urls` PostgreSQL table schema.\ntype FailedURL struct {\n\tID                   int64\n\tURL                  string\n\tFailureReason        string\n\tHTTPStatusCode       int\n\tLastAttemptTimestamp time.Time\n\tRetryCount           int\n\tNextRetryAt          time.Time\n}\n",
  "pkg/config/config.go": "package config\n\nimport (\n\t\"log\"\n\t\"os\"\n\t\"strconv\"\n\t\"time\"\n)\n\n// Config holds the application configuration.\ntype Config struct {\n\tServerPort string\n\tLogLevel   string\n\n\tPostgresHost     string\n\tPostgresPort     string\n\tPostgresUser     string\n\tPostgresPassword string\n\tPostgresDB       string\n\n\tRedisAddr     string\n\tRedisPassword string\n\tRedisDB       int\n\n\tMaxConcurrency int\n\tPageLoadTimeout time.Duration\n}\n\n// Load loads configuration from environment variables.\nfunc Load() *Config {\n\treturn \u0026Config{\n\t\tServerPort:       getEnv(\"SERVER_PORT\", \"8080\"),\n\t\tLogLevel:         getEnv(\"LOG_LEVEL\", \"info\"),\n\t\tPostgresHost:     getEnv(\"POSTGRES_HOST\", \"localhost\"),\n\t\tPostgresPort:     getEnv(\"POSTGRES_PORT\", \"5432\"),\n\t\tPostgresUser:     getEnv(\"POSTGRES_USER\", \"user\"),\n\t\tPostgresPassword: getEnv(\"POSTGRES_PASSWORD\", \"password\"),\n\t\tPostgresDB:       getEnv(\"POSTGRES_DB\", \"crawler\"),\n\t\tRedisAddr:        getEnv(\"REDIS_ADDR\", \"localhost:6379\"),\n\t\tRedisPassword:    getEnv(\"REDIS_PASSWORD\", \"\"),\n\t\tRedisDB:          getEnvAsInt(\"REDIS_DB\", 0),\n\t\tMaxConcurrency:   getEnvAsInt(\"MAX_CONCURRENCY\", 10),\n\t\tPageLoadTimeout:  getEnvAsDuration(\"PAGE_LOAD_TIMEOUT_SECONDS\", 60) * time.Second,\n\t}\n}\n\nfunc getEnv(key, fallback string) string {\n\tif value, ok := os.LookupEnv(key); ok {\n\t\treturn value\n\t}\n\treturn fallback\n}\n\nfunc getEnvAsInt(key string, fallback int) int {\n\tvalueStr := getEnv(key, \"\")\n\tif value, err := strconv.Atoi(valueStr); err == nil {\n\t\treturn value\n\t}\n\treturn fallback\n}\n\nfunc getEnvAsDuration(key string, fallback int) time.Duration {\n\treturn time.Duration(getEnvAsInt(key, fallback))\n}\n",
  "pkg/logger/logger.go": "package logger\n\nimport (\n\t\"io\"\n\t\"log/slog\"\n)\n\n// Init initializes the global slog logger.\nfunc Init(writer io.Writer, level slog.Level) {\n\thandler := slog.NewJSONHandler(writer, \u0026slog.HandlerOptions{\n\t\tLevel: level,\n\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\t// Customize attribute keys for consistency if needed\n\t\t\tif a.Key == slog.TimeKey {\n\t\t\t\ta.Key = \"timestamp\"\n\t\t\t}\n\t\t\tif a.Key == slog.LevelKey {\n\t\t\t\ta.Key = \"level\"\n\t\t\t}\n\t\t\tif a.Key == slog.MessageKey {\n\t\t\t\ta.Key = \"message\"\n\t\t\t}\n\t\t\treturn a\n\t\t},\n\t})\n\tlogger := slog.New(handler)\n\tslog.SetDefault(logger)\n}\n",
  "pkg/utils/url.go": "package utils\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/hex\"\n\t\"net/url\"\n)\n\n// HashURL creates a SHA256 hash of a URL string.\n// This is useful for creating consistent, safe keys for Redis.\nfunc HashURL(rawURL string) string {\n\th := sha256.New()\n\th.Write([]byte(rawURL))\n\treturn hex.EncodeToString(h.Sum(nil))\n}\n\n// ToAbsoluteURL converts a relative URL to an absolute URL given a base URL.\nfunc ToAbsoluteURL(base *url.URL, relative string) (string, error) {\n\trelURL, err := url.Parse(relative)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base.ResolveReference(relURL).String(), nil\n}\n```",
  "schema/001_init.sql": "CREATE TABLE IF NOT EXISTS extracted_data (\n    id BIGSERIAL PRIMARY KEY,\n    url TEXT NOT NULL UNIQUE,\n    title TEXT,\n    description TEXT,\n    keywords TEXT[],\n    h1_tags TEXT[],\n    content TEXT,\n    images JSONB,\n    crawl_timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    http_status_code INT,\n    response_time_ms INT\n);\n\nCREATE INDEX IF NOT EXISTS idx_extracted_data_url ON extracted_data(url);\nCREATE INDEX IF NOT EXISTS idx_extracted_data_crawl_timestamp ON extracted_data(crawl_timestamp);\n\nCREATE TABLE IF NOT EXISTS failed_urls (\n    id BIGSERIAL PRIMARY KEY,\n    url TEXT NOT NULL UNIQUE,\n    failure_reason TEXT,\n    http_status_code INT,\n    last_attempt_timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    retry_count INT NOT NULL DEFAULT 0,\n    next_retry_at TIMESTAMPTZ\n);\n\nCREATE INDEX IF NOT EXISTS idx_failed_urls_url ON failed_urls(url);\nCREATE INDEX IF NOT EXISTS idx_failed_urls_next_retry_at ON failed_urls(next_retry_at);\n\n"
}