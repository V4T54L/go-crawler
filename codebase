{
  "Dockerfile": "# Stage 1: Build the application\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Copy go.mod and go.sum files to download dependencies\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy the source code\nCOPY . .\n\n# Build the application\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /app/crawler-service ./cmd/api\n\n# Stage 2: Create the final, minimal image\nFROM alpine:latest\n\nWORKDIR /root/\n\n# Copy the binary from the builder stage\nCOPY --from=builder /app/crawler-service .\n\n# Expose port (will be used by the API later)\nEXPOSE 8080\n\n# Command to run the executable\nCMD [\"./crawler-service\"]\n",
  "Makefile": ".PHONY: build run tidy\n\nbuild:\n\t@echo \"Building binary...\"\n\t@go build -o ./bin/crawler-service ./cmd/api\n\nrun:\n\t@echo \"Running application...\"\n\t@go run ./cmd/api\n\ntidy:\n\t@echo \"Running go mod tidy...\"\n\t@go mod tidy\n",
  "cmd/api/main.go": "package main\n\nimport (\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/user/crawler-service/pkg/logger\"\n)\n\nfunc main() {\n\tlogger.Init(os.Stdout, slog.LevelInfo)\n\tslog.Info(\"Starting crawler service...\")\n\t// TODO: Initialize config, database, redis, and start the server\n}\n",
  "go.mod": "module github.com/user/crawler-service\n\ngo 1.21\n\nrequire (\n\tgithub.com/jackc/pgx/v5 v5.5.5\n\tgithub.com/redis/go-redis/v9 v9.5.1\n)\n\nrequire (\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect\n\tgithub.com/jackc/pgpassfile v1.0.0 // indirect\n\tgithub.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a // indirect\n\tgithub.com/jackc/puddle/v2 v2.2.1 // indirect\n\tgolang.org/x/crypto v0.17.0 // indirect\n\tgolang.org/x/sync v0.1.0 // indirect\n\tgolang.org/x/text v0.14.0 // indirect\n)",
  "internal/adapter/postgres/extracted_data_impl.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// ExtractedDataRepoImpl provides a concrete implementation for the ExtractedDataRepository interface using PostgreSQL.\ntype ExtractedDataRepoImpl struct {\n\tdb *pgxpool.Pool\n}\n\n// NewExtractedDataRepo creates a new instance of ExtractedDataRepoImpl.\nfunc NewExtractedDataRepo(db *pgxpool.Pool) *ExtractedDataRepoImpl {\n\treturn \u0026ExtractedDataRepoImpl{db: db}\n}\n\n// Save stores or updates the extracted data for a URL in the database.\nfunc (r *ExtractedDataRepoImpl) Save(ctx context.Context, data *entity.ExtractedData) error {\n\timagesJSON, err := json.Marshal(data.Images)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tquery := `\n\t\tINSERT INTO extracted_data (url, title, description, keywords, h1_tags, content, images, http_status_code, response_time_ms, crawl_timestamp)\n\t\tVALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)\n\t\tON CONFLICT (url) DO UPDATE SET\n\t\t\ttitle = EXCLUDED.title,\n\t\t\tdescription = EXCLUDED.description,\n\t\t\tkeywords = EXCLUDED.keywords,\n\t\t\th1_tags = EXCLUDED.h1_tags,\n\t\t\tcontent = EXCLUDED.content,\n\t\t\timages = EXCLUDED.images,\n\t\t\thttp_status_code = EXCLUDED.http_status_code,\n\t\t\tresponse_time_ms = EXCLUDED.response_time_ms,\n\t\t\tcrawl_timestamp = EXCLUDED.crawl_timestamp;\n\t`\n\n\t_, err = r.db.Exec(ctx, query,\n\t\tdata.URL,\n\t\tdata.Title,\n\t\tdata.Description,\n\t\tdata.Keywords,\n\t\tdata.H1Tags,\n\t\tdata.Content,\n\t\timagesJSON,\n\t\tdata.HTTPStatusCode,\n\t\tdata.ResponseTimeMS,\n\t\tdata.CrawlTimestamp,\n\t)\n\treturn err\n}\n\n// FindByURL retrieves the extracted data for a specific URL from the database.\nfunc (r *ExtractedDataRepoImpl) FindByURL(ctx context.Context, url string) (*entity.ExtractedData, error) {\n\tquery := `\n\t\tSELECT id, url, title, description, keywords, h1_tags, content, images, http_status_code, response_time_ms, crawl_timestamp\n\t\tFROM extracted_data\n\t\tWHERE url = $1;\n\t`\n\trow := r.db.QueryRow(ctx, query, url)\n\n\tvar data entity.ExtractedData\n\tvar imagesJSON []byte\n\n\terr := row.Scan(\n\t\t\u0026data.ID,\n\t\t\u0026data.URL,\n\t\t\u0026data.Title,\n\t\t\u0026data.Description,\n\t\t\u0026data.Keywords,\n\t\t\u0026data.H1Tags,\n\t\t\u0026data.Content,\n\t\t\u0026imagesJSON,\n\t\t\u0026data.HTTPStatusCode,\n\t\t\u0026data.ResponseTimeMS,\n\t\t\u0026data.CrawlTimestamp,\n\t)\n\tif err != nil {\n\t\treturn nil, err // pgx.ErrNoRows will be returned if not found\n\t}\n\n\tif err := json.Unmarshal(imagesJSON, \u0026data.Images); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn \u0026data, nil\n}\n",
  "internal/adapter/postgres/failed_url_impl.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// FailedURLRepoImpl provides a concrete implementation for the FailedURLRepository interface using PostgreSQL.\ntype FailedURLRepoImpl struct {\n\tdb *pgxpool.Pool\n}\n\n// NewFailedURLRepo creates a new instance of FailedURLRepoImpl.\nfunc NewFailedURLRepo(db *pgxpool.Pool) *FailedURLRepoImpl {\n\treturn \u0026FailedURLRepoImpl{db: db}\n}\n\n// SaveOrUpdate creates or updates a record for a failed URL.\n// It increments the retry_count on conflict.\nfunc (r *FailedURLRepoImpl) SaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error {\n\tquery := `\n\t\tINSERT INTO failed_urls (url, failure_reason, http_status_code, last_attempt_timestamp, retry_count, next_retry_at)\n\t\tVALUES ($1, $2, $3, $4, 1, $5)\n\t\tON CONFLICT (url) DO UPDATE SET\n\t\t\tfailure_reason = EXCLUDED.failure_reason,\n\t\t\thttp_status_code = EXCLUDED.http_status_code,\n\t\t\tlast_attempt_timestamp = EXCLUDED.last_attempt_timestamp,\n\t\t\tretry_count = failed_urls.retry_count + 1,\n\t\t\tnext_retry_at = EXCLUDED.next_retry_at;\n\t`\n\t_, err := r.db.Exec(ctx, query,\n\t\tfailedURL.URL,\n\t\tfailedURL.FailureReason,\n\t\tfailedURL.HTTPStatusCode,\n\t\tfailedURL.LastAttemptTimestamp,\n\t\tfailedURL.NextRetryAt,\n\t)\n\treturn err\n}\n\n// FindRetryable retrieves a batch of URLs that are due for a retry.\nfunc (r *FailedURLRepoImpl) FindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error) {\n\tquery := `\n\t\tSELECT id, url, failure_reason, http_status_code, last_attempt_timestamp, retry_count, next_retry_at\n\t\tFROM failed_urls\n\t\tWHERE next_retry_at \u003c= NOW()\n\t\tORDER BY next_retry_at ASC\n\t\tLIMIT $1;\n\t`\n\trows, err := r.db.Query(ctx, query, limit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\n\tvar failedURLs []*entity.FailedURL\n\tfor rows.Next() {\n\t\tvar fu entity.FailedURL\n\t\tif err := rows.Scan(\n\t\t\t\u0026fu.ID,\n\t\t\t\u0026fu.URL,\n\t\t\t\u0026fu.FailureReason,\n\t\t\t\u0026fu.HTTPStatusCode,\n\t\t\t\u0026fu.LastAttemptTimestamp,\n\t\t\t\u0026fu.RetryCount,\n\t\t\t\u0026fu.NextRetryAt,\n\t\t); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfailedURLs = append(failedURLs, \u0026fu)\n\t}\n\n\treturn failedURLs, rows.Err()\n}\n\n// Delete removes a failed URL record, typically after a successful crawl.\nfunc (r *FailedURLRepoImpl) Delete(ctx context.Context, url string) error {\n\tquery := `DELETE FROM failed_urls WHERE url = $1;`\n\t_, err := r.db.Exec(ctx, query, url)\n\treturn err\n}\n",
  "internal/adapter/redis/queue_impl.go": "package redis\n\nimport (\n\t\"context\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\nconst crawlQueueKey = \"crawler:queue\"\n\n// QueueRepoImpl provides a concrete implementation for the QueueRepository interface using Redis Lists.\ntype QueueRepoImpl struct {\n\tclient *redis.Client\n}\n\n// NewQueueRepo creates a new instance of QueueRepoImpl.\nfunc NewQueueRepo(client *redis.Client) *QueueRepoImpl {\n\treturn \u0026QueueRepoImpl{client: client}\n}\n\n// Push adds a URL to the left side of the Redis list (acting as a queue).\nfunc (r *QueueRepoImpl) Push(ctx context.Context, url string) error {\n\treturn r.client.LPush(ctx, crawlQueueKey, url).Err()\n}\n\n// Pop removes and returns a URL from the right side of the Redis list (acting as a queue).\n// It is a blocking operation if the list is empty, but we can add a timeout.\n// For simplicity, we use RPop which returns redis.Nil error if empty.\nfunc (r *QueueRepoImpl) Pop(ctx context.Context) (string, error) {\n\treturn r.client.RPop(ctx, crawlQueueKey).Result()\n}\n\n// Size returns the current number of items in the queue.\nfunc (r *QueueRepoImpl) Size(ctx context.Context) (int64, error) {\n\treturn r.client.LLen(ctx, crawlQueueKey).Result()\n}\n",
  "internal/adapter/redis/visited_impl.go": "package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/crawler-service/pkg/utils\"\n\t\"time\"\n)\n\nconst visitedURLPrefix = \"visited:\"\n\n// VisitedRepoImpl provides a concrete implementation for the VisitedRepository interface using Redis.\ntype VisitedRepoImpl struct {\n\tclient *redis.Client\n}\n\n// NewVisitedRepo creates a new instance of VisitedRepoImpl.\nfunc NewVisitedRepo(client *redis.Client) *VisitedRepoImpl {\n\treturn \u0026VisitedRepoImpl{client: client}\n}\n\n// generateKey creates a consistent Redis key for a given URL by hashing it.\nfunc (r *VisitedRepoImpl) generateKey(url string) string {\n\treturn fmt.Sprintf(\"%s%s\", visitedURLPrefix, utils.HashURL(url))\n}\n\n// MarkVisited marks a URL as visited by setting a key in Redis with a specific expiry time.\nfunc (r *VisitedRepoImpl) MarkVisited(ctx context.Context, url string, expiry time.Duration) error {\n\tkey := r.generateKey(url)\n\t// SETEX is atomic and sets the key with an expiry.\n\treturn r.client.SetEX(ctx, key, \"1\", expiry).Err()\n}\n\n// IsVisited checks if a URL has been visited recently by checking for the existence of its key.\nfunc (r *VisitedRepoImpl) IsVisited(ctx context.Context, url string) (bool, error) {\n\tkey := r.generateKey(url)\n\t// EXISTS returns 1 if the key exists, 0 otherwise.\n\tval, err := r.client.Exists(ctx, key).Result()\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn val == 1, nil\n}\n\n// RemoveVisited removes a URL from the visited set, used for force_crawl.\nfunc (r *VisitedRepoImpl) RemoveVisited(ctx context.Context, url string) error {\n\tkey := r.generateKey(url)\n\t// DEL removes the key.\n\treturn r.client.Del(ctx, key).Err()\n}\n",
  "internal/entity/extracted_data.go": "package entity\n\nimport \"time\"\n\n// ImageInfo represents the structured data for an image extracted from a page.\ntype ImageInfo struct {\n\tSrc     string `json:\"src\"`\n\tAlt     string `json:\"alt\"`\n\tDataSrc string `json:\"data_src,omitempty\"` // For lazy-loaded images\n}\n\n// ExtractedData mirrors the `extracted_data` PostgreSQL table schema.\ntype ExtractedData struct {\n\tID               int64\n\tURL              string\n\tTitle            string\n\tDescription      string\n\tKeywords         []string\n\tH1Tags           []string\n\tContent          string\n\tImages           []ImageInfo // Stored as JSONB in PostgreSQL\n\tCrawlTimestamp   time.Time\n\tHTTPStatusCode   int\n\tResponseTimeMS   int\n}\n",
  "internal/entity/failed_url.go": "package entity\n\nimport \"time\"\n\n// FailedURL mirrors the `failed_urls` PostgreSQL table schema.\ntype FailedURL struct {\n\tID                   int64\n\tURL                  string\n\tFailureReason        string\n\tHTTPStatusCode       int\n\tLastAttemptTimestamp time.Time\n\tRetryCount           int\n\tNextRetryAt          time.Time\n}\n",
  "internal/repository/extracted_data_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// ExtractedDataRepository defines the interface for storing and retrieving extracted page data.\ntype ExtractedDataRepository interface {\n\t// Save stores the extracted data for a URL. If the URL already exists, it should be updated.\n\tSave(ctx context.Context, data *entity.ExtractedData) error\n\t// FindByURL retrieves the extracted data for a specific URL.\n\tFindByURL(ctx context.Context, url string) (*entity.ExtractedData, error)\n}\n",
  "internal/repository/failed_url_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// FailedURLRepository defines the interface for managing URLs that failed to be crawled.\ntype FailedURLRepository interface {\n\t// SaveOrUpdate creates or updates a record for a failed URL.\n\tSaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error\n\t// FindRetryable retrieves a batch of URLs that are due for a retry.\n\tFindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error)\n\t// Delete removes a failed URL record, typically after a successful crawl.\n\tDelete(ctx context.Context, url string) error\n}\n",
  "internal/repository/queue_repo.go": "package repository\n\nimport \"context\"\n\n// QueueRepository defines the interface for a FIFO queue for URLs to be crawled.\ntype QueueRepository interface {\n\t// Push adds a URL to the end of the queue.\n\tPush(ctx context.Context, url string) error\n\t// Pop removes and returns a URL from the front of the queue.\n\tPop(ctx context.Context) (string, error)\n\t// Size returns the current number of items in the queue.\n\tSize(ctx context.Context) (int64, error)\n}\n",
  "internal/repository/visited_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"time\"\n)\n\n// VisitedRepository defines the interface for deduplication of visited URLs.\ntype VisitedRepository interface {\n\t// MarkVisited marks a URL as visited with a specific expiry time.\n\tMarkVisited(ctx context.Context, url string, expiry time.Duration) error\n\t// IsVisited checks if a URL has been visited recently.\n\tIsVisited(ctx context.Context, url string) (bool, error)\n\t// RemoveVisited removes a URL from the visited set, used for force_crawl.\n\tRemoveVisited(ctx context.Context, url string) error\n}\n```",
  "pkg/config/config.go": "package config\n\nimport (\n\t\"log\"\n\t\"os\"\n\t\"strconv\"\n\t\"time\"\n)\n\n// Config holds the application configuration.\ntype Config struct {\n\tServerPort string\n\tLogLevel   string\n\n\tPostgresHost     string\n\tPostgresPort     string\n\tPostgresUser     string\n\tPostgresPassword string\n\tPostgresDB       string\n\n\tRedisAddr     string\n\tRedisPassword string\n\tRedisDB       int\n\n\tMaxConcurrency int\n\tPageLoadTimeout time.Duration\n}\n\n// Load loads configuration from environment variables.\nfunc Load() *Config {\n\treturn \u0026Config{\n\t\tServerPort:       getEnv(\"SERVER_PORT\", \"8080\"),\n\t\tLogLevel:         getEnv(\"LOG_LEVEL\", \"info\"),\n\t\tPostgresHost:     getEnv(\"POSTGRES_HOST\", \"localhost\"),\n\t\tPostgresPort:     getEnv(\"POSTGRES_PORT\", \"5432\"),\n\t\tPostgresUser:     getEnv(\"POSTGRES_USER\", \"user\"),\n\t\tPostgresPassword: getEnv(\"POSTGRES_PASSWORD\", \"password\"),\n\t\tPostgresDB:       getEnv(\"POSTGRES_DB\", \"crawler\"),\n\t\tRedisAddr:        getEnv(\"REDIS_ADDR\", \"localhost:6379\"),\n\t\tRedisPassword:    getEnv(\"REDIS_PASSWORD\", \"\"),\n\t\tRedisDB:          getEnvAsInt(\"REDIS_DB\", 0),\n\t\tMaxConcurrency:   getEnvAsInt(\"MAX_CONCURRENCY\", 10),\n\t\tPageLoadTimeout:  getEnvAsDuration(\"PAGE_LOAD_TIMEOUT_SECONDS\", 60) * time.Second,\n\t}\n}\n\nfunc getEnv(key, fallback string) string {\n\tif value, ok := os.LookupEnv(key); ok {\n\t\treturn value\n\t}\n\treturn fallback\n}\n\nfunc getEnvAsInt(key string, fallback int) int {\n\tvalueStr := getEnv(key, \"\")\n\tif value, err := strconv.Atoi(valueStr); err == nil {\n\t\treturn value\n\t}\n\treturn fallback\n}\n\nfunc getEnvAsDuration(key string, fallback int) time.Duration {\n\treturn time.Duration(getEnvAsInt(key, fallback))\n}\n",
  "pkg/logger/logger.go": "package logger\n\nimport (\n\t\"io\"\n\t\"log/slog\"\n)\n\n// Init initializes the global slog logger.\nfunc Init(writer io.Writer, level slog.Level) {\n\thandler := slog.NewJSONHandler(writer, \u0026slog.HandlerOptions{\n\t\tLevel: level,\n\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\t// Customize attribute keys for consistency if needed\n\t\t\tif a.Key == slog.TimeKey {\n\t\t\t\ta.Key = \"timestamp\"\n\t\t\t}\n\t\t\tif a.Key == slog.LevelKey {\n\t\t\t\ta.Key = \"level\"\n\t\t\t}\n\t\t\tif a.Key == slog.MessageKey {\n\t\t\t\ta.Key = \"message\"\n\t\t\t}\n\t\t\treturn a\n\t\t},\n\t})\n\tlogger := slog.New(handler)\n\tslog.SetDefault(logger)\n}\n",
  "pkg/utils/url.go": "package utils\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/hex\"\n\t\"net/url\"\n)\n\n// HashURL creates a SHA256 hash of a URL string.\n// This is useful for creating consistent, safe keys for Redis.\nfunc HashURL(rawURL string) string {\n\th := sha256.New()\n\th.Write([]byte(rawURL))\n\treturn hex.EncodeToString(h.Sum(nil))\n}\n\n// ToAbsoluteURL converts a relative URL to an absolute URL given a base URL.\nfunc ToAbsoluteURL(base *url.URL, relative string) (string, error) {\n\trelURL, err := url.Parse(relative)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base.ResolveReference(relURL).String(), nil\n}\n```",
  "schema/001_init.sql": "CREATE TABLE IF NOT EXISTS extracted_data (\n    id BIGSERIAL PRIMARY KEY,\n    url TEXT NOT NULL UNIQUE,\n    title TEXT,\n    description TEXT,\n    keywords TEXT[],\n    h1_tags TEXT[],\n    content TEXT,\n    images JSONB,\n    crawl_timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    http_status_code INT,\n    response_time_ms INT\n);\n\nCREATE INDEX IF NOT EXISTS idx_extracted_data_url ON extracted_data(url);\nCREATE INDEX IF NOT EXISTS idx_extracted_data_crawl_timestamp ON extracted_data(crawl_timestamp);\n\nCREATE TABLE IF NOT EXISTS failed_urls (\n    id BIGSERIAL PRIMARY KEY,\n    url TEXT NOT NULL UNIQUE,\n    failure_reason TEXT,\n    http_status_code INT,\n    last_attempt_timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    retry_count INT NOT NULL DEFAULT 0,\n    next_retry_at TIMESTAMPTZ\n);\n\nCREATE INDEX IF NOT EXISTS idx_failed_urls_url ON failed_urls(url);\nCREATE INDEX IF NOT EXISTS idx_failed_urls_next_retry_at ON failed_urls(next_retry_at);\n\n"
}