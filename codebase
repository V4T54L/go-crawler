{
  "Dockerfile": "# Stage 1: Build the application\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Copy go.mod and go.sum files to download dependencies\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy the source code\nCOPY . .\n\n# Build the application\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /app/crawler-service ./cmd/api\n\n# Stage 2: Create the final, minimal image\nFROM alpine:latest\n\nWORKDIR /root/\n\n# Copy the binary from the builder stage\nCOPY --from=builder /app/crawler-service .\n\n# Expose port (will be used by the API later)\nEXPOSE 8080\n\n# Command to run the executable\nCMD [\"./crawler-service\"]\n",
  "Makefile": ".PHONY: build run tidy\n\nbuild:\n\t@echo \"Building binary...\"\n\t@go build -o ./bin/crawler-service ./cmd/api\n\nrun:\n\t@echo \"Running application...\"\n\t@go run ./cmd/api\n\ntidy:\n\t@echo \"Running go mod tidy...\"\n\t@go mod tidy\n",
  "cmd/api/main.go": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/redis/go-redis/v9\"\n\n\t\"github.com/user/crawler-service/internal/adapter/chromedp_crawler\"\n\tpostgres_adapter \"github.com/user/crawler-service/internal/adapter/postgres\"\n\tredis_adapter \"github.com/user/crawler-service/internal/adapter/redis\"\n\t\"github.com/user/crawler-service/internal/delivery/http/handler\"\n\t\"github.com/user/crawler-service/internal/delivery/http/router\"\n\t\"github.com/user/crawler-service/internal/usecase\"\n\t\"github.com/user/crawler-service/pkg/config\"\n\t\"github.com/user/crawler-service/pkg/logger\"\n\t\"github.com/user/crawler-service/pkg/metrics\"\n)\n\nfunc main() {\n\t// --- Configuration ---\n\tcfg := config.Load()\n\n\t// --- Logger ---\n\tlogLevel := slog.LevelInfo\n\tif cfg.LogLevel == \"debug\" {\n\t\tlogLevel = slog.LevelDebug\n\t}\n\tlogger.Init(os.Stdout, logLevel)\n\tslog.Info(\"Logger initialized\", \"level\", logLevel.String())\n\n\t// --- Metrics ---\n\tmetrics.Init()\n\tslog.Info(\"Metrics initialized\")\n\n\t// --- Database Connections ---\n\tdbURL := fmt.Sprintf(\"postgres://%s:%s@%s:%s/%s?sslmode=disable\",\n\t\tcfg.PostgresUser, cfg.PostgresPassword, cfg.PostgresHost, cfg.PostgresPort, cfg.PostgresDB)\n\tdbPool, err := pgxpool.New(context.Background(), dbURL)\n\tif err != nil {\n\t\tslog.Error(\"Unable to connect to database\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer dbPool.Close()\n\tslog.Info(\"Successfully connected to PostgreSQL\")\n\n\tredisClient := redis.NewClient(\u0026redis.Options{\n\t\tAddr:     cfg.RedisAddr,\n\t\tPassword: cfg.RedisPassword,\n\t\tDB:       cfg.RedisDB,\n\t})\n\tif _, err := redisClient.Ping(context.Background()).Result(); err != nil {\n\t\tslog.Error(\"Unable to connect to Redis\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tslog.Info(\"Successfully connected to Redis\")\n\n\t// --- Repositories ---\n\tvisitedRepo := redis_adapter.NewVisitedRepo(redisClient)\n\tqueueRepo := redis_adapter.NewQueueRepo(redisClient)\n\textractedDataRepo := postgres_adapter.NewExtractedDataRepo(dbPool)\n\tfailedURLRepo := postgres_adapter.NewFailedURLRepo(dbPool)\n\n\t// Initialize Crawler Repository\n\tcrawlerRepo, err := chromedp_crawler.NewChromedpCrawler(cfg.MaxConcurrency, cfg.PageLoadTimeout)\n\tif err != nil {\n\t\tslog.Error(\"Failed to initialize Chromedp Crawler\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tslog.Info(\"Chromedp crawler initialized\")\n\n\t// --- Use Cases ---\n\turlManager := usecase.NewURLManager(visitedRepo, queueRepo, extractedDataRepo, failedURLRepo)\n\n\t// Initialize Crawler Use Case\n\t// Note: We are not starting the crawler worker here. This is just setting up the components.\n\t// A future step will introduce a worker that calls crawlerUseCase.ProcessURLFromQueue in a loop.\n\t_ = usecase.NewCrawlerUseCase(queueRepo, crawlerRepo, extractedDataRepo, failedURLRepo)\n\tslog.Info(\"Crawler use case initialized\")\n\n\t// --- HTTP Server ---\n\tapiHandler := handler.NewHandler(urlManager)\n\thttpRouter := router.New(apiHandler)\n\n\t// Add Prometheus metrics handler\n\thttp.Handle(\"/metrics\", promhttp.Handler())\n\thttp.Handle(\"/\", httpRouter)\n\n\tserver := \u0026http.Server{\n\t\tAddr:         \":\" + cfg.ServerPort,\n\t\tHandler:      http.DefaultServeMux, // Use DefaultServeMux to handle both router and metrics\n\t\tReadTimeout:  5 * time.Second,\n\t\tWriteTimeout: 10 * time.Second,\n\t\tIdleTimeout:  120 * time.Second,\n\t}\n\n\t// --- Graceful Shutdown ---\n\tstop := make(chan os.Signal, 1)\n\tsignal.Notify(stop, os.Interrupt, syscall.SIGTERM)\n\n\tgo func() {\n\t\tslog.Info(\"Server is starting\", \"port\", cfg.ServerPort)\n\t\tif err := server.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed {\n\t\t\tslog.Error(\"Server failed to start\", \"error\", err)\n\t\t\tos.Exit(1)\n\t\t}\n\t}()\n\n\t\u003c-stop\n\tslog.Info(\"Shutting down server...\")\n\n\tctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n\tdefer cancel()\n\n\tif err := server.Shutdown(ctx); err != nil {\n\t\tslog.Error(\"Server shutdown failed\", \"error\", err)\n\t} else {\n\t\tslog.Info(\"Server gracefully stopped\")\n\t}\n}\n",
  "go.mod": "module github.com/user/crawler-service\n\ngo 1.21\n\nrequire (\n\tgithub.com/chromedp/cdproto v0.0.0-20240202022224-52cf98711c21\n\tgithub.com/chromedp/chromedp v0.9.5\n\tgithub.com/jackc/pgx/v5 v5.5.5\n\tgithub.com/prometheus/client_golang v1.19.0\n\tgithub.com/redis/go-redis/v9 v9.5.1\n)\n\nrequire (\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/chromedp/sysutil v1.0.0 // indirect\n\tgithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect\n\tgithub.com/gobwas/httphead v0.1.0 // indirect\n\tgithub.com/gobwas/pool v0.2.1 // indirect\n\tgithub.com/gobwas/ws v1.3.2 // indirect\n\tgithub.com/jackc/pgpassfile v1.0.0 // indirect\n\tgithub.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a // indirect\n\tgithub.com/jackc/puddle/v2 v2.2.1 // indirect\n\tgithub.com/josharian/intern v1.0.0 // indirect\n\tgithub.com/mailru/easyjson v0.7.7 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions/v2 v2.0.0 // indirect\n\tgithub.com/prometheus/client_model v0.5.0 // indirect\n\tgithub.com/prometheus/common v0.48.0 // indirect\n\tgithub.com/prometheus/procfs v0.12.0 // indirect\n\tgolang.org/x/crypto v0.17.0 // indirect\n\tgolang.org/x/sync v0.5.0 // indirect\n\tgolang.org/x/sys v0.16.0 // indirect\n\tgolang.org/x/text v0.14.0 // indirect\n\tgoogle.golang.org/protobuf v1.32.0 // indirect\n)",
  "internal/adapter/chromedp_crawler/crawler_impl.go": "package chromedp_crawler\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"math/rand\"\n\t\"net/url\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/chromedp/cdproto/cdp\"\n\t\"github.com/chromedp/cdproto/network\"\n\t\"github.com/chromedp/chromedp\"\n\t\"github.com/user/crawler-service/internal/entity\"\n\t\"github.com/user/crawler-service/internal/repository\"\n\t\"github.com/user/crawler-service/pkg/utils\"\n)\n\nvar (\n\tuserAgents = []string{\n\t\t\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\",\n\t\t\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\",\n\t\t\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\",\n\t\t\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\",\n\t}\n\n\tviewports = []struct{ W, H int }{\n\t\t{1920, 1080},\n\t\t{1366, 768},\n\t\t{1536, 864},\n\t\t{2560, 1440},\n\t}\n)\n\ntype domainRateLimiter struct {\n\tlastRequest map[string]time.Time\n\tdelay       time.Duration\n\tmu          sync.Mutex\n}\n\nfunc newDomainRateLimiter(delay time.Duration) *domainRateLimiter {\n\treturn \u0026domainRateLimiter{\n\t\tlastRequest: make(map[string]time.Time),\n\t\tdelay:       delay,\n\t}\n}\n\nfunc (rl *domainRateLimiter) Wait(domain string) {\n\trl.mu.Lock()\n\tdefer rl.mu.Unlock()\n\n\tif last, ok := rl.lastRequest[domain]; ok {\n\t\tsince := time.Since(last)\n\t\tif since \u003c rl.delay {\n\t\t\ttime.Sleep(rl.delay - since)\n\t\t}\n\t}\n\trl.lastRequest[domain] = time.Now()\n}\n\ntype ChromedpCrawler struct {\n\tallocatorPool *sync.Pool\n\ttimeout       time.Duration\n\tproxies       []string\n\tproxyIndex    int\n\tproxyMu       sync.Mutex\n\trateLimiter   *domainRateLimiter\n}\n\n// NewChromedpCrawler creates a new crawler implementation using chromedp.\nfunc NewChromedpCrawler(maxConcurrency int, pageLoadTimeout time.Duration, proxies []string) (repository.CrawlerRepository, error) {\n\tpool := \u0026sync.Pool{\n\t\tNew: func() interface{} {\n\t\t\topts := append(chromedp.DefaultExecAllocatorOptions[:],\n\t\t\t\tchromedp.Flag(\"headless\", true),\n\t\t\t\tchromedp.Flag(\"disable-gpu\", true),\n\t\t\t\tchromedp.Flag(\"no-sandbox\", true),\n\t\t\t\tchromedp.Flag(\"disable-dev-shm-usage\", true),\n\t\t\t\tchromedp.Flag(\"blink-settings\", \"imagesEnabled=false\"), // Optionally disable images\n\t\t\t)\n\t\t\tallocCtx, _ := chromedp.NewExecAllocator(context.Background(), opts...)\n\t\t\treturn allocCtx\n\t\t},\n\t}\n\n\t// Pre-warm the pool\n\tfor i := 0; i \u003c maxConcurrency; i++ {\n\t\tallocCtx := pool.Get().(context.Context)\n\t\tpool.Put(allocCtx)\n\t}\n\n\treturn \u0026ChromedpCrawler{\n\t\tallocatorPool: pool,\n\t\ttimeout:       pageLoadTimeout,\n\t\tproxies:       proxies,\n\t\trateLimiter:   newDomainRateLimiter(1 * time.Second), // Default 1s delay\n\t}, nil\n}\n\nfunc (c *ChromedpCrawler) getNextProxy() string {\n\tif len(c.proxies) == 0 {\n\t\treturn \"\"\n\t}\n\tc.proxyMu.Lock()\n\tdefer c.proxyMu.Unlock()\n\tproxy := c.proxies[c.proxyIndex]\n\tc.proxyIndex = (c.proxyIndex + 1) % len(c.proxies)\n\treturn proxy\n}\n\n// Crawl fetches a URL and extracts data from it.\nfunc (c *ChromedpCrawler) Crawl(ctx context.Context, rawURL string, sneaky bool) (*entity.ExtractedData, error) {\n\tparsedURL, err := url.Parse(rawURL)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to parse URL: %w\", err)\n\t}\n\tc.rateLimiter.Wait(parsedURL.Hostname())\n\n\t// Get an allocator context from the pool\n\tallocCtx := c.allocatorPool.Get().(context.Context)\n\tdefer c.allocatorPool.Put(allocCtx)\n\n\topts := []chromedp.ContextOption{chromedp.WithLogf(slog.Debugf)}\n\tif proxy := c.getNextProxy(); proxy != \"\" {\n\t\topts = append(opts, chromedp.ProxyServer(proxy))\n\t}\n\n\t// Create a new browser context from the allocator\n\ttaskCtx, cancel := chromedp.NewContext(allocCtx, opts...)\n\tdefer cancel()\n\n\t// Create a timeout for the entire crawl task\n\ttaskCtx, cancel = context.WithTimeout(taskCtx, c.timeout)\n\tdefer cancel()\n\n\tvar (\n\t\ttitle, description, keywords, content string\n\t\th1s                                   []string\n\t\timages                                []entity.ImageInfo\n\t\tstatusCode                            int64\n\t\tresponseHeaders                       network.Headers\n\t)\n\n\tstartTime := time.Now()\n\n\t// Listen for response to get status code\n\tchromedp.ListenTarget(taskCtx, func(ev interface{}) {\n\t\tif resp, ok := ev.(*network.EventResponseReceived); ok {\n\t\t\tif resp.Type == network.ResourceTypeDocument \u0026\u0026 resp.Response.URL == rawURL {\n\t\t\t\tstatusCode = resp.Response.Status\n\t\t\t\tresponseHeaders = resp.Response.Headers\n\t\t\t}\n\t\t}\n\t})\n\n\tactions := []chromedp.Action{\n\t\tnetwork.Enable(),\n\t\tchromedp.Navigate(rawURL),\n\t\tchromedp.WaitVisible(`body`, chromedp.ByQuery),\n\t}\n\n\tif sneaky {\n\t\tvp := viewports[rand.Intn(len(viewports))]\n\t\tactions = append(actions,\n\t\t\tchromedp.EmulateViewport(int64(vp.W), int64(vp.H)),\n\t\t\tnetwork.SetExtraHTTPHeaders(network.Headers{\n\t\t\t\t\"User-Agent\": userAgents[rand.Intn(len(userAgents))],\n\t\t\t\t\"Referer\":    \"https://www.google.com/\",\n\t\t\t}),\n\t\t)\n\t}\n\n\t// Extraction tasks\n\textractionTasks := chromedp.Tasks{\n\t\tchromedp.Title(\u0026title),\n\t\tchromedp.AttributeValue(`meta[name=\"description\"]`, \"content\", \u0026description, nil),\n\t\tchromedp.AttributeValue(`meta[name=\"keywords\"]`, \"content\", \u0026keywords, nil),\n\t\tchromedp.ActionFunc(func(ctx context.Context) error {\n\t\t\tvar h1Nodes []*cdp.Node\n\t\t\tif err := chromedp.Nodes(`h1`, \u0026h1Nodes, chromedp.ByQueryAll).Do(ctx); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfor _, node := range h1Nodes {\n\t\t\t\tvar text string\n\t\t\t\tif err := chromedp.Text(node.NodeValue, \u0026text, chromedp.ByNodeID).Do(ctx); err != nil {\n\t\t\t\t\tslog.Warn(\"failed to get text for h1 node\", \"url\", rawURL, \"error\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif text != \"\" {\n\t\t\t\t\th1s = append(h1s, strings.TrimSpace(text))\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}),\n\t\tchromedp.ActionFunc(func(ctx context.Context) error {\n\t\t\tvar pNodes []*cdp.Node\n\t\t\tif err := chromedp.Nodes(`p`, \u0026pNodes, chromedp.ByQueryAll).Do(ctx); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tvar contentBuilder strings.Builder\n\t\t\tfor _, node := range pNodes {\n\t\t\t\tvar text string\n\t\t\t\tif err := chromedp.Text(node.NodeValue, \u0026text, chromedp.ByNodeID).Do(ctx); err != nil {\n\t\t\t\t\tslog.Warn(\"failed to get text for p node\", \"url\", rawURL, \"error\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif text != \"\" {\n\t\t\t\t\tcontentBuilder.WriteString(strings.TrimSpace(text))\n\t\t\t\t\tcontentBuilder.WriteString(\"\\n\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontent = contentBuilder.String()\n\t\t\treturn nil\n\t\t}),\n\t\tchromedp.ActionFunc(func(ctx context.Context) error {\n\t\t\tvar imgNodes []*cdp.Node\n\t\t\tif err := chromedp.Nodes(`img`, \u0026imgNodes, chromedp.ByQueryAll).Do(ctx); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfor _, node := range imgNodes {\n\t\t\t\tattrs, err := chromedp.Attributes(node.NodeValue, chromedp.ByNodeID).Do(ctx)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tvar img entity.ImageInfo\n\t\t\t\tfor i := 0; i \u003c len(attrs); i += 2 {\n\t\t\t\t\tswitch attrs[i] {\n\t\t\t\t\tcase \"src\":\n\t\t\t\t\t\tabsSrc, _ := utils.ToAbsoluteURL(parsedURL, attrs[i+1])\n\t\t\t\t\t\timg.Src = absSrc\n\t\t\t\t\tcase \"alt\":\n\t\t\t\t\t\timg.Alt = attrs[i+1]\n\t\t\t\t\tcase \"data-src\":\n\t\t\t\t\t\tabsDataSrc, _ := utils.ToAbsoluteURL(parsedURL, attrs[i+1])\n\t\t\t\t\t\timg.DataSrc = absDataSrc\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif img.Src != \"\" || img.DataSrc != \"\" {\n\t\t\t\t\timages = append(images, img)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}),\n\t}\n\n\tactions = append(actions, extractionTasks...)\n\n\tif err := chromedp.Run(taskCtx, actions...); err != nil {\n\t\tslog.Error(\"Failed to crawl URL\", \"url\", rawURL, \"error\", err)\n\t\treturn nil, fmt.Errorf(\"chromedp run failed: %w\", err)\n\t}\n\n\tresponseTime := time.Since(startTime)\n\n\tif statusCode == 0 {\n\t\treturn nil, errors.New(\"failed to capture main document response\")\n\t}\n\tif statusCode \u003e= 400 {\n\t\treturn nil, fmt.Errorf(\"received non-success status code: %d\", statusCode)\n\t}\n\n\tslog.Info(\"Successfully crawled URL\", \"url\", rawURL, \"title\", title, \"status\", statusCode, \"duration_ms\", responseTime.Milliseconds())\n\n\t// Stubbed data extraction\n\tdata := \u0026entity.ExtractedData{\n\t\tURL:            rawURL,\n\t\tTitle:          title,\n\t\tDescription:    description,\n\t\tKeywords:       strings.Split(keywords, \",\"),\n\t\tH1Tags:         h1s,\n\t\tContent:        content,\n\t\tImages:         images,\n\t\tCrawlTimestamp: time.Now(),\n\t\tHTTPStatusCode: int(statusCode),\n\t\tResponseTimeMS: int(responseTime.Milliseconds()),\n\t}\n\n\t_ = responseHeaders // Can be used for rate limiting headers later\n\n\treturn data, nil\n}\n",
  "internal/adapter/postgres/extracted_data_impl.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// ExtractedDataRepoImpl provides a concrete implementation for the ExtractedDataRepository interface using PostgreSQL.\ntype ExtractedDataRepoImpl struct {\n\tdb *pgxpool.Pool\n}\n\n// NewExtractedDataRepo creates a new instance of ExtractedDataRepoImpl.\nfunc NewExtractedDataRepo(db *pgxpool.Pool) *ExtractedDataRepoImpl {\n\treturn \u0026ExtractedDataRepoImpl{db: db}\n}\n\n// Save stores or updates the extracted data for a URL in the database.\nfunc (r *ExtractedDataRepoImpl) Save(ctx context.Context, data *entity.ExtractedData) error {\n\timagesJSON, err := json.Marshal(data.Images)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tquery := `\n\t\tINSERT INTO extracted_data (url, title, description, keywords, h1_tags, content, images, http_status_code, response_time_ms, crawl_timestamp)\n\t\tVALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)\n\t\tON CONFLICT (url) DO UPDATE SET\n\t\t\ttitle = EXCLUDED.title,\n\t\t\tdescription = EXCLUDED.description,\n\t\t\tkeywords = EXCLUDED.keywords,\n\t\t\th1_tags = EXCLUDED.h1_tags,\n\t\t\tcontent = EXCLUDED.content,\n\t\t\timages = EXCLUDED.images,\n\t\t\thttp_status_code = EXCLUDED.http_status_code,\n\t\t\tresponse_time_ms = EXCLUDED.response_time_ms,\n\t\t\tcrawl_timestamp = EXCLUDED.crawl_timestamp;\n\t`\n\n\t_, err = r.db.Exec(ctx, query,\n\t\tdata.URL,\n\t\tdata.Title,\n\t\tdata.Description,\n\t\tdata.Keywords,\n\t\tdata.H1Tags,\n\t\tdata.Content,\n\t\timagesJSON,\n\t\tdata.HTTPStatusCode,\n\t\tdata.ResponseTimeMS,\n\t\tdata.CrawlTimestamp,\n\t)\n\treturn err\n}\n\n// FindByURL retrieves the extracted data for a specific URL from the database.\nfunc (r *ExtractedDataRepoImpl) FindByURL(ctx context.Context, url string) (*entity.ExtractedData, error) {\n\tquery := `\n\t\tSELECT id, url, title, description, keywords, h1_tags, content, images, http_status_code, response_time_ms, crawl_timestamp\n\t\tFROM extracted_data\n\t\tWHERE url = $1;\n\t`\n\trow := r.db.QueryRow(ctx, query, url)\n\n\tvar data entity.ExtractedData\n\tvar imagesJSON []byte\n\n\terr := row.Scan(\n\t\t\u0026data.ID,\n\t\t\u0026data.URL,\n\t\t\u0026data.Title,\n\t\t\u0026data.Description,\n\t\t\u0026data.Keywords,\n\t\t\u0026data.H1Tags,\n\t\t\u0026data.Content,\n\t\t\u0026imagesJSON,\n\t\t\u0026data.HTTPStatusCode,\n\t\t\u0026data.ResponseTimeMS,\n\t\t\u0026data.CrawlTimestamp,\n\t)\n\tif err != nil {\n\t\treturn nil, err // pgx.ErrNoRows will be returned if not found\n\t}\n\n\tif err := json.Unmarshal(imagesJSON, \u0026data.Images); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn \u0026data, nil\n}\n",
  "internal/adapter/postgres/failed_url_impl.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// FailedURLRepoImpl provides a concrete implementation for the FailedURLRepository interface using PostgreSQL.\ntype FailedURLRepoImpl struct {\n\tdb *pgxpool.Pool\n}\n\n// NewFailedURLRepo creates a new instance of FailedURLRepoImpl.\nfunc NewFailedURLRepo(db *pgxpool.Pool) *FailedURLRepoImpl {\n\treturn \u0026FailedURLRepoImpl{db: db}\n}\n\n// SaveOrUpdate creates or updates a record for a failed URL.\n// It increments the retry_count on conflict.\nfunc (r *FailedURLRepoImpl) SaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error {\n\tquery := `\n\t\tINSERT INTO failed_urls (url, failure_reason, http_status_code, last_attempt_timestamp, retry_count, next_retry_at)\n\t\tVALUES ($1, $2, $3, $4, 1, $5)\n\t\tON CONFLICT (url) DO UPDATE SET\n\t\t\tfailure_reason = EXCLUDED.failure_reason,\n\t\t\thttp_status_code = EXCLUDED.http_status_code,\n\t\t\tlast_attempt_timestamp = EXCLUDED.last_attempt_timestamp,\n\t\t\tretry_count = failed_urls.retry_count + 1,\n\t\t\tnext_retry_at = EXCLUDED.next_retry_at;\n\t`\n\t_, err := r.db.Exec(ctx, query,\n\t\tfailedURL.URL,\n\t\tfailedURL.FailureReason,\n\t\tfailedURL.HTTPStatusCode,\n\t\tfailedURL.LastAttemptTimestamp,\n\t\tfailedURL.NextRetryAt,\n\t)\n\treturn err\n}\n\n// FindRetryable retrieves a batch of URLs that are due for a retry.\nfunc (r *FailedURLRepoImpl) FindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error) {\n\tquery := `\n\t\tSELECT id, url, failure_reason, http_status_code, last_attempt_timestamp, retry_count, next_retry_at\n\t\tFROM failed_urls\n\t\tWHERE next_retry_at \u003c= NOW()\n\t\tORDER BY next_retry_at ASC\n\t\tLIMIT $1;\n\t`\n\trows, err := r.db.Query(ctx, query, limit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\n\tvar failedURLs []*entity.FailedURL\n\tfor rows.Next() {\n\t\tvar fu entity.FailedURL\n\t\tif err := rows.Scan(\n\t\t\t\u0026fu.ID,\n\t\t\t\u0026fu.URL,\n\t\t\t\u0026fu.FailureReason,\n\t\t\t\u0026fu.HTTPStatusCode,\n\t\t\t\u0026fu.LastAttemptTimestamp,\n\t\t\t\u0026fu.RetryCount,\n\t\t\t\u0026fu.NextRetryAt,\n\t\t); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfailedURLs = append(failedURLs, \u0026fu)\n\t}\n\n\treturn failedURLs, rows.Err()\n}\n\n// Delete removes a failed URL record, typically after a successful crawl.\nfunc (r *FailedURLRepoImpl) Delete(ctx context.Context, url string) error {\n\tquery := `DELETE FROM failed_urls WHERE url = $1;`\n\t_, err := r.db.Exec(ctx, query, url)\n\treturn err\n}\n",
  "internal/adapter/redis/queue_impl.go": "package redis\n\nimport (\n\t\"context\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\nconst crawlQueueKey = \"crawler:queue\"\n\n// QueueRepoImpl provides a concrete implementation for the QueueRepository interface using Redis Lists.\ntype QueueRepoImpl struct {\n\tclient *redis.Client\n}\n\n// NewQueueRepo creates a new instance of QueueRepoImpl.\nfunc NewQueueRepo(client *redis.Client) *QueueRepoImpl {\n\treturn \u0026QueueRepoImpl{client: client}\n}\n\n// Push adds a URL to the left side of the Redis list (acting as a queue).\nfunc (r *QueueRepoImpl) Push(ctx context.Context, url string) error {\n\treturn r.client.LPush(ctx, crawlQueueKey, url).Err()\n}\n\n// Pop removes and returns a URL from the right side of the Redis list (acting as a queue).\n// It is a blocking operation if the list is empty, but we can add a timeout.\n// For simplicity, we use RPop which returns redis.Nil error if empty.\nfunc (r *QueueRepoImpl) Pop(ctx context.Context) (string, error) {\n\treturn r.client.RPop(ctx, crawlQueueKey).Result()\n}\n\n// Size returns the current number of items in the queue.\nfunc (r *QueueRepoImpl) Size(ctx context.Context) (int64, error) {\n\treturn r.client.LLen(ctx, crawlQueueKey).Result()\n}\n",
  "internal/adapter/redis/visited_impl.go": "package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/crawler-service/pkg/utils\"\n\t\"time\"\n)\n\nconst visitedURLPrefix = \"visited:\"\n\n// VisitedRepoImpl provides a concrete implementation for the VisitedRepository interface using Redis.\ntype VisitedRepoImpl struct {\n\tclient *redis.Client\n}\n\n// NewVisitedRepo creates a new instance of VisitedRepoImpl.\nfunc NewVisitedRepo(client *redis.Client) *VisitedRepoImpl {\n\treturn \u0026VisitedRepoImpl{client: client}\n}\n\n// generateKey creates a consistent Redis key for a given URL by hashing it.\nfunc (r *VisitedRepoImpl) generateKey(url string) string {\n\treturn fmt.Sprintf(\"%s%s\", visitedURLPrefix, utils.HashURL(url))\n}\n\n// MarkVisited marks a URL as visited by setting a key in Redis with a specific expiry time.\nfunc (r *VisitedRepoImpl) MarkVisited(ctx context.Context, url string, expiry time.Duration) error {\n\tkey := r.generateKey(url)\n\t// SETEX is atomic and sets the key with an expiry.\n\treturn r.client.SetEX(ctx, key, \"1\", expiry).Err()\n}\n\n// IsVisited checks if a URL has been visited recently by checking for the existence of its key.\nfunc (r *VisitedRepoImpl) IsVisited(ctx context.Context, url string) (bool, error) {\n\tkey := r.generateKey(url)\n\t// EXISTS returns 1 if the key exists, 0 otherwise.\n\tval, err := r.client.Exists(ctx, key).Result()\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn val == 1, nil\n}\n\n// RemoveVisited removes a URL from the visited set, used for force_crawl.\nfunc (r *VisitedRepoImpl) RemoveVisited(ctx context.Context, url string) error {\n\tkey := r.generateKey(url)\n\t// DEL removes the key.\n\treturn r.client.Del(ctx, key).Err()\n}\n",
  "internal/delivery/http/handler.go": "package handler\n\nimport (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/user/crawler-service/internal/delivery/http/request\"\n\t\"github.com/user/crawler-service/internal/delivery/http/response\"\n\t\"github.com/user/crawler-service/internal/usecase\"\n)\n\ntype Handler struct {\n\turlManager usecase.URLManager\n}\n\nfunc NewHandler(urlManager usecase.URLManager) *Handler {\n\treturn \u0026Handler{\n\t\turlManager: urlManager,\n\t}\n}\n\nfunc (h *Handler) HandleSubmitCrawl(w http.ResponseWriter, r *http.Request) {\n\tif r.Method != http.MethodPost {\n\t\th.writeJSONError(w, \"Method not allowed\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\tvar req request.SubmitCrawlRequest\n\tif err := json.NewDecoder(r.Body).Decode(\u0026req); err != nil {\n\t\th.writeJSONError(w, \"Invalid request body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif _, err := url.ParseRequestURI(req.URL); err != nil {\n\t\th.writeJSONError(w, \"Invalid URL format\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tcrawlID, err := h.urlManager.Submit(r.Context(), req.URL, req.ForceCrawl)\n\tif err != nil {\n\t\tif errors.Is(err, usecase.ErrURLRecentlyCrawled) {\n\t\t\th.writeJSONError(w, err.Error(), http.StatusConflict)\n\t\t\treturn\n\t\t}\n\t\tslog.Error(\"Failed to submit URL\", \"url\", req.URL, \"error\", err)\n\t\th.writeJSONError(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tresp := response.SubmitCrawlResponse{\n\t\tStatus:         \"success\",\n\t\tMessage:        \"URL submitted for crawling\",\n\t\tCrawlRequestID: crawlID,\n\t}\n\th.writeJSON(w, http.StatusAccepted, resp)\n}\n\nfunc (h *Handler) HandleGetCrawlStatus(w http.ResponseWriter, r *http.Request) {\n\tif r.Method != http.MethodGet {\n\t\th.writeJSONError(w, \"Method not allowed\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\trawURL := r.URL.Query().Get(\"url\")\n\tif rawURL == \"\" {\n\t\th.writeJSONError(w, \"URL query parameter is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif _, err := url.ParseRequestURI(rawURL); err != nil {\n\t\th.writeJSONError(w, \"Invalid URL format in query parameter\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tstatus, err := h.urlManager.GetStatus(r.Context(), rawURL)\n\tif err != nil {\n\t\tslog.Error(\"Failed to get crawl status\", \"url\", rawURL, \"error\", err)\n\t\th.writeJSONError(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tif status.CurrentStatus == \"not_found\" {\n\t\th.writeJSONError(w, \"Crawl status not found for the given URL\", http.StatusNotFound)\n\t\treturn\n\t}\n\n\tresp := response.CrawlStatusResponse{\n\t\tURL:                status.URL,\n\t\tCurrentStatus:      status.CurrentStatus,\n\t\tLastCrawlTimestamp: status.LastCrawlTimestamp,\n\t\tNextRetryAt:        status.NextRetryAt,\n\t\tFailureReason:      status.FailureReason,\n\t}\n\n\th.writeJSON(w, http.StatusOK, resp)\n}\n\nfunc (h *Handler) HandleHealthCheck(w http.ResponseWriter, r *http.Request) {\n\th.writeJSON(w, http.StatusOK, map[string]string{\"status\": \"ok\"})\n}\n\nfunc (h *Handler) writeJSON(w http.ResponseWriter, status int, data interface{}) {\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tw.WriteHeader(status)\n\tif err := json.NewEncoder(w).Encode(data); err != nil {\n\t\tslog.Error(\"Failed to write JSON response\", \"error\", err)\n\t}\n}\n\nfunc (h *Handler) writeJSONError(w http.ResponseWriter, message string, status int) {\n\th.writeJSON(w, status, map[string]string{\"error\": message})\n}\n",
  "internal/delivery/http/middleware/logging.go": "package middleware\n\nimport (\n\t\"log/slog\"\n\t\"net/http\"\n\t\"time\"\n)\n\nfunc Logging(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tstart := time.Now()\n\t\tnext.ServeHTTP(w, r)\n\t\tslog.Info(\"HTTP Request\",\n\t\t\t\"method\", r.Method,\n\t\t\t\"path\", r.URL.Path,\n\t\t\t\"duration_ms\", time.Since(start).Milliseconds(),\n\t\t\t\"remote_addr\", r.RemoteAddr,\n\t\t)\n\t})\n}\n",
  "internal/delivery/http/middleware/metrics.go": "package middleware\n\nimport (\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/user/crawler-service/pkg/metrics\"\n)\n\ntype responseWriter struct {\n\thttp.ResponseWriter\n\tstatusCode int\n}\n\nfunc newResponseWriter(w http.ResponseWriter) *responseWriter {\n\treturn \u0026responseWriter{w, http.StatusOK}\n}\n\nfunc (rw *responseWriter) WriteHeader(code int) {\n\trw.statusCode = code\n\trw.ResponseWriter.WriteHeader(code)\n}\n\nfunc Metrics(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tstart := time.Now()\n\t\trw := newResponseWriter(w)\n\t\tnext.ServeHTTP(rw, r)\n\t\tduration := time.Since(start)\n\n\t\tmetrics.HTTPRequestDuration.WithLabelValues(r.Method, r.URL.Path, strconv.Itoa(rw.statusCode)).Observe(duration.Seconds())\n\t\tmetrics.HTTPRequestsTotal.WithLabelValues(r.Method, r.URL.Path, strconv.Itoa(rw.statusCode)).Inc()\n\t})\n}\n",
  "internal/delivery/http/request.go": "package request\n\ntype SubmitCrawlRequest struct {\n\tURL        string `json:\"url\"`\n\tForceCrawl bool   `json:\"force_crawl\"`\n\tCrawlMode  string `json:\"crawl_mode\"` // \"respectful\" or \"sneaky\" - not used in this step\n}\n",
  "internal/delivery/http/response.go": "package response\n\nimport \"time\"\n\ntype SubmitCrawlResponse struct {\n\tStatus         string `json:\"status\"`\n\tMessage        string `json:\"message\"`\n\tCrawlRequestID string `json:\"crawl_request_id\"`\n}\n\n// CrawlStatusResponse is a DTO for crawl status, mirroring entity.CrawlStatus\ntype CrawlStatusResponse struct {\n\tURL                string     `json:\"url\"`\n\tCurrentStatus      string     `json:\"current_status\"` // \"pending\", \"crawling\", \"completed\", \"failed\"\n\tLastCrawlTimestamp *time.Time `json:\"last_crawl_timestamp,omitempty\"`\n\tNextRetryAt        *time.Time `json:\"next_retry_at,omitempty\"`\n\tFailureReason      string     `json:\"failure_reason,omitempty\"`\n}\n",
  "internal/delivery/http/router.go": "package router\n\nimport (\n\t\"net/http\"\n\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/user/crawler-service/internal/delivery/http/handler\"\n\t\"github.com/user/crawler-service/internal/delivery/http/middleware\"\n)\n\nfunc New(h *handler.Handler) http.Handler {\n\tmux := http.NewServeMux()\n\n\tmux.HandleFunc(\"GET /api/health\", h.HandleHealthCheck)\n\tmux.HandleFunc(\"POST /api/crawl\", h.HandleSubmitCrawl)\n\tmux.HandleFunc(\"GET /api/status\", h.HandleGetCrawlStatus)\n\n\t// Prometheus metrics endpoint\n\tmux.Handle(\"/metrics\", promhttp.Handler())\n\n\t// Apply middlewares\n\tvar chainedHandler http.Handler = mux\n\tchainedHandler = middleware.Metrics(chainedHandler)\n\tchainedHandler = middleware.Logging(chainedHandler)\n\n\treturn chainedHandler\n}\n",
  "internal/entity/crawl_status.go": "package entity\n\nimport \"time\"\n\ntype CrawlStatus struct {\n\tURL                string\n\tCurrentStatus      string // \"pending\", \"crawling\", \"completed\", \"failed\", \"not_found\"\n\tLastCrawlTimestamp *time.Time\n\tNextRetryAt        *time.Time\n\tFailureReason      string\n}\n",
  "internal/entity/extracted_data.go": "package entity\n\nimport \"time\"\n\n// ImageInfo represents the structured data for an image extracted from a page.\ntype ImageInfo struct {\n\tSrc     string `json:\"src\"`\n\tAlt     string `json:\"alt\"`\n\tDataSrc string `json:\"data_src,omitempty\"` // For lazy-loaded images\n}\n\n// ExtractedData mirrors the `extracted_data` PostgreSQL table schema.\ntype ExtractedData struct {\n\tID               int64\n\tURL              string\n\tTitle            string\n\tDescription      string\n\tKeywords         []string\n\tH1Tags           []string\n\tContent          string\n\tImages           []ImageInfo // Stored as JSONB in PostgreSQL\n\tCrawlTimestamp   time.Time\n\tHTTPStatusCode   int\n\tResponseTimeMS   int\n}\n",
  "internal/entity/failed_url.go": "package entity\n\nimport \"time\"\n\n// FailedURL mirrors the `failed_urls` PostgreSQL table schema.\ntype FailedURL struct {\n\tID                   int64\n\tURL                  string\n\tFailureReason        string\n\tHTTPStatusCode       int\n\tLastAttemptTimestamp time.Time\n\tRetryCount           int\n\tNextRetryAt          time.Time\n}\n",
  "internal/repository/crawler_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// CrawlerRepository defines the contract for the actual web page crawling mechanism.\ntype CrawlerRepository interface {\n\t// Crawl fetches a URL and extracts data from it.\n\tCrawl(ctx context.Context, url string, sneaky bool) (*entity.ExtractedData, error)\n}\n",
  "internal/repository/extracted_data_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// ExtractedDataRepository defines the interface for storing and retrieving extracted page data.\ntype ExtractedDataRepository interface {\n\t// Save stores the extracted data for a URL. If the URL already exists, it should be updated.\n\tSave(ctx context.Context, data *entity.ExtractedData) error\n\t// FindByURL retrieves the extracted data for a specific URL.\n\tFindByURL(ctx context.Context, url string) (*entity.ExtractedData, error)\n}\n",
  "internal/repository/failed_url_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// FailedURLRepository defines the interface for managing URLs that failed to be crawled.\ntype FailedURLRepository interface {\n\t// SaveOrUpdate creates or updates a record for a failed URL.\n\tSaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error\n\t// FindRetryable retrieves a batch of URLs that are due for a retry.\n\tFindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error)\n\t// Delete removes a failed URL record, typically after a successful crawl.\n\tDelete(ctx context.Context, url string) error\n}\n",
  "internal/repository/queue_repo.go": "package repository\n\nimport \"context\"\n\n// QueueRepository defines the interface for a FIFO queue for URLs to be crawled.\ntype QueueRepository interface {\n\t// Push adds a URL to the end of the queue.\n\tPush(ctx context.Context, url string) error\n\t// Pop removes and returns a URL from the front of the queue.\n\tPop(ctx context.Context) (string, error)\n\t// Size returns the current number of items in the queue.\n\tSize(ctx context.Context) (int64, error)\n}\n",
  "internal/repository/visited_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"time\"\n)\n\n// VisitedRepository defines the interface for deduplication of visited URLs.\ntype VisitedRepository interface {\n\t// MarkVisited marks a URL as visited with a specific expiry time.\n\tMarkVisited(ctx context.Context, url string, expiry time.Duration) error\n\t// IsVisited checks if a URL has been visited recently.\n\tIsVisited(ctx context.Context, url string) (bool, error)\n\t// RemoveVisited removes a URL from the visited set, used for force_crawl.\n\tRemoveVisited(ctx context.Context, url string) error\n}\n```",
  "internal/usecase/crawler_usecase.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"math\"\n\t\"math/rand\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/crawler-service/internal/entity\"\n\t\"github.com/user/crawler-service/internal/repository\"\n)\n\nconst (\n\tinitialBackoff = 5 * time.Second\n\tmaxRetries     = 5\n\tjitterFactor   = 0.2\n)\n\n// Crawler defines the interface for the core crawling process.\ntype Crawler interface {\n\tProcessURLFromQueue(ctx context.Context) error\n}\n\ntype crawlerUseCase struct {\n\tqueueRepo         repository.QueueRepository\n\tcrawlerRepo       repository.CrawlerRepository\n\textractedDataRepo repository.ExtractedDataRepository\n\tfailedURLRepo     repository.FailedURLRepository\n}\n\n// NewCrawlerUseCase creates a new instance of the crawler use case.\nfunc NewCrawlerUseCase(\n\tqueueRepo repository.QueueRepository,\n\tcrawlerRepo repository.CrawlerRepository,\n\textractedDataRepo repository.ExtractedDataRepository,\n\tfailedURLRepo repository.FailedURLRepository,\n) Crawler {\n\treturn \u0026crawlerUseCase{\n\t\tqueueRepo:         queueRepo,\n\t\tcrawlerRepo:       crawlerRepo,\n\t\textractedDataRepo: extractedDataRepo,\n\t\tfailedURLRepo:     failedURLRepo,\n\t}\n}\n\n// ProcessURLFromQueue fetches a single URL from the queue and processes it.\n// It handles success by saving data and failure by scheduling a retry.\nfunc (uc *crawlerUseCase) ProcessURLFromQueue(ctx context.Context) error {\n\turl, err := uc.queueRepo.Pop(ctx)\n\tif err != nil {\n\t\tif errors.Is(err, redis.Nil) {\n\t\t\t// Queue is empty, which is a normal state.\n\t\t\treturn nil\n\t\t}\n\t\tslog.Error(\"Failed to pop URL from queue\", \"error\", err)\n\t\treturn err\n\t}\n\n\tslog.Info(\"Processing URL from queue\", \"url\", url)\n\n\t// For now, we hardcode sneaky mode to false. This can be extended later\n\t// by storing crawl options along with the URL in the queue.\n\tconst useSneakyMode = false\n\textractedData, crawlErr := uc.crawlerRepo.Crawl(ctx, url, useSneakyMode)\n\n\tif crawlErr != nil {\n\t\tslog.Warn(\"Crawling failed for URL, scheduling retry\", \"url\", url, \"error\", crawlErr)\n\t\treturn uc.handleCrawlFailure(ctx, url, crawlErr)\n\t}\n\n\tslog.Info(\"Crawling successful for URL, saving data\", \"url\", url)\n\treturn uc.handleCrawlSuccess(ctx, extractedData)\n}\n\nfunc (uc *crawlerUseCase) handleCrawlSuccess(ctx context.Context, data *entity.ExtractedData) error {\n\tif err := uc.extractedDataRepo.Save(ctx, data); err != nil {\n\t\tslog.Error(\"Failed to save extracted data\", \"url\", data.URL, \"error\", err)\n\t\t// If saving fails, we might want to re-queue or handle it differently.\n\t\t// For now, we just log the error.\n\t\treturn err\n\t}\n\n\t// If the URL was previously failed, remove it from the failed table.\n\tif err := uc.failedURLRepo.Delete(ctx, data.URL); err != nil {\n\t\t// This is not a critical error, just log it.\n\t\tslog.Warn(\"Failed to delete URL from failed_urls table after successful crawl\", \"url\", data.URL, \"error\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (uc *crawlerUseCase) handleCrawlFailure(ctx context.Context, url string, crawlErr error) error {\n\tfailedURL := \u0026entity.FailedURL{\n\t\tURL:           url,\n\t\tFailureReason: crawlErr.Error(),\n\t\t// HTTPStatusCode would need to be parsed from the error, which can be complex.\n\t\t// We'll leave it as 0 for now unless the error provides it.\n\t}\n\n\t// This is a simplified logic. A real implementation would fetch the existing record.\n\t// The `SaveOrUpdate` in our postgres impl increments the count.\n\t// We need to calculate the next retry time here.\n\t// Let's assume we can get the current retry count from the DB or it's 0.\n\t// The current `SaveOrUpdate` increments the count, so we calculate based on that.\n\t// This is a bit of a chicken-and-egg problem without fetching first.\n\t// Let's just calculate a default first retry time. The repo can refine this.\n\t// A better way: The use case should own the retry logic.\n\t// Let's simulate fetching the retry count. For now, we'll just assume it's the first failure.\n\t// A proper implementation would be:\n\t// 1. failedRecord, err := failedURLRepo.FindByURL(ctx, url)\n\t// 2. if err == pgx.ErrNoRows -\u003e new record, retryCount = 0\n\t// 3. else -\u003e existing record, retryCount = failedRecord.RetryCount\n\n\t// Simplified logic for this step:\n\tretryCount := 0 // Assume we would fetch this. The DB ON CONFLICT will increment it.\n\tif retryCount \u003e= maxRetries {\n\t\tslog.Warn(\"URL has reached max retries, marking as permanently failed\", \"url\", url)\n\t\t// Set NextRetryAt to null or a far-future date to stop retrying.\n\t\t// For now, we just won't schedule a new retry.\n\t\tfailedURL.NextRetryAt = time.Time{} // Or a specific sentinel value\n\t} else {\n\t\tbackoff := initialBackoff * time.Duration(math.Pow(2, float64(retryCount)))\n\t\tjitter := time.Duration(rand.Float64()*jitterFactor*float64(backoff)) * (time.Duration(rand.Intn(2)*2 - 1))\n\t\tnextRetry := time.Now().Add(backoff + jitter)\n\t\tfailedURL.NextRetryAt = nextRetry\n\t\tslog.Info(\"Scheduling retry for failed URL\", \"url\", url, \"next_retry_at\", nextRetry)\n\t}\n\n\tif err := uc.failedURLRepo.SaveOrUpdate(ctx, failedURL); err != nil {\n\t\tslog.Error(\"Failed to save failed URL record\", \"url\", url, \"error\", err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n",
  "internal/usecase/url_manager_usecase.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/user/crawler-service/internal/entity\"\n\t\"github.com/user/crawler-service/internal/repository\"\n\t\"github.com/user/crawler-service/pkg/utils\"\n)\n\nvar ErrURLRecentlyCrawled = errors.New(\"url has been crawled recently\")\n\nconst deduplicationExpiry = 48 * time.Hour // 2 days\n\n// URLManager defines the interface for submitting and checking URLs.\ntype URLManager interface {\n\tSubmit(ctx context.Context, url string, force bool) (string, error)\n\tGetStatus(ctx context.Context, url string) (*entity.CrawlStatus, error)\n}\n\ntype urlManagerUseCase struct {\n\tvisitedRepo       repository.VisitedRepository\n\tqueueRepo         repository.QueueRepository\n\textractedDataRepo repository.ExtractedDataRepository\n\tfailedURLRepo     repository.FailedURLRepository\n}\n\n// NewURLManager creates a new URLManager use case.\nfunc NewURLManager(\n\tvisitedRepo repository.VisitedRepository,\n\tqueueRepo repository.QueueRepository,\n\textractedDataRepo repository.ExtractedDataRepository,\n\tfailedURLRepo repository.FailedURLRepository,\n) URLManager {\n\treturn \u0026urlManagerUseCase{\n\t\tvisitedRepo:       visitedRepo,\n\t\tqueueRepo:         queueRepo,\n\t\textractedDataRepo: extractedDataRepo,\n\t\tfailedURLRepo:     failedURLRepo,\n\t}\n}\n\nfunc (uc *urlManagerUseCase) Submit(ctx context.Context, url string, force bool) (string, error) {\n\tif !force {\n\t\tvisited, err := uc.visitedRepo.IsVisited(ctx, url)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tif visited {\n\t\t\treturn \"\", ErrURLRecentlyCrawled\n\t\t}\n\t} else {\n\t\t// If forcing, remove from visited to allow re-queuing immediately.\n\t\tif err := uc.visitedRepo.RemoveVisited(ctx, url); err != nil {\n\t\t\tslog.Warn(\"Failed to remove visited key for force crawl\", \"url\", url, \"error\", err)\n\t\t\t// Continue anyway, as this is not a critical failure\n\t\t}\n\t}\n\n\tif err := uc.queueRepo.Push(ctx, url); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Mark as visited to prevent re-queuing from other sources.\n\tif err := uc.visitedRepo.MarkVisited(ctx, url, deduplicationExpiry); err != nil {\n\t\t// Log the error but don't fail the submission, as it's already queued.\n\t\tslog.Error(\"Failed to mark URL as visited after queueing\", \"url\", url, \"error\", err)\n\t}\n\n\treturn utils.HashURL(url), nil\n}\n\nfunc (uc *urlManagerUseCase) GetStatus(ctx context.Context, url string) (*entity.CrawlStatus, error) {\n\t// 1. Check if successfully extracted\n\tdata, err := uc.extractedDataRepo.FindByURL(ctx, url)\n\tif err == nil \u0026\u0026 data != nil {\n\t\treturn \u0026entity.CrawlStatus{\n\t\t\tURL:                url,\n\t\t\tCurrentStatus:      \"completed\",\n\t\t\tLastCrawlTimestamp: \u0026data.CrawlTimestamp,\n\t\t}, nil\n\t}\n\tif err != nil \u0026\u0026 !errors.Is(err, pgx.ErrNoRows) {\n\t\treturn nil, err // Actual DB error\n\t}\n\n\t// 2. Check if it's in the failed table\n\t// A proper implementation would have a FindByURL method on the failedURLRepo.\n\t// For now, we'll assume this check is part of a more complete repo.\n\t// Let's add a placeholder for this logic.\n\t// For this step, we'll skip the failed check as the repo doesn't have FindByURL.\n\tfailedURL, err := uc.failedURLRepo.FindByURL(ctx, url) // Assuming this method exists now\n\tif err == nil \u0026\u0026 failedURL != nil {\n\t\tstatus := \"failed\"\n\t\tif failedURL.NextRetryAt.After(time.Now()) {\n\t\t\tstatus = \"retrying\"\n\t\t}\n\t\treturn \u0026entity.CrawlStatus{\n\t\t\tURL:           url,\n\t\t\tCurrentStatus: status,\n\t\t\tNextRetryAt:   \u0026failedURL.NextRetryAt,\n\t\t\tFailureReason: failedURL.FailureReason,\n\t\t}, nil\n\t}\n\tif err != nil \u0026\u0026 !errors.Is(err, pgx.ErrNoRows) {\n\t\treturn nil, err // Actual DB error\n\t}\n\n\t// 3. Check if it's \"pending\" (i.e., in the visited set but not completed or failed)\n\tvisited, err := uc.visitedRepo.IsVisited(ctx, url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif visited {\n\t\treturn \u0026entity.CrawlStatus{\n\t\t\tURL:           url,\n\t\t\tCurrentStatus: \"pending\",\n\t\t}, nil\n\t}\n\n\t// 4. If none of the above, it's not found\n\treturn \u0026entity.CrawlStatus{\n\t\tURL:           url,\n\t\tCurrentStatus: \"not_found\",\n\t}, nil\n}",
  "pkg/config/config.go": "package config\n\nimport (\n\t\"log\"\n\t\"os\"\n\t\"strconv\"\n\t\"time\"\n)\n\n// Config holds the application configuration.\ntype Config struct {\n\tServerPort string\n\tLogLevel   string\n\n\tPostgresHost     string\n\tPostgresPort     string\n\tPostgresUser     string\n\tPostgresPassword string\n\tPostgresDB       string\n\n\tRedisAddr     string\n\tRedisPassword string\n\tRedisDB       int\n\n\tMaxConcurrency int\n\tPageLoadTimeout time.Duration\n}\n\n// Load loads configuration from environment variables.\nfunc Load() *Config {\n\treturn \u0026Config{\n\t\tServerPort:       getEnv(\"SERVER_PORT\", \"8080\"),\n\t\tLogLevel:         getEnv(\"LOG_LEVEL\", \"info\"),\n\t\tPostgresHost:     getEnv(\"POSTGRES_HOST\", \"localhost\"),\n\t\tPostgresPort:     getEnv(\"POSTGRES_PORT\", \"5432\"),\n\t\tPostgresUser:     getEnv(\"POSTGRES_USER\", \"user\"),\n\t\tPostgresPassword: getEnv(\"POSTGRES_PASSWORD\", \"password\"),\n\t\tPostgresDB:       getEnv(\"POSTGRES_DB\", \"crawler\"),\n\t\tRedisAddr:        getEnv(\"REDIS_ADDR\", \"localhost:6379\"),\n\t\tRedisPassword:    getEnv(\"REDIS_PASSWORD\", \"\"),\n\t\tRedisDB:          getEnvAsInt(\"REDIS_DB\", 0),\n\t\tMaxConcurrency:   getEnvAsInt(\"MAX_CONCURRENCY\", 10),\n\t\tPageLoadTimeout:  getEnvAsDuration(\"PAGE_LOAD_TIMEOUT_SECONDS\", 60) * time.Second,\n\t}\n}\n\nfunc getEnv(key, fallback string) string {\n\tif value, ok := os.LookupEnv(key); ok {\n\t\treturn value\n\t}\n\treturn fallback\n}\n\nfunc getEnvAsInt(key string, fallback int) int {\n\tvalueStr := getEnv(key, \"\")\n\tif value, err := strconv.Atoi(valueStr); err == nil {\n\t\treturn value\n\t}\n\treturn fallback\n}\n\nfunc getEnvAsDuration(key string, fallback int) time.Duration {\n\treturn time.Duration(getEnvAsInt(key, fallback))\n}\n",
  "pkg/logger/logger.go": "package logger\n\nimport (\n\t\"io\"\n\t\"log/slog\"\n)\n\n// Init initializes the global slog logger.\nfunc Init(writer io.Writer, level slog.Level) {\n\thandler := slog.NewJSONHandler(writer, \u0026slog.HandlerOptions{\n\t\tLevel: level,\n\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\t// Customize attribute keys for consistency if needed\n\t\t\tif a.Key == slog.TimeKey {\n\t\t\t\ta.Key = \"timestamp\"\n\t\t\t}\n\t\t\tif a.Key == slog.LevelKey {\n\t\t\t\ta.Key = \"level\"\n\t\t\t}\n\t\t\tif a.Key == slog.MessageKey {\n\t\t\t\ta.Key = \"message\"\n\t\t\t}\n\t\t\treturn a\n\t\t},\n\t})\n\tlogger := slog.New(handler)\n\tslog.SetDefault(logger)\n}\n",
  "pkg/metrics/metrics.go": "package metrics\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\nvar (\n\tHTTPRequestsTotal *prometheus.CounterVec\n\tHTTPRequestDuration *prometheus.HistogramVec\n\tURLsInQueue prometheus.Gauge\n)\n\nfunc Init() {\n\tHTTPRequestsTotal = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"http_requests_total\",\n\t\t\tHelp: \"Total number of HTTP requests.\",\n\t\t},\n\t\t[]string{\"method\", \"path\", \"code\"},\n\t)\n\n\tHTTPRequestDuration = promauto.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"http_request_duration_seconds\",\n\t\t\tHelp:    \"Duration of HTTP requests.\",\n\t\t\tBuckets: prometheus.DefBuckets,\n\t\t},\n\t\t[]string{\"method\", \"path\", \"code\"},\n\t)\n\n\tURLsInQueue = promauto.NewGauge(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"crawler_urls_in_queue\",\n\t\t\tHelp: \"The current number of URLs in the crawl queue.\",\n\t\t},\n\t)\n}\n```",
  "pkg/utils/url.go": "package utils\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/hex\"\n\t\"net/url\"\n)\n\n// HashURL creates a SHA256 hash of a URL string.\n// This is useful for creating consistent, safe keys for Redis.\nfunc HashURL(rawURL string) string {\n\th := sha256.New()\n\th.Write([]byte(rawURL))\n\treturn hex.EncodeToString(h.Sum(nil))\n}\n\n// ToAbsoluteURL converts a relative URL to an absolute URL given a base URL.\nfunc ToAbsoluteURL(base *url.URL, relative string) (string, error) {\n\trelURL, err := url.Parse(relative)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base.ResolveReference(relURL).String(), nil\n}\n```",
  "schema/001_init.sql": "CREATE TABLE IF NOT EXISTS extracted_data (\n    id BIGSERIAL PRIMARY KEY,\n    url TEXT NOT NULL UNIQUE,\n    title TEXT,\n    description TEXT,\n    keywords TEXT[],\n    h1_tags TEXT[],\n    content TEXT,\n    images JSONB,\n    crawl_timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    http_status_code INT,\n    response_time_ms INT\n);\n\nCREATE INDEX IF NOT EXISTS idx_extracted_data_url ON extracted_data(url);\nCREATE INDEX IF NOT EXISTS idx_extracted_data_crawl_timestamp ON extracted_data(crawl_timestamp);\n\nCREATE TABLE IF NOT EXISTS failed_urls (\n    id BIGSERIAL PRIMARY KEY,\n    url TEXT NOT NULL UNIQUE,\n    failure_reason TEXT,\n    http_status_code INT,\n    last_attempt_timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    retry_count INT NOT NULL DEFAULT 0,\n    next_retry_at TIMESTAMPTZ\n);\n\nCREATE INDEX IF NOT EXISTS idx_failed_urls_url ON failed_urls(url);\nCREATE INDEX IF NOT EXISTS idx_failed_urls_next_retry_at ON failed_urls(next_retry_at);\n\n"
}