{
  "Dockerfile": "# Stage 1: Build the application\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Copy go.mod and go.sum files to download dependencies\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy the source code\nCOPY . .\n\n# Build the application\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /app/crawler-service ./cmd/api\n\n# Stage 2: Create the final, minimal image\nFROM alpine:latest\n\nWORKDIR /root/\n\n# Copy the binary from the builder stage\nCOPY --from=builder /app/crawler-service .\n\n# Expose port (will be used by the API later)\nEXPOSE 8080\n\n# Command to run the executable\nCMD [\"./crawler-service\"]\n",
  "Makefile": ".PHONY: build run tidy\n\nbuild:\n\t@echo \"Building binary...\"\n\t@go build -o ./bin/crawler-service ./cmd/api\n\nrun:\n\t@echo \"Running application...\"\n\t@go run ./cmd/api\n\ntidy:\n\t@echo \"Running go mod tidy...\"\n\t@go mod tidy\n",
  "cmd/api/main.go": "package main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/go-redis/redis/v9\" // Changed from github.com/redis/go-redis/v9\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\" // Kept for /metrics endpoint\n\n\t\"github.com/user/crawler-service/internal/adapter/chromedp_crawler\"\n\t\"github.com/user/crawler-service/internal/adapter/postgres\" // Changed import alias\n\tredis_adapter \"github.com/user/crawler-service/internal/adapter/redis\"\n\thttp_delivery \"github.com/user/crawler-service/internal/delivery/http\" // Changed import alias\n\t\"github.com/user/crawler-service/internal/repository\"                 // Added for QueueRepository in metrics collector\n\t\"github.com/user/crawler-service/internal/usecase\"\n\t\"github.com/user/crawler-service/pkg/config\"\n\t\"github.com/user/crawler-service/pkg/logger\"\n\t\"github.com/user/crawler-service/pkg/metrics\"\n)\n\nfunc main() {\n\t// --- Configuration ---\n\tcfg := config.Load()\n\n\t// --- Logger ---\n\tlogLevel := slog.LevelInfo\n\tif cfg.LogLevel == \"debug\" {\n\t\tlogLevel = slog.LevelDebug\n\t}\n\tlogger.Init(os.Stdout, logLevel)\n\tslog.Info(\"Logger initialized\", \"level\", logLevel.String())\n\n\t// --- Metrics ---\n\tmetrics.Init()\n\tslog.Info(\"Metrics initialized\")\n\n\t// Create a context that is cancelled on interruption signals\n\tctx, stop := signal.NotifyContext(context.Background(), syscall.SIGINT, syscall.SIGTERM)\n\tdefer stop()\n\n\t// --- Database Connections ---\n\tpgConnString := fmt.Sprintf(\"host=%s port=%s user=%s password=%s dbname=%s sslmode=disable\",\n\t\tcfg.PostgresHost, cfg.PostgresPort, cfg.PostgresUser, cfg.PostgresPassword, cfg.PostgresDB)\n\tdbPool, err := pgxpool.New(ctx, pgConnString) // Use ctx for connection\n\tif err != nil {\n\t\tslog.Error(\"Unable to connect to database\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer dbPool.Close()\n\tslog.Info(\"Successfully connected to PostgreSQL\")\n\n\tredisClient := redis.NewClient(\u0026redis.Options{\n\t\tAddr:     cfg.RedisAddr,\n\t\tPassword: cfg.RedisPassword,\n\t\tDB:       cfg.RedisDB,\n\t})\n\tif err := redisClient.Ping(ctx).Err(); err != nil { // Use ctx for ping\n\t\tslog.Error(\"Unable to connect to Redis\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer redisClient.Close() // Add defer close for redis\n\tslog.Info(\"Successfully connected to Redis\")\n\n\t// --- Repositories ---\n\tvisitedRepo := redis_adapter.NewVisitedRepo(redisClient)\n\tqueueRepo := redis_adapter.NewQueueRepo(redisClient)\n\textractedDataRepo := postgres.NewExtractedDataRepo(dbPool) // Use new postgres adapter\n\tfailedURLRepo := postgres.NewFailedURLRepo(dbPool)         // Use new postgres adapter\n\n\t// Initialize Crawler Repository\n\t// For now, no proxies are configured. This can be loaded from config.\n\tvar proxies []string\n\tcrawlerRepo, err := chromedp_crawler.NewChromedpCrawler(cfg.MaxConcurrency, cfg.PageLoadTimeout, proxies) // Added proxies argument\n\tif err != nil {\n\t\tslog.Error(\"Failed to initialize Chromedp Crawler\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tslog.Info(\"Chromedp crawler initialized\")\n\n\t// --- Use Cases ---\n\turlManager := usecase.NewURLManager(visitedRepo, queueRepo, extractedDataRepo, failedURLRepo)\n\t// The crawler use case would be run by background workers.\n\t// For the API, we only need the URL manager.\n\t// _ = usecase.NewCrawlerUseCase(queueRepo, crawlerRepo, extractedDataRepo, failedURLRepo) // Commented out as per attempted content\n\tslog.Info(\"URL Manager use case initialized\") // Updated log message\n\n\t// --- Start Background Services ---\n\tgo startQueueMetricsCollector(ctx, queueRepo) // Added from attempted content\n\n\t// --- HTTP Server ---\n\tapiHandler := http_delivery.NewHandler(urlManager) // Use http_delivery\n\thttpRouter := http_delivery.New(apiHandler)        // Use http_delivery\n\n\t// Add Prometheus metrics handler\n\thttp.Handle(\"/metrics\", promhttp.Handler())\n\thttp.Handle(\"/\", httpRouter) // Use the new router\n\n\tserver := \u0026http.Server{\n\t\tAddr:         net.JoinHostPort(\"\", cfg.ServerPort), // Use net.JoinHostPort\n\t\tHandler:      http.DefaultServeMux,                  // Use DefaultServeMux to handle both router and metrics\n\t\tReadTimeout:  10 * time.Second,                      // Adopted from attempted content\n\t\tWriteTimeout: 10 * time.Second,\n\t\tIdleTimeout:  120 * time.Second, // Kept from original\n\t}\n\n\tgo func() {\n\t\tslog.Info(\"Server is starting\", \"port\", cfg.ServerPort)\n\t\tif err := server.ListenAndServe(); err != nil \u0026\u0026 !errors.Is(err, http.ErrServerClosed) { // Adopted error check\n\t\t\tslog.Error(\"Server failed to start\", \"error\", err)\n\t\t\tos.Exit(1)\n\t\t}\n\t}()\n\n\t// --- Graceful Shutdown ---\n\t\u003c-ctx.Done() // Use the context from signal.NotifyContext\n\tslog.Info(\"Shutting down server...\")\n\n\tshutdownCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second) // Kept original timeout duration\n\tdefer cancel()\n\n\tif err := server.Shutdown(shutdownCtx); err != nil {\n\t\tslog.Error(\"Server shutdown failed\", \"error\", err)\n\t} else {\n\t\tslog.Info(\"Server gracefully stopped\")\n\t}\n}\n\n// startQueueMetricsCollector periodically polls the queue for its size and updates the Prometheus gauge.\nfunc startQueueMetricsCollector(ctx context.Context, queueRepo repository.QueueRepository) {\n\tticker := time.NewTicker(5 * time.Second)\n\tdefer ticker.Stop()\n\n\tslog.Info(\"Starting queue metrics collector\")\n\n\tfor {\n\t\tselect {\n\t\tcase \u003c-ticker.C:\n\t\t\tsize, err := queueRepo.Size(context.Background()) // Use background context for short-lived operation\n\t\t\tif err != nil {\n\t\t\t\tslog.Error(\"Failed to get queue size for metrics\", \"error\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tmetrics.URLsInQueue.Set(float64(size))\n\t\tcase \u003c-ctx.Done():\n\t\t\tslog.Info(\"Stopping queue metrics collector\")\n\t\t\treturn\n\t\t}\n\t}\n}\n",
  "go.mod": "module github.com/user/crawler-service\n\ngo 1.21\n\nrequire (\n\tgithub.com/chromedp/cdproto v0.0.0-20240202022224-52cf98711c21\n\tgithub.com/chromedp/chromedp v0.9.5\n\tgithub.com/jackc/pgx/v5 v5.5.5\n\tgithub.com/prometheus/client_golang v1.19.0\n\tgithub.com/redis/go-redis/v9 v9.5.1\n)\n\nrequire (\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/chromedp/sysutil v1.0.0 // indirect\n\tgithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect\n\tgithub.com/gobwas/httphead v0.1.0 // indirect\n\tgithub.com/gobwas/pool v0.2.1 // indirect\n\tgithub.com/gobwas/ws v1.3.2 // indirect\n\tgithub.com/jackc/pgpassfile v1.0.0 // indirect\n\tgithub.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a // indirect\n\tgithub.com/jackc/puddle/v2 v2.2.1 // indirect\n\tgithub.com/josharian/intern v1.0.0 // indirect\n\tgithub.com/mailru/easyjson v0.7.7 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions/v2 v2.0.0 // indirect\n\tgithub.com/prometheus/client_model v0.5.0 // indirect\n\tgithub.com/prometheus/common v0.48.0 // indirect\n\tgithub.com/prometheus/procfs v0.12.0 // indirect\n\tgolang.org/x/crypto v0.17.0 // indirect\n\tgolang.org/x/sync v0.5.0 // indirect\n\tgolang.org/x/sys v0.16.0 // indirect\n\tgolang.org/x/text v0.14.0 // indirect\n\tgoogle.golang.org/protobuf v1.32.0 // indirect\n)",
  "internal/adapter/chromedp_crawler/crawler_impl.go": "package chromedp_crawler\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"math/rand\"\n\t\"net/url\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/chromedp/cdproto/cdp\"\n\t\"github.com/chromedp/cdproto/network\"\n\t\"github.com/chromedp/chromedp\"\n\t\"github.com/user/crawler-service/internal/entity\"\n\t\"github.com/user/crawler-service/internal/repository\"\n\t\"github.com/user/crawler-service/pkg/utils\"\n)\n\nvar (\n\tuserAgents = []string{\n\t\t\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\",\n\t\t\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\",\n\t\t\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15\", // From original\n\t\t\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\",\n\t\t\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\", // From attempted\n\t}\n\n\tviewports = []struct{ W, H int }{\n\t\t{1920, 1080},\n\t\t{1366, 768},\n\t\t{1536, 864},\n\t\t{2560, 1440}, // From original\n\t}\n)\n\ntype domainRateLimiter struct {\n\tlastRequest map[string]time.Time\n\tdelay       time.Duration\n\tmu          sync.Mutex\n}\n\nfunc newDomainRateLimiter(delay time.Duration) *domainRateLimiter {\n\treturn \u0026domainRateLimiter{\n\t\tlastRequest: make(map[string]time.Time),\n\t\tdelay:       delay,\n\t}\n}\n\nfunc (rl *domainRateLimiter) Wait(domain string) {\n\trl.mu.Lock()\n\tdefer rl.mu.Unlock()\n\n\tif last, ok := rl.lastRequest[domain]; ok { // Kept original's 'ok'\n\t\tsince := time.Since(last) // Kept original's 'since'\n\t\tif since \u003c rl.delay {\n\t\t\ttime.Sleep(rl.delay - since)\n\t\t}\n\t}\n\trl.lastRequest[domain] = time.Now()\n}\n\ntype ChromedpCrawler struct {\n\tallocatorPool *sync.Pool\n\ttimeout       time.Duration\n\tproxies       []string\n\tproxyIndex    int\n\tproxyMu       sync.Mutex\n\trateLimiter   *domainRateLimiter\n}\n\n// NewChromedpCrawler creates a new crawler implementation using chromedp.\nfunc NewChromedpCrawler(maxConcurrency int, pageLoadTimeout time.Duration, proxies []string) (repository.CrawlerRepository, error) {\n\tpool := \u0026sync.Pool{\n\t\tNew: func() interface{} {\n\t\t\topts := append(chromedp.DefaultExecAllocatorOptions[:],\n\t\t\t\tchromedp.Flag(\"headless\", true),\n\t\t\t\tchromedp.Flag(\"disable-gpu\", true),\n\t\t\t\tchromedp.Flag(\"no-sandbox\", true),\n\t\t\t\tchromedp.Flag(\"disable-dev-shm-usage\", true),\n\t\t\t\tchromedp.Flag(\"blink-settings\", \"imagesEnabled=false\"), // Kept from original\n\t\t\t)\n\t\t\tallocCtx, cancel := chromedp.NewExecAllocator(context.Background(), opts...) // Adopted cancel from attempted\n\t\t\treturn context.CancelFunc(func() { // Adopted context.CancelFunc for pool management\n\t\t\t\tcancel()\n\t\t\t\tchromedp.Cancel(allocCtx)\n\t\t\t})\n\t\t},\n\t}\n\n\t// Pre-warm the pool\n\tfor i := 0; i \u003c maxConcurrency; i++ {\n\t\tpool.Put(pool.New()) // Adopted simpler pre-warming\n\t}\n\n\treturn \u0026ChromedpCrawler{\n\t\tallocatorPool: pool,\n\t\ttimeout:       pageLoadTimeout,\n\t\tproxies:       proxies,\n\t\trateLimiter:   newDomainRateLimiter(1 * time.Second), // Default 1s delay\n\t}, nil\n}\n\nfunc (c *ChromedpCrawler) getNextProxy() string {\n\tif len(c.proxies) == 0 {\n\t\treturn \"\"\n\t}\n\tc.proxyMu.Lock()\n\tdefer c.proxyMu.Unlock()\n\tproxy := c.proxies[c.proxyIndex]\n\tc.proxyIndex = (c.proxyIndex + 1) % len(c.proxies)\n\treturn proxy\n}\n\n// Crawl fetches a URL and extracts data from it.\nfunc (c *ChromedpCrawler) Crawl(ctx context.Context, rawURL string, sneaky bool) (*entity.ExtractedData, error) {\n\tparsedURL, err := url.Parse(rawURL)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to parse URL: %w\", err)\n\t}\n\tdomain := parsedURL.Hostname() // Adopted from attempted\n\tc.rateLimiter.Wait(domain)\n\n\t// Get an allocator context from the pool\n\tallocatorCtx, cancelAllocator := c.allocatorPool.Get().(context.CancelFunc) // Adopted pool management\n\tdefer c.allocatorPool.Put(cancelAllocator)                                   // Adopted pool management\n\n\t// Create a new browser context from the allocator\n\tbrowserCtx, cancelBrowser := chromedp.NewContext(context.Background(), chromedp.WithLogf(slog.Debugf)) // Original approach for browser context\n\tdefer cancelBrowser()\n\n\t// Create a timeout for the entire crawl task\n\ttaskCtx, cancelTask := context.WithTimeout(browserCtx, c.timeout) // Original approach for task timeout\n\tdefer cancelTask()\n\n\tvar (\n\t\ttitle, description, content string\n\t\tkeywords                    []string // Changed to slice for keywords\n\t\th1s                         []string\n\t\timages                      []*cdp.Node // Adopted from attempted for image nodes\n\t\tstatusCode                  int64\n\t\tfinalURL                    string // Adopted from attempted\n\t)\n\n\tstartTime := time.Now()\n\n\t// Listen for network responses to capture status code and final URL\n\t// Adopted from attempted content for more robust status/final URL capture\n\tlistenCtx, cancelListen := context.WithCancel(taskCtx)\n\tdefer cancelListen()\n\n\tchromedp.ListenTarget(listenCtx, func(ev interface{}) {\n\t\tif resp, ok := ev.(*network.EventResponseReceived); ok {\n\t\t\tif resp.Type == network.ResourceTypeDocument {\n\t\t\t\t// Capture the status code of the main document request\n\t\t\t\tif statusCode == 0 {\n\t\t\t\t\tstatusCode = resp.Response.Status\n\t\t\t\t\tfinalURL = resp.Response.URL\n\t\t\t\t\tslog.Debug(\"Captured response\", \"url\", rawURL, \"final_url\", finalURL, \"status\", statusCode)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\tactions := []chromedp.Action{\n\t\tnetwork.Enable(),\n\t}\n\n\tif proxy := c.getNextProxy(); proxy != \"\" {\n\t\tactions = append(actions, chromedp.ProxyServer(proxy)) // Proxy added as an action\n\t}\n\n\tif sneaky {\n\t\tvp := viewports[rand.Intn(len(viewports))]\n\t\tua := userAgents[rand.Intn(len(userAgents))] // Adopted from attempted\n\t\tactions = append(actions,\n\t\t\tchromedp.EmulateViewport(int64(vp.W), int64(vp.H)),\n\t\t\tnetwork.SetExtraHTTPHeaders(network.Headers{ // Kept from original\n\t\t\t\t\"User-Agent\": ua,\n\t\t\t\t\"Referer\":    \"https://www.google.com/\",\n\t\t\t}),\n\t\t\tchromedp.UserAgent(ua), // Adopted from attempted\n\t\t)\n\t}\n\n\tactions = append(actions,\n\t\tchromedp.Navigate(rawURL),\n\t\tchromedp.WaitVisible(`body`, chromedp.ByQuery),\n\t\tchromedp.Title(\u0026title),\n\t\tchromedp.Location(\u0026finalURL), // Fallback for final URL, adopted from attempted\n\t\tchromedp.AttributeValue(`meta[name=\"description\"]`, \"content\", \u0026description, nil),\n\t\tchromedp.AttributeValue(`meta[name=\"keywords\"]`, \"content\", \u0026keywords, nil), // Changed to slice\n\t\tchromedp.ActionFunc(func(ctx context.Context) error { // Kept original H1 extraction\n\t\t\tvar h1Nodes []*cdp.Node\n\t\t\tif err := chromedp.Nodes(`h1`, \u0026h1Nodes, chromedp.ByQueryAll).Do(ctx); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfor _, node := range h1Nodes {\n\t\t\t\tvar text string\n\t\t\t\tif err := chromedp.Text(node.NodeValue, \u0026text, chromedp.ByNodeID).Do(ctx); err != nil {\n\t\t\t\t\tslog.Warn(\"failed to get text for h1 node\", \"url\", rawURL, \"error\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif text != \"\" {\n\t\t\t\t\th1s = append(h1s, strings.TrimSpace(text))\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}),\n\t\tchromedp.ActionFunc(func(ctx context.Context) error { // Kept original P content extraction\n\t\t\tvar pNodes []*cdp.Node\n\t\t\tif err := chromedp.Nodes(`p`, \u0026pNodes, chromedp.ByQueryAll).Do(ctx); err != nil {\n\t\t\t\treturn err\n\t\t\t\t// If we want to concatenate all p tags into a single string, we can do this:\n\t\t\t\t// var contentBuilder strings.Builder\n\t\t\t\t// for _, node := range pNodes {\n\t\t\t\t// \tvar text string\n\t\t\t\t// \tif err := chromedp.Text(node.NodeValue, \u0026text, chromedp.ByNodeID).Do(ctx); err != nil {\n\t\t\t\t// \t\tslog.Warn(\"failed to get text for p node\", \"url\", rawURL, \"error\", err)\n\t\t\t\t// \t\tcontinue\n\t\t\t\t// \t}\n\t\t\t\t// \tif text != \"\" {\n\t\t\t\t// \t\tcontentBuilder.WriteString(strings.TrimSpace(text))\n\t\t\t\t// \t\tcontentBuilder.WriteString(\"\\n\")\n\t\t\t\t// \t}\n\t\t\t\t// }\n\t\t\t\t// content = contentBuilder.String()\n\t\t\t}\n\t\t\t// For now, let's just get the text of the first p tag or concatenate them.\n\t\t\t// The attempted content uses chromedp.Text(`p`, \u0026content, chromedp.ByQueryAll) which concatenates.\n\t\t\t// Let's adopt that simpler concatenation for 'content'.\n\t\t\tif err := chromedp.Text(`p`, \u0026content, chromedp.ByQueryAll).Do(ctx); err != nil {\n\t\t\t\tslog.Warn(\"failed to get text for p tags\", \"url\", rawURL, \"error\", err)\n\t\t\t}\n\t\t\treturn nil\n\t\t}),\n\t\tchromedp.Nodes(`img`, \u0026images, chromedp.ByQueryAll), // Adopted from attempted for image nodes\n\t)\n\n\tif err := chromedp.Run(taskCtx, actions...); err != nil {\n\t\tif errors.Is(err, context.DeadlineExceeded) { // Adopted specific error handling\n\t\t\treturn nil, fmt.Errorf(\"%w: %v\", repository.ErrCrawlTimeout, err)\n\t\t}\n\t\tif strings.Contains(err.Error(), \"net::\") { // Adopted specific error handling\n\t\t\treturn nil, fmt.Errorf(\"%w: %v\", repository.ErrNavigationFailed, err)\n\t\t}\n\t\tslog.Error(\"Chromedp run failed\", \"url\", rawURL, \"error\", err)\n\t\treturn nil, fmt.Errorf(\"%w: %v\", repository.ErrExtractionFailed, err) // Adopted specific error handling\n\t}\n\n\tresponseTime := time.Since(startTime)\n\n\t// If listener didn't catch status, it might be a cached response or other issue.\n\t// We can consider this a partial success or failure. For now, let's mark as 200 if successful.\n\tif statusCode == 0 { // Adopted from attempted\n\t\tslog.Warn(\"Could not determine status code from network events, assuming 200\", \"url\", rawURL)\n\t\tstatusCode = 200\n\t}\n\n\tif statusCode \u003e= 400 \u0026\u0026 statusCode \u003c 500 { // Adopted from attempted\n\t\treturn nil, fmt.Errorf(\"%w: received status code %d\", repository.ErrContentRestricted, statusCode)\n\t}\n\tif statusCode \u003e= 500 { // Adopted from attempted\n\t\treturn nil, fmt.Errorf(\"%w: received status code %d\", repository.ErrNavigationFailed, statusCode)\n\t}\n\n\tslog.Info(\"Successfully crawled URL\", \"url\", rawURL, \"title\", title, \"status\", statusCode, \"duration_ms\", responseTime.Milliseconds())\n\n\tdata := \u0026entity.ExtractedData{\n\t\tURL:            rawURL, // Store original URL\n\t\tTitle:          title,\n\t\tDescription:    description,\n\t\tH1Tags:         h1s,\n\t\tContent:        content,\n\t\tCrawlTimestamp: time.Now(),\n\t\tHTTPStatusCode: int(statusCode),\n\t\tResponseTimeMS: int(responseTime.Milliseconds()),\n\t}\n\n\tif len(keywords) \u003e 0 { // Adopted from attempted for keyword processing\n\t\tdata.Keywords = strings.Split(keywords[0], \",\")\n\t\tfor i := range data.Keywords {\n\t\t\tdata.Keywords[i] = strings.TrimSpace(data.Keywords[i])\n\t\t}\n\t}\n\n\tbase, _ := url.Parse(finalURL) // Use finalURL for base, adopted from attempted\n\tfor _, imgNode := range images {\n\t\tsrc, _ := imgNode.Attribute(\"src\")\n\t\tdataSrc, _ := imgNode.Attribute(\"data-src\")\n\t\talt, _ := imgNode.Attribute(\"alt\")\n\n\t\tabsSrc, _ := utils.ToAbsoluteURL(base, src)\n\t\tabsDataSrc, _ := utils.ToAbsoluteURL(base, dataSrc)\n\n\t\tif absSrc != \"\" || absDataSrc != \"\" {\n\t\t\tdata.Images = append(data.Images, entity.ImageInfo{\n\t\t\t\tSrc:     absSrc,\n\t\t\t\tAlt:     alt,\n\t\t\t\tDataSrc: absDataSrc,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn data, nil\n}\n",
  "internal/adapter/postgres/extracted_data_impl.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// ExtractedDataRepoImpl provides a concrete implementation for the ExtractedDataRepository interface using PostgreSQL.\ntype ExtractedDataRepoImpl struct {\n\tdb *pgxpool.Pool\n}\n\n// NewExtractedDataRepo creates a new instance of ExtractedDataRepoImpl.\nfunc NewExtractedDataRepo(db *pgxpool.Pool) *ExtractedDataRepoImpl {\n\treturn \u0026ExtractedDataRepoImpl{db: db}\n}\n\n// Save stores or updates the extracted data for a URL in the database.\nfunc (r *ExtractedDataRepoImpl) Save(ctx context.Context, data *entity.ExtractedData) error {\n\timagesJSON, err := json.Marshal(data.Images)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tquery := `\n\t\tINSERT INTO extracted_data (url, title, description, keywords, h1_tags, content, images, http_status_code, response_time_ms, crawl_timestamp)\n\t\tVALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)\n\t\tON CONFLICT (url) DO UPDATE SET\n\t\t\ttitle = EXCLUDED.title,\n\t\t\tdescription = EXCLUDED.description,\n\t\t\tkeywords = EXCLUDED.keywords,\n\t\t\th1_tags = EXCLUDED.h1_tags,\n\t\t\tcontent = EXCLUDED.content,\n\t\t\timages = EXCLUDED.images,\n\t\t\thttp_status_code = EXCLUDED.http_status_code,\n\t\t\tresponse_time_ms = EXCLUDED.response_time_ms,\n\t\t\tcrawl_timestamp = EXCLUDED.crawl_timestamp;\n\t`\n\n\t_, err = r.db.Exec(ctx, query,\n\t\tdata.URL,\n\t\tdata.Title,\n\t\tdata.Description,\n\t\tdata.Keywords,\n\t\tdata.H1Tags,\n\t\tdata.Content,\n\t\timagesJSON,\n\t\tdata.HTTPStatusCode,\n\t\tdata.ResponseTimeMS,\n\t\tdata.CrawlTimestamp,\n\t)\n\treturn err\n}\n\n// FindByURL retrieves the extracted data for a specific URL from the database.\nfunc (r *ExtractedDataRepoImpl) FindByURL(ctx context.Context, url string) (*entity.ExtractedData, error) {\n\tquery := `\n\t\tSELECT id, url, title, description, keywords, h1_tags, content, images, http_status_code, response_time_ms, crawl_timestamp\n\t\tFROM extracted_data\n\t\tWHERE url = $1;\n\t`\n\trow := r.db.QueryRow(ctx, query, url)\n\n\tvar data entity.ExtractedData\n\tvar imagesJSON []byte\n\n\terr := row.Scan(\n\t\t\u0026data.ID,\n\t\t\u0026data.URL,\n\t\t\u0026data.Title,\n\t\t\u0026data.Description,\n\t\t\u0026data.Keywords,\n\t\t\u0026data.H1Tags,\n\t\t\u0026data.Content,\n\t\t\u0026imagesJSON,\n\t\t\u0026data.HTTPStatusCode,\n\t\t\u0026data.ResponseTimeMS,\n\t\t\u0026data.CrawlTimestamp,\n\t)\n\tif err != nil {\n\t\treturn nil, err // pgx.ErrNoRows will be returned if not found\n\t}\n\n\tif err := json.Unmarshal(imagesJSON, \u0026data.Images); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn \u0026data, nil\n}\n",
  "internal/adapter/postgres/failed_url_impl.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\" // Added from attempted content\n\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\nconst (\n\tmaxRetries     = 5             // Adopted from attempted content\n\tinitialBackoff = 5 * time.Second // Adopted from attempted content\n)\n\n// FailedURLRepoImpl provides a concrete implementation for the FailedURLRepository interface using PostgreSQL.\ntype FailedURLRepoImpl struct {\n\tdb *pgxpool.Pool\n}\n\n// NewFailedURLRepo creates a new instance of FailedURLRepoImpl.\nfunc NewFailedURLRepo(db *pgxpool.Pool) *FailedURLRepoImpl {\n\treturn \u0026FailedURLRepoImpl{db: db}\n}\n\n// SaveOrUpdate creates or updates a record for a failed URL.\n// It increments the retry_count on conflict and calculates next_retry_at using exponential backoff with jitter.\nfunc (r *FailedURLRepoImpl) SaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error {\n\t// Adopted the sophisticated query from attempted content\n\tquery := `\n        INSERT INTO failed_urls (url, failure_reason, http_status_code, last_attempt_timestamp, retry_count, next_retry_at)\n        VALUES ($1, $2, $3, $4, 1, NOW() + ($5 * INTERVAL '1 second'))\n        ON CONFLICT (url) DO UPDATE\n        SET\n            failure_reason = EXCLUDED.failure_reason,\n            http_status_code = EXCLUDED.http_status_code,\n            last_attempt_timestamp = EXCLUDED.last_attempt_timestamp,\n            retry_count = failed_urls.retry_count + 1,\n            next_retry_at = CASE\n                WHEN failed_urls.retry_count + 1 \u003e= $6 THEN NULL\n                ELSE NOW() + (\n                    ($5 * pow(2, failed_urls.retry_count)) -- Exponential backoff\n                    * (1 + random() * 0.4 - 0.2)           -- Jitter +/- 20%\n                ) * INTERVAL '1 second'\n            END;\n    `\n\t_, err := r.db.Exec(ctx, query,\n\t\tfailedURL.URL,\n\t\tfailedURL.FailureReason,\n\t\tfailedURL.HTTPStatusCode,\n\t\tfailedURL.LastAttemptTimestamp,\n\t\tinitialBackoff.Seconds(), // Use constant\n\t\tmaxRetries,               // Use constant\n\t)\n\n\tif err != nil { // Adopted improved error wrapping\n\t\treturn fmt.Errorf(\"failed to save or update failed URL: %w\", err)\n\t}\n\treturn nil\n}\n\n// FindRetryable retrieves a batch of URLs that are due for a retry.\nfunc (r *FailedURLRepoImpl) FindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error) {\n\tquery := `\n        SELECT id, url, failure_reason, http_status_code, last_attempt_timestamp, retry_count, next_retry_at\n        FROM failed_urls\n        WHERE next_retry_at IS NOT NULL AND next_retry_at \u003c= NOW() -- Added IS NOT NULL from attempted\n        ORDER BY next_retry_at\n        LIMIT $1;\n    `\n\trows, err := r.db.Query(ctx, query, limit)\n\tif err != nil { // Adopted improved error wrapping\n\t\treturn nil, fmt.Errorf(\"failed to query retryable URLs: %w\", err)\n\t}\n\tdefer rows.Close()\n\n\tvar failedURLs []*entity.FailedURL // Renamed from 'urls' for clarity\n\tfor rows.Next() {\n\t\tvar fu entity.FailedURL // Renamed from 'u' for clarity\n\t\tif err := rows.Scan(\n\t\t\t\u0026fu.ID,\n\t\t\t\u0026fu.URL,\n\t\t\t\u0026fu.FailureReason,\n\t\t\t\u0026fu.HTTPStatusCode,\n\t\t\t\u0026fu.LastAttemptTimestamp,\n\t\t\t\u0026fu.RetryCount,\n\t\t\t\u0026fu.NextRetryAt,\n\t\t); err != nil { // Adopted improved error wrapping\n\t\t\treturn nil, fmt.Errorf(\"failed to scan retryable URL: %w\", err)\n\t\t}\n\t\tfailedURLs = append(failedURLs, \u0026fu)\n\t}\n\n\tif err := rows.Err(); err != nil { // Adopted improved error wrapping\n\t\treturn nil, fmt.Errorf(\"error after iterating over retryable URLs: %w\", err)\n\t}\n\n\treturn failedURLs, nil\n}\n\n// Delete removes a failed URL record, typically after a successful crawl.\nfunc (r *FailedURLRepoImpl) Delete(ctx context.Context, url string) error {\n\tquery := `DELETE FROM failed_urls WHERE url = $1;`\n\tcmdTag, err := r.db.Exec(ctx, query, url) // Adopted cmdTag from attempted\n\tif err != nil {                           // Adopted improved error wrapping\n\t\treturn fmt.Errorf(\"failed to delete failed URL: %w\", err)\n\t}\n\tif cmdTag.RowsAffected() == 0 { // Adopted from attempted\n\t\t// This is not necessarily an error, could just mean the URL was never in the failed table.\n\t\t// For strictness, one could return pgx.ErrNoRows, but for this use case, it's fine.\n\t}\n\treturn nil\n}\n",
  "internal/adapter/redis/queue_impl.go": "package redis\n\nimport (\n\t\"context\"\n\t\"github.com/redis/go-redis/v9\"\n)\n\nconst crawlQueueKey = \"crawler:queue\"\n\n// QueueRepoImpl provides a concrete implementation for the QueueRepository interface using Redis Lists.\ntype QueueRepoImpl struct {\n\tclient *redis.Client\n}\n\n// NewQueueRepo creates a new instance of QueueRepoImpl.\nfunc NewQueueRepo(client *redis.Client) *QueueRepoImpl {\n\treturn \u0026QueueRepoImpl{client: client}\n}\n\n// Push adds a URL to the left side of the Redis list (acting as a queue).\nfunc (r *QueueRepoImpl) Push(ctx context.Context, url string) error {\n\treturn r.client.LPush(ctx, crawlQueueKey, url).Err()\n}\n\n// Pop removes and returns a URL from the right side of the Redis list (acting as a queue).\n// It is a blocking operation if the list is empty, but we can add a timeout.\n// For simplicity, we use RPop which returns redis.Nil error if empty.\nfunc (r *QueueRepoImpl) Pop(ctx context.Context) (string, error) {\n\treturn r.client.RPop(ctx, crawlQueueKey).Result()\n}\n\n// Size returns the current number of items in the queue.\nfunc (r *QueueRepoImpl) Size(ctx context.Context) (int64, error) {\n\treturn r.client.LLen(ctx, crawlQueueKey).Result()\n}\n",
  "internal/adapter/redis/visited_impl.go": "package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/crawler-service/pkg/utils\"\n\t\"time\"\n)\n\nconst visitedURLPrefix = \"visited:\"\n\n// VisitedRepoImpl provides a concrete implementation for the VisitedRepository interface using Redis.\ntype VisitedRepoImpl struct {\n\tclient *redis.Client\n}\n\n// NewVisitedRepo creates a new instance of VisitedRepoImpl.\nfunc NewVisitedRepo(client *redis.Client) *VisitedRepoImpl {\n\treturn \u0026VisitedRepoImpl{client: client}\n}\n\n// generateKey creates a consistent Redis key for a given URL by hashing it.\nfunc (r *VisitedRepoImpl) generateKey(url string) string {\n\treturn fmt.Sprintf(\"%s%s\", visitedURLPrefix, utils.HashURL(url))\n}\n\n// MarkVisited marks a URL as visited by setting a key in Redis with a specific expiry time.\nfunc (r *VisitedRepoImpl) MarkVisited(ctx context.Context, url string, expiry time.Duration) error {\n\tkey := r.generateKey(url)\n\t// SETEX is atomic and sets the key with an expiry.\n\treturn r.client.SetEX(ctx, key, \"1\", expiry).Err()\n}\n\n// IsVisited checks if a URL has been visited recently by checking for the existence of its key.\nfunc (r *VisitedRepoImpl) IsVisited(ctx context.Context, url string) (bool, error) {\n\tkey := r.generateKey(url)\n\t// EXISTS returns 1 if the key exists, 0 otherwise.\n\tval, err := r.client.Exists(ctx, key).Result()\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn val == 1, nil\n}\n\n// RemoveVisited removes a URL from the visited set, used for force_crawl.\nfunc (r *VisitedRepoImpl) RemoveVisited(ctx context.Context, url string) error {\n\tkey := r.generateKey(url)\n\t// DEL removes the key.\n\treturn r.client.Del(ctx, key).Err()\n}\n",
  "internal/delivery/http/handler.go": "package handler\n\nimport (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"net/url\"\n\n\t\"github.com/user/crawler-service/internal/delivery/http/request\"\n\t\"github.com/user/crawler-service/internal/delivery/http/response\"\n\t\"github.com/user/crawler-service/internal/usecase\"\n)\n\ntype Handler struct {\n\turlManager usecase.URLManager\n}\n\nfunc NewHandler(urlManager usecase.URLManager) *Handler {\n\treturn \u0026Handler{\n\t\turlManager: urlManager,\n\t}\n}\n\nfunc (h *Handler) HandleSubmitCrawl(w http.ResponseWriter, r *http.Request) {\n\tif r.Method != http.MethodPost {\n\t\th.writeJSONError(w, \"Method not allowed\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\tvar req request.SubmitCrawlRequest\n\tif err := json.NewDecoder(r.Body).Decode(\u0026req); err != nil {\n\t\th.writeJSONError(w, \"Invalid request body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif _, err := url.ParseRequestURI(req.URL); err != nil {\n\t\th.writeJSONError(w, \"Invalid URL format\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tcrawlID, err := h.urlManager.Submit(r.Context(), req.URL, req.ForceCrawl)\n\tif err != nil {\n\t\tif errors.Is(err, usecase.ErrURLRecentlyCrawled) {\n\t\t\th.writeJSONError(w, err.Error(), http.StatusConflict)\n\t\t\treturn\n\t\t}\n\t\tslog.Error(\"Failed to submit URL\", \"url\", req.URL, \"error\", err)\n\t\th.writeJSONError(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tresp := response.SubmitCrawlResponse{\n\t\tStatus:         \"success\",\n\t\tMessage:        \"URL submitted for crawling\",\n\t\tCrawlRequestID: crawlID,\n\t}\n\th.writeJSON(w, http.StatusAccepted, resp)\n}\n\nfunc (h *Handler) HandleGetCrawlStatus(w http.ResponseWriter, r *http.Request) {\n\tif r.Method != http.MethodGet {\n\t\th.writeJSONError(w, \"Method not allowed\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\trawURL := r.URL.Query().Get(\"url\")\n\tif rawURL == \"\" {\n\t\th.writeJSONError(w, \"URL query parameter is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif _, err := url.ParseRequestURI(rawURL); err != nil {\n\t\th.writeJSONError(w, \"Invalid URL format in query parameter\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tstatus, err := h.urlManager.GetStatus(r.Context(), rawURL)\n\tif err != nil {\n\t\tslog.Error(\"Failed to get crawl status\", \"url\", rawURL, \"error\", err)\n\t\th.writeJSONError(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tif status.CurrentStatus == \"not_found\" {\n\t\th.writeJSONError(w, \"Crawl status not found for the given URL\", http.StatusNotFound)\n\t\treturn\n\t}\n\n\tresp := response.CrawlStatusResponse{\n\t\tURL:                status.URL,\n\t\tCurrentStatus:      status.CurrentStatus,\n\t\tLastCrawlTimestamp: status.LastCrawlTimestamp,\n\t\tNextRetryAt:        status.NextRetryAt,\n\t\tFailureReason:      status.FailureReason,\n\t}\n\n\th.writeJSON(w, http.StatusOK, resp)\n}\n\nfunc (h *Handler) HandleHealthCheck(w http.ResponseWriter, r *http.Request) {\n\th.writeJSON(w, http.StatusOK, map[string]string{\"status\": \"ok\"})\n}\n\nfunc (h *Handler) writeJSON(w http.ResponseWriter, status int, data interface{}) {\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tw.WriteHeader(status)\n\tif err := json.NewEncoder(w).Encode(data); err != nil {\n\t\tslog.Error(\"Failed to write JSON response\", \"error\", err)\n\t}\n}\n\nfunc (h *Handler) writeJSONError(w http.ResponseWriter, message string, status int) {\n\th.writeJSON(w, status, map[string]string{\"error\": message})\n}\n",
  "internal/delivery/http/middleware/logging.go": "package middleware\n\nimport (\n\t\"log/slog\"\n\t\"net/http\"\n\t\"time\"\n)\n\nfunc Logging(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tstart := time.Now()\n\t\tnext.ServeHTTP(w, r)\n\t\tslog.Info(\"HTTP Request\",\n\t\t\t\"method\", r.Method,\n\t\t\t\"path\", r.URL.Path,\n\t\t\t\"duration_ms\", time.Since(start).Milliseconds(),\n\t\t\t\"remote_addr\", r.RemoteAddr,\n\t\t)\n\t})\n}\n",
  "internal/delivery/http/middleware/metrics.go": "package middleware\n\nimport (\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/user/crawler-service/pkg/metrics\"\n)\n\ntype responseWriter struct {\n\thttp.ResponseWriter\n\tstatusCode int\n}\n\nfunc newResponseWriter(w http.ResponseWriter) *responseWriter {\n\treturn \u0026responseWriter{w, http.StatusOK}\n}\n\nfunc (rw *responseWriter) WriteHeader(code int) {\n\trw.statusCode = code\n\trw.ResponseWriter.WriteHeader(code)\n}\n\nfunc Metrics(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tstart := time.Now()\n\t\trw := newResponseWriter(w)\n\t\tnext.ServeHTTP(rw, r)\n\t\tduration := time.Since(start)\n\n\t\tmetrics.HTTPRequestDuration.WithLabelValues(r.Method, r.URL.Path, strconv.Itoa(rw.statusCode)).Observe(duration.Seconds())\n\t\tmetrics.HTTPRequestsTotal.WithLabelValues(r.Method, r.URL.Path, strconv.Itoa(rw.statusCode)).Inc()\n\t})\n}\n",
  "internal/delivery/http/request.go": "package request\n\ntype SubmitCrawlRequest struct {\n\tURL        string `json:\"url\"`\n\tForceCrawl bool   `json:\"force_crawl\"`\n\tCrawlMode  string `json:\"crawl_mode\"` // \"respectful\" or \"sneaky\" - not used in this step\n}\n",
  "internal/delivery/http/response.go": "package response\n\nimport \"time\"\n\ntype SubmitCrawlResponse struct {\n\tStatus         string `json:\"status\"`\n\tMessage        string `json:\"message\"`\n\tCrawlRequestID string `json:\"crawl_request_id\"`\n}\n\n// CrawlStatusResponse is a DTO for crawl status, mirroring entity.CrawlStatus\ntype CrawlStatusResponse struct {\n\tURL                string     `json:\"url\"`\n\tCurrentStatus      string     `json:\"current_status\"` // \"pending\", \"crawling\", \"completed\", \"failed\"\n\tLastCrawlTimestamp *time.Time `json:\"last_crawl_timestamp,omitempty\"`\n\tNextRetryAt        *time.Time `json:\"next_retry_at,omitempty\"`\n\tFailureReason      string     `json:\"failure_reason,omitempty\"`\n}\n",
  "internal/delivery/http/router.go": "package router\n\nimport (\n\t\"net/http\"\n\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/user/crawler-service/internal/delivery/http/handler\"\n\t\"github.com/user/crawler-service/internal/delivery/http/middleware\"\n)\n\nfunc New(h *handler.Handler) http.Handler {\n\tmux := http.NewServeMux()\n\n\tmux.HandleFunc(\"GET /api/health\", h.HandleHealthCheck)\n\tmux.HandleFunc(\"POST /api/crawl\", h.HandleSubmitCrawl)\n\tmux.HandleFunc(\"GET /api/status\", h.HandleGetCrawlStatus)\n\n\t// Prometheus metrics endpoint\n\tmux.Handle(\"/metrics\", promhttp.Handler())\n\n\t// Apply middlewares\n\tvar chainedHandler http.Handler = mux\n\tchainedHandler = middleware.Metrics(chainedHandler)\n\tchainedHandler = middleware.Logging(chainedHandler)\n\n\treturn chainedHandler\n}\n",
  "internal/entity/crawl_status.go": "package entity\n\nimport \"time\"\n\ntype CrawlStatus struct {\n\tURL                string\n\tCurrentStatus      string // \"pending\", \"crawling\", \"completed\", \"failed\", \"not_found\"\n\tLastCrawlTimestamp *time.Time\n\tNextRetryAt        *time.Time\n\tFailureReason      string\n}\n",
  "internal/entity/extracted_data.go": "package entity\n\nimport \"time\"\n\n// ImageInfo represents the structured data for an image extracted from a page.\ntype ImageInfo struct {\n\tSrc     string `json:\"src\"`\n\tAlt     string `json:\"alt\"`\n\tDataSrc string `json:\"data_src,omitempty\"` // For lazy-loaded images\n}\n\n// ExtractedData mirrors the `extracted_data` PostgreSQL table schema.\ntype ExtractedData struct {\n\tID               int64\n\tURL              string\n\tTitle            string\n\tDescription      string\n\tKeywords         []string\n\tH1Tags           []string\n\tContent          string\n\tImages           []ImageInfo // Stored as JSONB in PostgreSQL\n\tCrawlTimestamp   time.Time\n\tHTTPStatusCode   int\n\tResponseTimeMS   int\n}\n",
  "internal/entity/failed_url.go": "package entity\n\nimport \"time\"\n\n// FailedURL mirrors the `failed_urls` PostgreSQL table schema.\ntype FailedURL struct {\n\tID                   int64\n\tURL                  string\n\tFailureReason        string\n\tHTTPStatusCode       int\n\tLastAttemptTimestamp time.Time\n\tRetryCount           int\n\tNextRetryAt          time.Time\n}\n",
  "internal/repository/crawler_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"errors\" // Added from attempted content\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// Adopted specific error definitions from attempted content\nvar (\n\tErrCrawlTimeout      = errors.New(\"crawl operation timed out\")\n\tErrNavigationFailed  = errors.New(\"navigation to URL failed\")\n\tErrExtractionFailed  = errors.New(\"data extraction failed\")\n\tErrContentRestricted = errors.New(\"content is restricted or requires authentication\")\n)\n\n// CrawlerRepository defines the interface for the actual crawling component.\ntype CrawlerRepository interface {\n\t// Crawl fetches the given URL, renders it, and extracts structured data.\n\t// The 'sneaky' flag indicates whether to use anti-bot evasion techniques.\n\tCrawl(ctx context.Context, url string, sneaky bool) (*entity.ExtractedData, error)\n}\n",
  "internal/repository/extracted_data_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// ExtractedDataRepository defines the interface for storing and retrieving extracted page data.\ntype ExtractedDataRepository interface {\n\t// Save stores the extracted data for a URL. If the URL already exists, it should be updated.\n\tSave(ctx context.Context, data *entity.ExtractedData) error\n\t// FindByURL retrieves the extracted data for a specific URL.\n\tFindByURL(ctx context.Context, url string) (*entity.ExtractedData, error)\n}\n",
  "internal/repository/failed_url_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"github.com/user/crawler-service/internal/entity\"\n)\n\n// FailedURLRepository defines the interface for managing URLs that failed to be crawled.\ntype FailedURLRepository interface {\n\t// SaveOrUpdate creates or updates a record for a failed URL.\n\tSaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error\n\t// FindRetryable retrieves a batch of URLs that are due for a retry.\n\tFindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error)\n\t// Delete removes a failed URL record, typically after a successful crawl.\n\tDelete(ctx context.Context, url string) error\n}\n",
  "internal/repository/queue_repo.go": "package repository\n\nimport \"context\"\n\n// QueueRepository defines the interface for a FIFO queue for URLs to be crawled.\ntype QueueRepository interface {\n\t// Push adds a URL to the end of the queue.\n\tPush(ctx context.Context, url string) error\n\t// Pop removes and returns a URL from the front of the queue.\n\tPop(ctx context.Context) (string, error)\n\t// Size returns the current number of items in the queue.\n\tSize(ctx context.Context) (int64, error)\n}\n",
  "internal/repository/visited_repo.go": "package repository\n\nimport (\n\t\"context\"\n\t\"time\"\n)\n\n// VisitedRepository defines the interface for deduplication of visited URLs.\ntype VisitedRepository interface {\n\t// MarkVisited marks a URL as visited with a specific expiry time.\n\tMarkVisited(ctx context.Context, url string, expiry time.Duration) error\n\t// IsVisited checks if a URL has been visited recently.\n\tIsVisited(ctx context.Context, url string) (bool, error)\n\t// RemoveVisited removes a URL from the visited set, used for force_crawl.\n\tRemoveVisited(ctx context.Context, url string) error\n}\n```",
  "internal/usecase/crawler_usecase.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\" // Added from attempted content\n\t\"log/slog\"\n\t\"net/url\" // Added from attempted content\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/crawler-service/internal/entity\"\n\t\"github.com/user/crawler-service/internal/repository\"\n\t\"github.com/user/crawler-service/pkg/metrics\" // Added from attempted content\n)\n\nconst (\n\t// These constants are now primarily used by the FailedURLRepoImpl for initial backoff,\n\t// but kept here for consistency if the use case needs to reference them.\n\tinitialBackoff = 5 * time.Second\n\tmaxRetries     = 5\n\tjitterFactor   = 0.2 // +/- 20%\n)\n\n// Crawler defines the interface for the core crawling process.\ntype Crawler interface {\n\tProcessURLFromQueue(ctx context.Context) error\n}\n\ntype crawlerUseCase struct {\n\tqueueRepo         repository.QueueRepository\n\tcrawlerRepo       repository.CrawlerRepository\n\textractedDataRepo repository.ExtractedDataRepository\n\tfailedURLRepo     repository.FailedURLRepository\n}\n\n// NewCrawlerUseCase creates a new instance of the crawler use case.\nfunc NewCrawlerUseCase(\n\tqueueRepo repository.QueueRepository,\n\tcrawlerRepo repository.CrawlerRepository,\n\textractedDataRepo repository.ExtractedDataRepository,\n\tfailedURLRepo repository.FailedURLRepository,\n) Crawler {\n\treturn \u0026crawlerUseCase{\n\t\tqueueRepo:         queueRepo,\n\t\tcrawlerRepo:       crawlerRepo,\n\t\textractedDataRepo: extractedDataRepo,\n\t\tfailedURLRepo:     failedURLRepo,\n\t}\n}\n\n// ProcessURLFromQueue fetches a single URL from the queue and processes it.\n// It handles success by saving data and failure by scheduling a retry.\nfunc (uc *crawlerUseCase) ProcessURLFromQueue(ctx context.Context) error {\n\turlToCrawl, err := uc.queueRepo.Pop(ctx) // Renamed variable from 'url'\n\tif err != nil {\n\t\tif errors.Is(err, redis.Nil) {\n\t\t\t// Queue is empty, which is a normal state.\n\t\t\treturn nil\n\t\t}\n\t\treturn fmt.Errorf(\"failed to pop URL from queue: %w\", err) // Adopted improved error wrapping\n\t}\n\n\tslog.Info(\"Processing URL from queue\", \"url\", urlToCrawl)\n\n\tstartTime := time.Now()\n\tparsedURL, _ := url.Parse(urlToCrawl) // Adopted from attempted content\n\tdomain := \"unknown\"\n\tif parsedURL != nil {\n\t\tdomain = parsedURL.Hostname()\n\t}\n\n\t// For now, we default to \"sneaky\" mode for robustness. This could be configurable per URL.\n\tconst useSneakyMode = true // Adopted from attempted content\n\textractedData, crawlErr := uc.crawlerRepo.Crawl(ctx, urlToCrawl, useSneakyMode)\n\n\tduration := time.Since(startTime)\n\tmetrics.CrawlDuration.WithLabelValues(domain).Observe(duration.Seconds()) // Adopted from attempted content\n\n\tif crawlErr != nil {\n\t\tslog.Error(\"Crawling failed for URL, scheduling retry\", \"url\", urlToCrawl, \"error\", crawlErr) // Changed log level to Error\n\t\treturn uc.handleCrawlFailure(ctx, urlToCrawl, crawlErr)\n\t}\n\n\tslog.Info(\"Crawling successful for URL, saving data\", \"url\", urlToCrawl, \"duration_ms\", duration.Milliseconds()) // Adopted from attempted content\n\treturn uc.handleCrawlSuccess(ctx, extractedData)\n}\n\nfunc (uc *crawlerUseCase) handleCrawlSuccess(ctx context.Context, data *entity.ExtractedData) error {\n\tmetrics.CrawlsTotal.WithLabelValues(\"success\", \"\").Inc() // Adopted from attempted content\n\n\tif err := uc.extractedDataRepo.Save(ctx, data); err != nil {\n\t\treturn fmt.Errorf(\"failed to save extracted data for %s: %w\", data.URL, err) // Adopted improved error wrapping\n\t}\n\n\t// If the URL was previously failed, remove it from the failed table.\n\tif err := uc.failedURLRepo.Delete(ctx, data.URL); err != nil {\n\t\t// This is not a critical error, just log it.\n\t\tslog.Warn(\"Failed to delete URL from failed_urls table after successful crawl\", \"url\", data.URL, \"error\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (uc *crawlerUseCase) handleCrawlFailure(ctx context.Context, url string, crawlErr error) error {\n\terrorType := \"unknown\" // Adopted from attempted content\n\tvar httpStatusCode int // Adopted from attempted content\n\tswitch {\n\tcase errors.Is(crawlErr, repository.ErrCrawlTimeout):\n\t\terrorType = \"timeout\"\n\tcase errors.Is(crawlErr, repository.ErrNavigationFailed):\n\t\terrorType = \"navigation\"\n\tcase errors.Is(crawlErr, repository.ErrExtractionFailed):\n\t\terrorType = \"extraction\"\n\tcase errors.Is(crawlErr, repository.ErrContentRestricted):\n\t\terrorType = \"restricted\"\n\t\t// Try to extract status code from error message for logging\n\t\tfmt.Sscanf(crawlErr.Error(), \"content is restricted or requires authentication: received status code %d\", \u0026httpStatusCode)\n\t}\n\tmetrics.CrawlsTotal.WithLabelValues(\"failure\", errorType).Inc() // Adopted from attempted content\n\n\tfailedURL := \u0026entity.FailedURL{\n\t\tURL:                  url,\n\t\tFailureReason:        crawlErr.Error(),\n\t\tHTTPStatusCode:       httpStatusCode,       // Adopted from attempted content\n\t\tLastAttemptTimestamp: time.Now(),           // Adopted from attempted content\n\t\t// NextRetryAt is now handled by the repository's SaveOrUpdate method\n\t}\n\n\tif err := uc.failedURLRepo.SaveOrUpdate(ctx, failedURL); err != nil {\n\t\treturn fmt.Errorf(\"failed to save or update failed URL record for %s: %w\", url, err) // Adopted improved error wrapping\n\t}\n\n\treturn nil\n}\n",
  "internal/usecase/url_manager_usecase.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/user/crawler-service/internal/entity\"\n\t\"github.com/user/crawler-service/internal/repository\"\n\t\"github.com/user/crawler-service/pkg/utils\"\n)\n\nvar ErrURLRecentlyCrawled = errors.New(\"url has been crawled recently\")\n\nconst deduplicationExpiry = 48 * time.Hour // 2 days\n\n// URLManager defines the interface for submitting and checking URLs.\ntype URLManager interface {\n\tSubmit(ctx context.Context, url string, force bool) (string, error)\n\tGetStatus(ctx context.Context, url string) (*entity.CrawlStatus, error)\n}\n\ntype urlManagerUseCase struct {\n\tvisitedRepo       repository.VisitedRepository\n\tqueueRepo         repository.QueueRepository\n\textractedDataRepo repository.ExtractedDataRepository\n\tfailedURLRepo     repository.FailedURLRepository\n}\n\n// NewURLManager creates a new URLManager use case.\nfunc NewURLManager(\n\tvisitedRepo repository.VisitedRepository,\n\tqueueRepo repository.QueueRepository,\n\textractedDataRepo repository.ExtractedDataRepository,\n\tfailedURLRepo repository.FailedURLRepository,\n) URLManager {\n\treturn \u0026urlManagerUseCase{\n\t\tvisitedRepo:       visitedRepo,\n\t\tqueueRepo:         queueRepo,\n\t\textractedDataRepo: extractedDataRepo,\n\t\tfailedURLRepo:     failedURLRepo,\n\t}\n}\n\nfunc (uc *urlManagerUseCase) Submit(ctx context.Context, url string, force bool) (string, error) {\n\tif !force {\n\t\tvisited, err := uc.visitedRepo.IsVisited(ctx, url)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tif visited {\n\t\t\treturn \"\", ErrURLRecentlyCrawled\n\t\t}\n\t} else {\n\t\t// If forcing, remove from visited to allow re-queuing immediately.\n\t\tif err := uc.visitedRepo.RemoveVisited(ctx, url); err != nil {\n\t\t\tslog.Warn(\"Failed to remove visited key for force crawl\", \"url\", url, \"error\", err)\n\t\t\t// Continue anyway, as this is not a critical failure\n\t\t}\n\t}\n\n\tif err := uc.queueRepo.Push(ctx, url); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Mark as visited to prevent re-queuing from other sources.\n\tif err := uc.visitedRepo.MarkVisited(ctx, url, deduplicationExpiry); err != nil {\n\t\t// Log the error but don't fail the submission, as it's already queued.\n\t\tslog.Error(\"Failed to mark URL as visited after queueing\", \"url\", url, \"error\", err)\n\t}\n\n\treturn utils.HashURL(url), nil\n}\n\nfunc (uc *urlManagerUseCase) GetStatus(ctx context.Context, url string) (*entity.CrawlStatus, error) {\n\t// 1. Check if successfully extracted\n\tdata, err := uc.extractedDataRepo.FindByURL(ctx, url)\n\tif err == nil \u0026\u0026 data != nil {\n\t\treturn \u0026entity.CrawlStatus{\n\t\t\tURL:                url,\n\t\t\tCurrentStatus:      \"completed\",\n\t\t\tLastCrawlTimestamp: \u0026data.CrawlTimestamp,\n\t\t}, nil\n\t}\n\tif err != nil \u0026\u0026 !errors.Is(err, pgx.ErrNoRows) {\n\t\treturn nil, err // Actual DB error\n\t}\n\n\t// 2. Check if it's in the failed table\n\t// A proper implementation would have a FindByURL method on the failedURLRepo.\n\t// For now, we'll assume this check is part of a more complete repo.\n\t// Let's add a placeholder for this logic.\n\t// For this step, we'll skip the failed check as the repo doesn't have FindByURL.\n\tfailedURL, err := uc.failedURLRepo.FindByURL(ctx, url) // Assuming this method exists now\n\tif err == nil \u0026\u0026 failedURL != nil {\n\t\tstatus := \"failed\"\n\t\tif failedURL.NextRetryAt.After(time.Now()) {\n\t\t\tstatus = \"retrying\"\n\t\t}\n\t\treturn \u0026entity.CrawlStatus{\n\t\t\tURL:           url,\n\t\t\tCurrentStatus: status,\n\t\t\tNextRetryAt:   \u0026failedURL.NextRetryAt,\n\t\t\tFailureReason: failedURL.FailureReason,\n\t\t}, nil\n\t}\n\tif err != nil \u0026\u0026 !errors.Is(err, pgx.ErrNoRows) {\n\t\treturn nil, err // Actual DB error\n\t}\n\n\t// 3. Check if it's \"pending\" (i.e., in the visited set but not completed or failed)\n\tvisited, err := uc.visitedRepo.IsVisited(ctx, url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif visited {\n\t\treturn \u0026entity.CrawlStatus{\n\t\t\tURL:           url,\n\t\t\tCurrentStatus: \"pending\",\n\t\t}, nil\n\t}\n\n\t// 4. If none of the above, it's not found\n\treturn \u0026entity.CrawlStatus{\n\t\tURL:           url,\n\t\tCurrentStatus: \"not_found\",\n\t}, nil\n}",
  "pkg/config/config.go": "package config\n\nimport (\n\t\"log\"\n\t\"os\"\n\t\"strconv\"\n\t\"time\"\n)\n\n// Config holds the application configuration.\ntype Config struct {\n\tServerPort string\n\tLogLevel   string\n\n\tPostgresHost     string\n\tPostgresPort     string\n\tPostgresUser     string\n\tPostgresPassword string\n\tPostgresDB       string\n\n\tRedisAddr     string\n\tRedisPassword string\n\tRedisDB       int\n\n\tMaxConcurrency int\n\tPageLoadTimeout time.Duration\n}\n\n// Load loads configuration from environment variables.\nfunc Load() *Config {\n\treturn \u0026Config{\n\t\tServerPort:       getEnv(\"SERVER_PORT\", \"8080\"),\n\t\tLogLevel:         getEnv(\"LOG_LEVEL\", \"info\"),\n\t\tPostgresHost:     getEnv(\"POSTGRES_HOST\", \"localhost\"),\n\t\tPostgresPort:     getEnv(\"POSTGRES_PORT\", \"5432\"),\n\t\tPostgresUser:     getEnv(\"POSTGRES_USER\", \"user\"),\n\t\tPostgresPassword: getEnv(\"POSTGRES_PASSWORD\", \"password\"),\n\t\tPostgresDB:       getEnv(\"POSTGRES_DB\", \"crawler\"),\n\t\tRedisAddr:        getEnv(\"REDIS_ADDR\", \"localhost:6379\"),\n\t\tRedisPassword:    getEnv(\"REDIS_PASSWORD\", \"\"),\n\t\tRedisDB:          getEnvAsInt(\"REDIS_DB\", 0),\n\t\tMaxConcurrency:   getEnvAsInt(\"MAX_CONCURRENCY\", 10),\n\t\tPageLoadTimeout:  getEnvAsDuration(\"PAGE_LOAD_TIMEOUT_SECONDS\", 60) * time.Second,\n\t}\n}\n\nfunc getEnv(key, fallback string) string {\n\tif value, ok := os.LookupEnv(key); ok {\n\t\treturn value\n\t}\n\treturn fallback\n}\n\nfunc getEnvAsInt(key string, fallback int) int {\n\tvalueStr := getEnv(key, \"\")\n\tif value, err := strconv.Atoi(valueStr); err == nil {\n\t\treturn value\n\t}\n\treturn fallback\n}\n\nfunc getEnvAsDuration(key string, fallback int) time.Duration {\n\treturn time.Duration(getEnvAsInt(key, fallback))\n}\n",
  "pkg/logger/logger.go": "package logger\n\nimport (\n\t\"io\"\n\t\"log/slog\"\n)\n\n// Init initializes the global slog logger.\nfunc Init(writer io.Writer, level slog.Level) {\n\thandler := slog.NewJSONHandler(writer, \u0026slog.HandlerOptions{\n\t\tLevel: level,\n\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\t// Customize attribute keys for consistency if needed\n\t\t\tif a.Key == slog.TimeKey {\n\t\t\t\ta.Key = \"timestamp\"\n\t\t\t}\n\t\t\tif a.Key == slog.LevelKey {\n\t\t\t\ta.Key = \"level\"\n\t\t\t}\n\t\t\tif a.Key == slog.MessageKey {\n\t\t\t\ta.Key = \"message\"\n\t\t\t}\n\t\t\treturn a\n\t\t},\n\t})\n\tlogger := slog.New(handler)\n\tslog.SetDefault(logger)\n}\n",
  "pkg/metrics/metrics.go": "package metrics\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\nvar (\n\tHTTPRequestsTotal   *prometheus.CounterVec\n\tHTTPRequestDuration *prometheus.HistogramVec\n\tURLsInQueue         prometheus.Gauge\n\tCrawlsTotal         *prometheus.CounterVec   // Added from attempted content\n\tCrawlDuration       *prometheus.HistogramVec // Added from attempted content\n)\n\nfunc Init() {\n\tHTTPRequestsTotal = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"http_requests_total\",\n\t\t\tHelp: \"Total number of HTTP requests.\",\n\t\t},\n\t\t[]string{\"method\", \"path\", \"status\"}, // Changed 'code' to 'status' from attempted content\n\t)\n\n\tHTTPRequestDuration = promauto.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"http_request_duration_seconds\",\n\t\t\tHelp:    \"Duration of HTTP requests.\",\n\t\t\tBuckets: prometheus.DefBuckets,\n\t\t},\n\t\t[]string{\"method\", \"path\", \"status\"}, // Changed 'code' to 'status' from attempted content\n\t)\n\n\tURLsInQueue = promauto.NewGauge(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"urls_in_queue\", // Changed name from 'crawler_urls_in_queue' from attempted content\n\t\t\tHelp: \"Current number of URLs in the crawl queue.\",\n\t\t},\n\t)\n\n\tCrawlsTotal = promauto.NewCounterVec( // Added from attempted content\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"crawls_total\",\n\t\t\tHelp: \"Total number of crawl attempts.\",\n\t\t},\n\t\t[]string{\"status\", \"error_type\"}, // status: success, failure\n\t)\n\n\tCrawlDuration = promauto.NewHistogramVec( // Added from attempted content\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"crawl_duration_seconds\",\n\t\t\tHelp:    \"Duration of crawl operations.\",\n\t\t\tBuckets: []float64{1, 5, 10, 15, 30, 60, 120}, // Adopted specific buckets\n\t\t},\n\t\t[]string{\"domain\"},\n\t)\n}",
  "pkg/utils/url.go": "package utils\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/hex\"\n\t\"net/url\"\n)\n\n// HashURL creates a SHA256 hash of a URL string.\n// This is useful for creating consistent, safe keys for Redis.\nfunc HashURL(rawURL string) string {\n\th := sha256.New()\n\th.Write([]byte(rawURL))\n\treturn hex.EncodeToString(h.Sum(nil))\n}\n\n// ToAbsoluteURL converts a relative URL to an absolute URL given a base URL.\nfunc ToAbsoluteURL(base *url.URL, relative string) (string, error) {\n\trelURL, err := url.Parse(relative)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base.ResolveReference(relURL).String(), nil\n}\n```",
  "schema/001_init.sql": "CREATE TABLE IF NOT EXISTS extracted_data (\n    id BIGSERIAL PRIMARY KEY,\n    url TEXT NOT NULL UNIQUE,\n    title TEXT,\n    description TEXT,\n    keywords TEXT[],\n    h1_tags TEXT[],\n    content TEXT,\n    images JSONB,\n    crawl_timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    http_status_code INT,\n    response_time_ms INT\n);\n\nCREATE INDEX IF NOT EXISTS idx_extracted_data_url ON extracted_data(url);\nCREATE INDEX IF NOT EXISTS idx_extracted_data_crawl_timestamp ON extracted_data(crawl_timestamp);\n\nCREATE TABLE IF NOT EXISTS failed_urls (\n    id BIGSERIAL PRIMARY KEY,\n    url TEXT NOT NULL UNIQUE,\n    failure_reason TEXT,\n    http_status_code INT,\n    last_attempt_timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    retry_count INT NOT NULL DEFAULT 0,\n    next_retry_at TIMESTAMPTZ\n);\n\nCREATE INDEX IF NOT EXISTS idx_failed_urls_url ON failed_urls(url);\nCREATE INDEX IF NOT EXISTS idx_failed_urls_next_retry_at ON failed_urls(next_retry_at);\n\n"
}