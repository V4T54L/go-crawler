{
  "Dockerfile": "- File Path: Dockerfile\n- High-Level Purpose: Provides instructions for building a multi-stage Docker image for the `crawler-service` Go application.\n- Definitions in the File:\n  - Variables / Constants:\n    - `builder` (alias): Name for the first build stage using `golang:1.21-alpine`.\n    - `WORKDIR /app`: Sets the working directory inside the builder container.\n    - `CGO_ENABLED=0 GOOS=linux`: Environment variables for cross-compilation.\n    - `WORKDIR /root/`: Sets the working directory in the final `alpine:latest` image.\n    - `EXPOSE 8080`: Declares that the container listens on port 8080.\n  - Notable Patterns or Logic:\n    - Multi-stage build: Separates the build environment from the final runtime environment to create a smaller image.\n",
  "Makefile": "- File Path: Makefile\n- High-Level Purpose: Defines common build, run, and dependency management commands for the Go application.\n- Definitions in the File:\n  - Functions / Methods:\n    - `build`: Builds the Go application binary `crawler-service` and places it in `./bin`.\n    - `run`: Executes the compiled `crawler-service` binary.\n    - `tidy`: Runs `go mod tidy` to clean up Go module dependencies.\n",
  "cmd/api/main.go": "- File Path: cmd/api/main.go\n- High-Level Purpose: The main entry point for the `crawler-service` API. It initializes all necessary components including configuration, logging, metrics, database connections (PostgreSQL and Redis), sets up repositories and use cases, and starts the HTTP server with graceful shutdown.\n- Definitions in the File:\n  - Functions / Methods:\n    - `main()`: The application's entry point.\n      - Loads configuration.\n      - Initializes `slog` logger and Prometheus metrics.\n      - Establishes connections to PostgreSQL (`pgxpool`) and Redis (`redis.Client`).\n      - Initializes various repository implementations (Redis for `Visited` and `Queue`, PostgreSQL for `ExtractedData` and `FailedURL`).\n      - Initializes `ChromedpCrawler` as the `CrawlerRepository`.\n      - Sets up `URLManager` and `CrawlerUseCase` (though the latter is not actively started as a worker here).\n      - Configures and starts the HTTP server, registering API routes and a `/metrics` endpoint.\n      - Implements graceful shutdown for the HTTP server upon receiving interrupt signals.\n- Notable Patterns or Logic:\n  - Dependency Injection: Repositories and use cases are created and injected into their consumers.\n  - Graceful Shutdown: Uses `os.Signal` and `context.WithTimeout` to allow the HTTP server to shut down cleanly.\n  - Centralized Initialization: All major application components are initialized in `main`.\n",
  "go.mod": "- File Path: go.mod\n- High-Level Purpose: Manages the Go module dependencies for the `crawler-service` project, specifying direct and indirect requirements.\n- Definitions in the File:\n  - Variables / Constants:\n    - `module`: `github.com/user/crawler-service` (module path).\n    - `go`: `1.21` (Go language version).\n    - `require`: Lists direct dependencies including `chromedp`, `pgx/v5`, `prometheus/client_golang`, and `redis/go-redis/v9`.\n    - `require (indirect)`: Lists transitive dependencies.\n",
  "internal/adapter/chromedp_crawler/crawler_impl.go": "- File Path: internal/adapter/chromedp_crawler/crawler_impl.go\n- High-Level Purpose: Provides a concrete implementation of the `CrawlerRepository` interface using the `chromedp` library to automate a headless Chrome browser for web page crawling and initial data extraction.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ChromedpCrawler`: Struct that implements the `CrawlerRepository` interface, managing a pool of `chromedp` allocator contexts and a page load timeout.\n      - Fields: `allocatorPool` (*sync.Pool), `timeout` (time.Duration).\n  - Functions / Methods:\n    - `NewChromedpCrawler(maxConcurrency int, pageLoadTimeout time.Duration) (repository.CrawlerRepository, error)`: Public constructor function. Creates and returns a new `ChromedpCrawler` instance. It initializes a `sync.Pool` for `chromedp` execution allocators, configuring them for headless operation and pre-warming the pool to support concurrency.\n    - `Crawl(ctx context.Context, url string) (*entity.ExtractedData, error)`: Public method on `ChromedpCrawler`. Retrieves an allocator context from the pool, creates a new `chromedp` task context with a timeout, navigates to the given URL, and extracts the page title. It records the response time and returns a stubbed `entity.ExtractedData` with the URL, title, and crawl metadata.\n- Notable Patterns or Logic:\n  - Object Pool: Uses `sync.Pool` to manage and reuse `chromedp` allocator contexts, reducing overhead for concurrent crawling tasks.\n  - Headless Browser Automation: Leverages `chromedp` to control a headless Chrome instance for web page interaction and data extraction.",
  "internal/adapter/postgres/extracted_data_impl.go": "- File Path: internal/adapter/postgres/extracted_data_impl.go\n- High-Level Purpose: Provides a concrete implementation for storing and retrieving `ExtractedData` entities in a PostgreSQL database.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ExtractedDataRepoImpl`: Struct that implements the `ExtractedDataRepository` interface, holding a `pgxpool.Pool` database connection.\n      - Fields: `db` (*pgxpool.Pool).\n  - Functions / Methods:\n    - `NewExtractedDataRepo(db *pgxpool.Pool) *ExtractedDataRepoImpl`: Public constructor function. Creates and returns a new instance of `ExtractedDataRepoImpl`.\n    - `Save(ctx context.Context, data *entity.ExtractedData) error`: Public method on `ExtractedDataRepoImpl`. Inserts or updates an `entity.ExtractedData` record in the `extracted_data` table. It marshals the `Images` field to JSON before saving and uses `ON CONFLICT (url) DO UPDATE` for upsert functionality.\n    - `FindByURL(ctx context.Context, url string) (*entity.ExtractedData, error)`: Public method on `ExtractedDataRepoImpl`. Retrieves an `entity.ExtractedData` record from the `extracted_data` table based on the provided URL. It unmarshals the `images` JSON column back into the `entity.ImageInfo` slice.\n",
  "internal/adapter/postgres/failed_url_impl.go": "- File Path: internal/adapter/postgres/failed_url_impl.go\n- High-Level Purpose: Provides a concrete implementation for managing `FailedURL` entities in a PostgreSQL database, including saving, updating, retrieving retryable URLs, and deleting records.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `FailedURLRepoImpl`: Struct that implements the `FailedURLRepository` interface, holding a `pgxpool.Pool` database connection.\n      - Fields: `db` (*pgxpool.Pool).\n  - Functions / Methods:\n    - `NewFailedURLRepo(db *pgxpool.Pool) *FailedURLRepoImpl`: Public constructor function. Creates and returns a new instance of `FailedURLRepoImpl`.\n    - `SaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error`: Public method on `FailedURLRepoImpl`. Inserts a new `entity.FailedURL` record or updates an existing one based on the URL. On update, it increments the `retry_count` and updates other fields.\n    - `FindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error)`: Public method on `FailedURLRepoImpl`. Retrieves a batch of `entity.FailedURL` records that are scheduled for a retry (i.e., `next_retry_at` is in the past or present), ordered by `next_retry_at`.\n    - `Delete(ctx context.Context, url string) error`: Public method on `FailedURLRepoImpl`. Deletes a `failed_urls` record from the database for a given URL.\n",
  "internal/adapter/redis/queue_impl.go": "- File Path: internal/adapter/redis/queue_impl.go\n- High-Level Purpose: Implements the `QueueRepository` interface using Redis Lists, providing a FIFO queue for managing URLs to be crawled.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `QueueRepoImpl`: Struct that implements the `QueueRepository` interface, holding a `redis.Client` instance.\n      - Fields: `client` (*redis.Client).\n  - Functions / Methods:\n    - `NewQueueRepo(client *redis.Client) *QueueRepoImpl`: Public constructor function. Creates and returns a new instance of `QueueRepoImpl`.\n    - `Push(ctx context.Context, url string) error`: Public method on `QueueRepoImpl`. Adds a URL to the left side of the Redis list, effectively pushing it onto the queue.\n    - `Pop(ctx context.Context) (string, error)`: Public method on `QueueRepoImpl`. Removes and returns a URL from the right side of the Redis list, effectively popping it from the queue. Returns `redis.Nil` if the queue is empty.\n    - `Size(ctx context.Context) (int64, error)`: Public method on `QueueRepoImpl`. Returns the current number of elements in the Redis list (queue).\n  - Variables / Constants:\n    - `crawlQueueKey` (string): Constant string \"crawler:queue\" used as the key for the Redis list representing the crawl queue.\n",
  "internal/adapter/redis/visited_impl.go": "- File Path: internal/adapter/redis/visited_impl.go\n- High-Level Purpose: Implements the `VisitedRepository` interface using Redis, providing mechanisms for URL deduplication by marking and checking visited URLs.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `VisitedRepoImpl`: Struct that implements the `VisitedRepository` interface, holding a `redis.Client` instance.\n      - Fields: `client` (*redis.Client).\n  - Functions / Methods:\n    - `NewVisitedRepo(client *redis.Client) *VisitedRepoImpl`: Public constructor function. Creates and returns a new instance of `VisitedRepoImpl`.\n    - `generateKey(url string) string`: Internal method on `VisitedRepoImpl`. Generates a consistent Redis key for a given URL by hashing it and prepending a constant prefix.\n    - `MarkVisited(ctx context.Context, url string, expiry time.Duration) error`: Public method on `VisitedRepoImpl`. Marks a URL as visited in Redis by setting a key with a specified expiry time.\n    - `IsVisited(ctx context.Context, url string) (bool, error)`: Public method on `VisitedRepoImpl`. Checks if a URL has been marked as visited in Redis.\n    - `RemoveVisited(ctx context.Context, url string) error`: Public method on `VisitedRepoImpl`. Deletes the Redis key associated with a URL, effectively removing it from the visited set.\n  - Variables / Constants:\n    - `visitedURLPrefix` (string): Constant string \"visited:\" used as a prefix for Redis keys.\n",
  "internal/delivery/http/handler.go": "- File Path: internal/delivery/http/handler.go\n- High-Level Purpose: Implements HTTP handlers for the crawler service API, including endpoints for submitting URLs, checking crawl status, and health checks. It interacts with the `usecase.URLManager`.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Handler`: Struct holding a `usecase.URLManager` instance to interact with the application's business logic.\n      - Fields: `urlManager` (usecase.URLManager).\n  - Functions / Methods:\n    - `NewHandler(urlManager usecase.URLManager) *Handler`: Public constructor function. Creates and returns a new `Handler` instance.\n    - `HandleSubmitCrawl(w http.ResponseWriter, r *http.Request)`: Public method on `Handler`. Handles POST requests to submit a URL for crawling. It decodes the request, validates the URL, calls `urlManager.Submit`, and returns a JSON response. Handles `ErrURLRecentlyCrawled` with `HTTP 409 Conflict`.\n    - `HandleGetCrawlStatus(w http.ResponseWriter, r *http.Request)`: Public method on `Handler`. Handles GET requests to retrieve the crawl status of a URL. It extracts the URL from query parameters, calls `urlManager.GetStatus`, and returns a JSON response. Returns `HTTP 404 Not Found` if status is \"not_found\".\n    - `HandleHealthCheck(w http.ResponseWriter, r *http.Request)`: Public method on `Handler`. Handles GET requests for a simple health check, returning a \"status: ok\" JSON response.\n    - `writeJSON(w http.ResponseWriter, status int, data interface{})`: Internal method on `Handler`. Helper function to write a JSON response with a given HTTP status code and data.\n    - `writeJSONError(w http.ResponseWriter, message string, status int)`: Internal method on `Handler`. Helper function to write a JSON error response with a given message and HTTP status code.\n",
  "internal/delivery/http/middleware/logging.go": "- File Path: internal/delivery/http/middleware/logging.go\n- High-Level Purpose: Provides an HTTP middleware function for logging incoming requests with details like method, path, duration, and remote address.\n- Definitions in the File:\n  - Functions / Methods:\n    - `Logging(next http.Handler) http.Handler`: Public function. Returns an `http.Handler` middleware that logs details of each HTTP request (method, path, duration, remote address) using `slog` after the request has been processed.\n- Notable Patterns or Logic:\n  - HTTP Middleware: Implements the `http.Handler` interface pattern to wrap other handlers and add cross-cutting concerns (logging).\n",
  "internal/delivery/http/middleware/metrics.go": "- File Path: internal/delivery/http/middleware/metrics.go\n- High-Level Purpose: Provides an HTTP middleware function for collecting Prometheus metrics (request duration and total count) for incoming HTTP requests.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `responseWriter`: Internal struct that wraps `http.ResponseWriter` to capture the HTTP status code written by the downstream handler.\n      - Fields: `ResponseWriter` (http.ResponseWriter), `statusCode` (int).\n  - Functions / Methods:\n    - `newResponseWriter(w http.ResponseWriter) *responseWriter`: Internal constructor function. Creates a new `responseWriter` instance.\n    - `WriteHeader(code int)`: Method on `responseWriter`. Overrides the default `WriteHeader` to store the status code before calling the underlying `ResponseWriter`'s method.\n    - `Metrics(next http.Handler) http.Handler`: Public function. Returns an `http.Handler` middleware that records HTTP request duration and total count using `pkg/metrics` for each request, labeled by method, path, and status code.\n- Notable Patterns or Logic:\n  - HTTP Middleware: Implements the `http.Handler` interface pattern to wrap other handlers and add cross-cutting concerns (metrics).\n  - Response Writer Wrapper: Uses a custom `responseWriter` to capture the HTTP status code, which is not directly accessible from `http.ResponseWriter` after `ServeHTTP` returns.\n",
  "internal/delivery/http/request.go": "- File Path: internal/delivery/http/request.go\n- High-Level Purpose: Defines data transfer objects (DTOs) for HTTP requests, specifically for submitting a URL for crawling.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `SubmitCrawlRequest`: Struct for the request body when submitting a URL for crawling.\n      - Fields: `URL` (string), `ForceCrawl` (bool), `CrawlMode` (string, noted as not currently used).",
  "internal/delivery/http/response.go": "- File Path: internal/delivery/http/response.go\n- High-Level Purpose: Defines data transfer objects (DTOs) for HTTP responses related to submitting crawl requests and retrieving crawl status.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `SubmitCrawlResponse`: Struct for the response after submitting a URL for crawling.\n      - Fields: `Status` (string), `Message` (string), `CrawlRequestID` (string).\n    - `CrawlStatusResponse`: Struct for the response when querying the status of a crawl, mirroring `entity.CrawlStatus`.\n      - Fields: `URL` (string), `CurrentStatus` (string), `LastCrawlTimestamp` (*time.Time, omitempty), `NextRetryAt` (*time.Time, omitempty), `FailureReason` (string, omitempty).\n",
  "internal/delivery/http/router.go": "- File Path: internal/delivery/http/router.go\n- High-Level Purpose: Configures and sets up the HTTP router for the `crawler-service` API, mapping endpoints to handlers and applying global middlewares.\n- Definitions in the File:\n  - Functions / Methods:\n    - `New(h *handler.Handler) http.Handler`: Public function. Creates and returns a new `http.Handler` (router) for the application. It registers routes for health checks, submitting crawls, getting crawl status, and Prometheus metrics. It also applies `Metrics` and `Logging` middlewares to the entire router.\n- Notable Patterns or Logic:\n  - HTTP Router: Uses `http.ServeMux` to define API endpoints.\n  - Middleware Chaining: Applies multiple HTTP middlewares (`Metrics`, `Logging`) to the main router.\n",
  "internal/entity/crawl_status.go": "- File Path: internal/entity/crawl_status.go\n- High-Level Purpose: Defines the `CrawlStatus` struct, which represents the current state and relevant metadata of a URL's crawling process.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `CrawlStatus`: Struct representing the status of a URL crawl.\n      - Fields: `URL` (string), `CurrentStatus` (string, e.g., \"pending\", \"crawling\", \"completed\", \"failed\", \"not_found\"), `LastCrawlTimestamp` (*time.Time), `NextRetryAt` (*time.Time), `FailureReason` (string).\n",
  "internal/entity/extracted_data.go": "- File Path: internal/entity/extracted_data.go\n- High-Level Purpose: Defines Go data structures (structs) that represent the entities related to extracted web page content, mirroring the `extracted_data` database table.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ImageInfo`: Struct representing details of an image found on a page.\n      - Fields: `Src` (string), `Alt` (string), `DataSrc` (string, optional for lazy-loaded images).\n    - `ExtractedData`: Struct representing a complete record of data extracted from a crawled URL.\n      - Fields: `ID` (int64), `URL` (string), `Title` (string), `Description` (string), `Keywords` ([]string), `H1Tags` ([]string), `Content` (string), `Images` ([]ImageInfo), `CrawlTimestamp` (time.Time), `HTTPStatusCode` (int), `ResponseTimeMS` (int).\n",
  "internal/entity/failed_url.go": "- File Path: internal/entity/failed_url.go\n- High-Level Purpose: Defines a Go data structure (struct) that represents the entity for URLs that failed processing, mirroring the `failed_urls` database table.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `FailedURL`: Struct representing a record of a URL that failed to be processed.\n      - Fields: `ID` (int64), `URL` (string), `FailureReason` (string), `HTTPStatusCode` (int), `LastAttemptTimestamp` (time.Time), `RetryCount` (int), `NextRetryAt` (time.Time).",
  "internal/repository/crawler_repo.go": "- File Path: internal/repository/crawler_repo.go\n- High-Level Purpose: Defines the interface for the core web page crawling mechanism, specifying the contract for fetching a URL and extracting data.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `CrawlerRepository`: Interface defining the contract for crawling operations.\n      - Methods:\n        - `Crawl(ctx context.Context, url string) (*entity.ExtractedData, error)`: Fetches a URL and extracts data from it.\n",
  "internal/repository/extracted_data_repo.go": "- File Path: internal/repository/extracted_data_repo.go\n- High-Level Purpose: Defines the interface for a repository responsible for storing and retrieving extracted web page data.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ExtractedDataRepository`: Interface defining the contract for operations related to extracted web page data.\n      - Methods:\n        - `Save(ctx context.Context, data *entity.ExtractedData) error`: Stores or updates extracted data for a URL.\n        - `FindByURL(ctx context.Context, url string) (*entity.ExtractedData, error)`: Retrieves extracted data for a specific URL.",
  "internal/repository/failed_url_repo.go": "- File Path: internal/repository/failed_url_repo.go\n- High-Level Purpose: Defines the interface for a repository that manages URLs which failed to be crawled, including operations for persistence, retrieval of retryable URLs, and deletion.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `FailedURLRepository`: Interface defining the contract for operations related to failed URLs.\n      - Methods:\n        - `SaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error`: Creates or updates a record for a failed URL.\n        - `FindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error)`: Retrieves a batch of URLs that are due for a retry.\n        - `Delete(ctx context.Context, url string) error`: Removes a failed URL record.\n",
  "internal/repository/queue_repo.go": "- File Path: internal/repository/queue_repo.go\n- High-Level Purpose: Defines the interface for a generic FIFO queue repository, specifically for managing URLs that are awaiting processing.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `QueueRepository`: Interface defining the contract for basic queue operations.\n      - Methods:\n        - `Push(ctx context.Context, url string) error`: Adds an item (URL) to the queue.\n        - `Pop(ctx context.Context) (string, error)`: Removes and returns an item (URL) from the queue.\n        - `Size(ctx context.Context) (int64, error)`: Returns the current number of items in the queue.\n",
  "internal/repository/visited_repo.go": "- File Path: internal/repository/visited_repo.go\n- High-Level Purpose: Defines the interface for a repository that manages the state of visited URLs, primarily for deduplication purposes in a web crawling context.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `VisitedRepository`: Interface defining the contract for operations related to visited URLs.\n      - Methods:\n        - `MarkVisited(ctx context.Context, url string, expiry time.Duration) error`: Marks a URL as visited with a given expiration.\n        - `IsVisited(ctx context.Context, url string) (bool, error)`: Checks if a URL has been previously marked as visited.\n        - `RemoveVisited(ctx context.Context, url string) error`: Removes a URL from the visited set.\n",
  "internal/usecase/crawler_usecase.go": "- File Path: internal/usecase/crawler_usecase.go\n- High-Level Purpose: Implements the core business logic for processing URLs from a queue. It orchestrates the crawling process, handles saving extracted data, and manages failed crawl attempts by scheduling retries.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Crawler`: Interface defining the contract for the core crawling process.\n      - Methods:\n        - `ProcessURLFromQueue(ctx context.Context) error`: Fetches a URL from the queue and processes it.\n    - `crawlerUseCase`: Concrete implementation of the `Crawler` interface.\n      - Fields: `queueRepo` (repository.QueueRepository), `crawlerRepo` (repository.CrawlerRepository), `extractedDataRepo` (repository.ExtractedDataRepository), `failedURLRepo` (repository.FailedURLRepository).\n  - Functions / Methods:\n    - `NewCrawlerUseCase(queueRepo repository.QueueRepository, crawlerRepo repository.CrawlerRepository, extractedDataRepo repository.ExtractedDataRepository, failedURLRepo repository.FailedURLRepository) Crawler`: Public constructor function. Creates and returns a new `crawlerUseCase` instance, injecting required repository dependencies.\n    - `ProcessURLFromQueue(ctx context.Context) error`: Public method on `crawlerUseCase`. Pops a URL from the queue, attempts to crawl it, and then calls `handleCrawlSuccess` or `handleCrawlFailure` based on the outcome. Returns `nil` if the queue is empty.\n    - `handleCrawlSuccess(ctx context.Context, data *entity.ExtractedData) error`: Internal method on `crawlerUseCase`. Saves the successfully extracted data and attempts to delete the URL from the `failed_urls` table if it was previously there.\n    - `handleCrawlFailure(ctx context.Context, url string, crawlErr error) error`: Internal method on `crawlerUseCase`. Creates or updates a `FailedURL` record with the failure reason, allowing the `failedURLRepo` to manage retry logic.\n",
  "internal/usecase/url_manager_usecase.go": "- File Path: internal/usecase/url_manager_usecase.go\n- High-Level Purpose: Implements the core business logic for managing URLs, including submitting them for crawling, handling deduplication, and retrieving their crawl status. It orchestrates interactions with various repositories.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `URLManager`: Interface defining the contract for URL management operations.\n      - Methods:\n        - `Submit(ctx context.Context, url string, force bool) (string, error)`: Submits a URL for crawling, handling deduplication and queueing.\n        - `GetStatus(ctx context.Context, url string) (*entity.CrawlStatus, error)`: Retrieves the current crawl status of a URL.\n    - `urlManagerUseCase`: Concrete implementation of the `URLManager` interface.\n      - Fields: `visitedRepo` (repository.VisitedRepository), `queueRepo` (repository.QueueRepository), `extractedDataRepo` (repository.ExtractedDataRepository), `failedURLRepo` (repository.FailedURLRepository).\n  - Functions / Methods:\n    - `NewURLManager(visitedRepo repository.VisitedRepository, queueRepo repository.QueueRepository, extractedDataRepo repository.ExtractedDataRepository, failedURLRepo repository.FailedURLRepository) URLManager`: Public constructor function. Creates and returns a new `urlManagerUseCase` instance, injecting required repository dependencies.\n    - `Submit(ctx context.Context, url string, force bool) (string, error)`: Public method on `urlManagerUseCase`. Handles submitting a URL for crawling. It checks if the URL was recently visited (unless `force` is true), removes it from visited if `force` is true, pushes it to the queue, and marks it as visited. Returns a hashed `crawlID`.\n    - `GetStatus(ctx context.Context, url string) (*entity.CrawlStatus, error)`: Public method on `urlManagerUseCase`. Retrieves the crawl status of a URL. It first checks `extractedDataRepo` for completion, then `visitedRepo` for pending status, and finally defaults to \"not_found\".\n  - Variables / Constants:\n    - `ErrURLRecentlyCrawled` (error): Public error indicating a URL was recently crawled and `force_crawl` was false.\n    - `deduplicationExpiry` (time.Duration): Constant defining the duration (2 days) for which a URL is considered recently visited for deduplication.\n",
  "pkg/config/config.go": "- File Path: pkg/config/config.go\n- High-Level Purpose: Provides functionality to load and manage application configuration settings from environment variables, with sensible default values.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Config`: Struct holding all application configuration parameters.\n      - Fields: `ServerPort` (string), `LogLevel` (string), `PostgresHost` (string), `PostgresPort` (string), `PostgresUser` (string), `PostgresPassword` (string), `PostgresDB` (string), `RedisAddr` (string), `RedisPassword` (string), `RedisDB` (int), `MaxConcurrency` (int), `PageLoadTimeout` (time.Duration).\n  - Functions / Methods:\n    - `Load() *Config`: Public function that loads configuration values from environment variables (e.g., `SERVER_PORT`, `POSTGRES_HOST`) and returns a `Config` struct. Provides default values if environment variables are not set.\n    - `getEnv(key, fallback string) string`: Internal helper function to retrieve an environment variable's string value or a fallback.\n    - `getEnvAsInt(key string, fallback int) int`: Internal helper function to retrieve an environment variable's integer value or a fallback.\n    - `getEnvAsDuration(key string, fallback int) time.Duration`: Internal helper function to retrieve an environment variable's integer value and convert it to a `time.Duration` (assuming seconds), or a fallback.\n",
  "pkg/logger/logger.go": "- File Path: pkg/logger/logger.go\n- High-Level Purpose: Provides a utility function to initialize and configure the global structured logger (`slog`).\n- Definitions in the File:\n  - Functions / Methods:\n    - `Init(writer io.Writer, level slog.Level)`: Public function that initializes the default `slog` logger. It configures a JSON handler, sets the logging level, and customizes attribute keys (e.g., `time` to `timestamp`, `level` to `level`, `msg` to `message`).\n",
  "pkg/metrics/metrics.go": "- File Path: pkg/metrics/metrics.go\n- High-Level Purpose: Initializes and provides global Prometheus metrics for the application, including HTTP request counts, durations, and queue size.\n- Definitions in the File:\n  - Functions / Methods:\n    - `Init()`: Public function. Initializes the global Prometheus metrics by creating and registering `HTTPRequestsTotal`, `HTTPRequestDuration`, and `URLsInQueue`.\n  - Variables / Constants:\n    - `HTTPRequestsTotal` (*prometheus.CounterVec): Global counter for total HTTP requests, labeled by method, path, and status code.\n    - `HTTPRequestDuration` (*prometheus.HistogramVec): Global histogram for HTTP request durations, labeled by method, path, and status code.\n    - `URLsInQueue` (prometheus.Gauge): Global gauge for the current number of URLs in the crawl queue.\n",
  "pkg/utils/url.go": "- File Path: pkg/utils/url.go\n- High-Level Purpose: Contains utility functions for URL manipulation, specifically for hashing URLs and converting relative URLs to absolute ones.\n- Definitions in the File:\n  - Functions / Methods:\n    - `HashURL(rawURL string) string`: Public function that generates a SHA256 hash of a given URL string, useful for creating consistent keys.\n    - `ToAbsoluteURL(base *url.URL, relative string) (string, error)`: Public function that converts a relative URL string into an absolute URL string, given a base URL.\n",
  "schema/001_init.sql": "- File Path: schema/001_init.sql\n- High-Level Purpose: Defines the initial database schema for the `crawler-service`, including tables for storing extracted web data and tracking failed URL attempts.\n- Definitions in the File:\n  - Tables:\n    - `extracted_data`: Stores detailed information extracted from crawled web pages.\n      - Fields: `id` (BIGSERIAL PK), `url` (TEXT UNIQUE), `title` (TEXT), `description` (TEXT), `keywords` (TEXT[]), `h1_tags` (TEXT[]), `content` (TEXT), `images` (JSONB), `crawl_timestamp` (TIMESTAMPTZ DEFAULT NOW()), `http_status_code` (INT), `response_time_ms` (INT).\n    - `failed_urls`: Stores information about URLs that failed processing, including reasons and retry attempts.\n      - Fields: `id` (BIGSERIAL PK), `url` (TEXT UNIQUE), `failure_reason` (TEXT), `http_status_code` (INT), `last_attempt_timestamp` (TIMESTAMPTZ DEFAULT NOW()), `retry_count` (INT DEFAULT 0), `next_retry_at` (TIMESTAMPTZ).\n  - Indexes:\n    - `idx_extracted_data_url` on `extracted_data(url)`.\n    - `idx_extracted_data_crawl_timestamp` on `extracted_data(crawl_timestamp)`.\n    - `idx_failed_urls_url` on `failed_urls(url)`.\n    - `idx_failed_urls_next_retry_at` on `failed_urls(next_retry_at)`.\n"
}