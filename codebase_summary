{
  "Dockerfile": "- File Path: Dockerfile\n- High-Level Purpose: Provides instructions for building a multi-stage Docker image for the `crawler-service` Go application.\n- Definitions in the File:\n  - Variables / Constants:\n    - `builder` (alias): Name for the first build stage using `golang:1.21-alpine`.\n    - `WORKDIR /app`: Sets the working directory inside the builder container.\n    - `CGO_ENABLED=0 GOOS=linux`: Environment variables for cross-compilation.\n    - `WORKDIR /root/`: Sets the working directory in the final `alpine:latest` image.\n    - `EXPOSE 8080`: Declares that the container listens on port 8080.\n  - Notable Patterns or Logic:\n    - Multi-stage build: Separates the build environment from the final runtime environment to create a smaller image.\n",
  "Makefile": "- File Path: Makefile\n- High-Level Purpose: Defines common build, run, and dependency management commands for the Go application.\n- Definitions in the File:\n  - Functions / Methods:\n    - `build`: Builds the Go application binary `crawler-service` and places it in `./bin`.\n    - `run`: Executes the compiled `crawler-service` binary.\n    - `tidy`: Runs `go mod tidy` to clean up Go module dependencies.\n",
  "cmd/api/main.go": "- File Path: cmd/api/main.go\n- High-Level Purpose: This is the main entry point for the `crawler-service` API application. It initializes configuration, logging, metrics, establishes database connections (PostgreSQL and Redis), sets up repositories and use cases, starts the HTTP server, and manages graceful shutdown.\n- Definitions in the File:\n  - Functions / Methods:\n    - `main()`: Public function. Orchestrates the entire application lifecycle: loads configuration, initializes logger and metrics, connects to PostgreSQL and Redis, instantiates various repositories (Redis-based for visited/queue, Postgres-based for extracted data/failed URLs), initializes the `ChromedpCrawler` and `URLManager` use case. It then starts a background goroutine for queue metrics, configures and starts the HTTP server (including Prometheus metrics endpoint), and handles graceful shutdown upon receiving interrupt signals.\n    - `startQueueMetricsCollector(ctx context.Context, queueRepo repository.QueueRepository)`: Internal function. Runs as a goroutine, periodically polls the `queueRepo` for its size, and updates the `metrics.URLsInQueue` Prometheus gauge. It stops when the provided context is cancelled.\n- Variables / Constants:\n  - `cfg` (*config.Config): Module-level variable, holds the loaded application configuration.\n  - `logLevel` (slog.Level): Local variable in `main`, determines the logging verbosity.\n  - `pgConnString` (string): Local variable in `main`, the connection string for PostgreSQL.\n  - `dbPool` (*pgxpool.Pool): Local variable in `main`, the PostgreSQL connection pool.\n  - `redisClient` (*redis.Client): Local variable in `main`, the Redis client instance.\n  - `visitedRepo`, `queueRepo`, `extractedDataRepo`, `failedURLRepo`, `crawlerRepo`, `urlManager`: Local variables in `main`, instances of the respective repository and use case interfaces.\n  - `apiHandler` (*http_delivery.Handler): Local variable in `main`, the HTTP handler for API routes.\n  - `httpRouter` (http.Handler): Local variable in `main`, the configured HTTP router.\n  - `server` (*http.Server): Local variable in `main`, the HTTP server instance.\n- Notable Patterns or Logic:\n  - Dependency Injection: Repositories and use cases are instantiated and injected into their consumers.\n  - Graceful Shutdown: Uses `context.WithTimeout` and `signal.NotifyContext` to handle OS signals for a controlled server shutdown.\n  - Background Task: A dedicated goroutine `startQueueMetricsCollector` runs in the background to collect and expose queue-related metrics.\n  - Prometheus Integration: Exposes a `/metrics` endpoint for Prometheus scraping and updates custom application metrics.\n",
  "go.mod": "- File Path: go.mod\n- High-Level Purpose: Manages the Go module dependencies for the `crawler-service` project, specifying direct and indirect requirements.\n- Definitions in the File:\n  - Variables / Constants:\n    - `module`: `github.com/user/crawler-service` (module path).\n    - `go`: `1.21` (Go language version).\n    - `require`: Lists direct dependencies including `chromedp`, `pgx/v5`, `prometheus/client_golang`, and `redis/go-redis/v9`.\n    - `require (indirect)`: Lists transitive dependencies.\n",
  "internal/adapter/chromedp_crawler/crawler_impl.go": "- File Path: internal/adapter/chromedp_crawler/crawler_impl.go\n- High-Level Purpose: Implements the `CrawlerRepository` interface using `chromedp` to simulate a web browser, fetch URLs, render content, and extract structured data. It includes features like rate limiting, proxy rotation, and anti-bot evasion.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `domainRateLimiter`: Internal struct to enforce a minimum delay between requests to the same domain.\n      - Fields: `lastRequest` (map[string]time.Time), `delay` (time.Duration), `mu` (sync.Mutex).\n    - `ChromedpCrawler`: Struct that implements the `repository.CrawlerRepository` interface.\n      - Fields: `allocatorPool` (*sync.Pool), `timeout` (time.Duration), `proxies` ([]string), `proxyIndex` (int), `proxyMu` (sync.Mutex), `rateLimiter` (*domainRateLimiter).\n  - Functions / Methods:\n    - `newDomainRateLimiter(delay time.Duration) *domainRateLimiter`: Internal constructor. Creates a new `domainRateLimiter` with a specified delay.\n    - `Wait(domain string)`: Internal method on `domainRateLimiter`. Blocks the current goroutine if a request to the given `domain` was made within the `delay` period, ensuring rate limiting.\n    - `NewChromedpCrawler(maxConcurrency int, pageLoadTimeout time.Duration, proxies []string) (repository.CrawlerRepository, error)`: Public constructor. Creates and returns a new `ChromedpCrawler` instance. It initializes an `allocatorPool` for `chromedp` browser instances, pre-warming it with `maxConcurrency` allocators.\n    - `getNextProxy() string`: Internal method on `ChromedpCrawler`. Returns the next proxy from the configured list in a round-robin fashion, ensuring thread-safe access.\n    - `Crawl(ctx context.Context, rawURL string, sneaky bool) (*entity.ExtractedData, error)`: Public method on `ChromedpCrawler`. Navigates to the `rawURL` using `chromedp`. It applies domain-specific rate limiting, optionally uses a proxy, and can enable \"sneaky\" mode (random user-agent, viewport emulation). It extracts the page title, description, keywords, H1 tags, main content, and image information. It also captures the HTTP status code and final URL, handling various network and extraction errors.\n- Variables / Constants:\n  - `userAgents` ([]string): Global constant, a list of common user-agent strings used for browser emulation.\n  - `viewports` ([]struct{ W, H int }): Global constant, a list of screen dimensions used for viewport emulation.\n- Notable Patterns or Logic:\n  - Object Pool: Uses `sync.Pool` to manage `chromedp` allocator contexts, reducing overhead for creating new browser instances.\n  - Rate Limiting: Implements a per-domain rate limiter to prevent overwhelming target websites.\n  - Proxy Rotation: Cycles through a list of proxies for outgoing requests.\n  - Anti-Bot Evasion: Supports \"sneaky\" mode with random user-agents and viewport sizes to mimic real user behavior.\n  - Error Handling: Distinguishes and wraps specific crawl-related errors (timeout, navigation failure, extraction failure, content restriction).\n",
  "internal/adapter/postgres/extracted_data_impl.go": "- File Path: internal/adapter/postgres/extracted_data_impl.go\n- High-Level Purpose: Provides a concrete implementation for storing and retrieving `ExtractedData` entities in a PostgreSQL database.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ExtractedDataRepoImpl`: Struct that implements the `ExtractedDataRepository` interface, holding a `pgxpool.Pool` database connection.\n      - Fields: `db` (*pgxpool.Pool).\n  - Functions / Methods:\n    - `NewExtractedDataRepo(db *pgxpool.Pool) *ExtractedDataRepoImpl`: Public constructor function. Creates and returns a new instance of `ExtractedDataRepoImpl`.\n    - `Save(ctx context.Context, data *entity.ExtractedData) error`: Public method on `ExtractedDataRepoImpl`. Inserts or updates an `entity.ExtractedData` record in the `extracted_data` table. It marshals the `Images` field to JSON before saving and uses `ON CONFLICT (url) DO UPDATE` for upsert functionality.\n    - `FindByURL(ctx context.Context, url string) (*entity.ExtractedData, error)`: Public method on `ExtractedDataRepoImpl`. Retrieves an `entity.ExtractedData` record from the `extracted_data` table based on the provided URL. It unmarshals the `images` JSON column back into the `entity.ImageInfo` slice.\n",
  "internal/adapter/postgres/failed_url_impl.go": "- File Path: internal/adapter/postgres/failed_url_impl.go\n- High-Level Purpose: Provides a concrete implementation of the `FailedURLRepository` interface, using a PostgreSQL database to store, retrieve, and manage records of URLs that failed to be crawled, including retry logic.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `FailedURLRepoImpl`: Struct that implements the `repository.FailedURLRepository` interface.\n      - Fields: `db` (*pgxpool.Pool).\n  - Functions / Methods:\n    - `NewFailedURLRepo(db *pgxpool.Pool) *FailedURLRepoImpl`: Public constructor function. Creates and returns a new instance of `FailedURLRepoImpl`.\n    - `SaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error`: Public method on `FailedURLRepoImpl`. Inserts a new `failed_urls` record or updates an existing one based on the URL. On update, it increments the `retry_count` and calculates `next_retry_at` using an exponential backoff strategy with added jitter, up to a `maxRetries` limit. If `maxRetries` is reached, `next_retry_at` is set to `NULL`.\n    - `FindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error)`: Public method on `FailedURLRepoImpl`. Queries the `failed_urls` table to retrieve a batch of URLs that are due for a retry (i.e., `next_retry_at` is not NULL and is less than or equal to the current time), ordered by their `next_retry_at` timestamp.\n    - `Delete(ctx context.Context, url string) error`: Public method on `FailedURLRepoImpl`. Removes a specific URL record from the `failed_urls` table.\n- Variables / Constants:\n  - `maxRetries` (int): Constant, defines the maximum number of retry attempts for a failed URL (5).\n  - `initialBackoff` (time.Duration): Constant, defines the initial backoff duration for retries (5 seconds).\n- Notable Patterns or Logic:\n  - Database Repository: Provides an abstraction layer for interacting with the `failed_urls` table.\n  - Upsert Logic: Uses PostgreSQL's `ON CONFLICT (url) DO UPDATE` clause to handle both insertion and updating of failed URL records.\n  - Exponential Backoff with Jitter: Implements a sophisticated retry scheduling mechanism by calculating `next_retry_at` with increasing delays and randomness to avoid thundering herd problems.",
  "internal/adapter/redis/queue_impl.go": "- File Path: internal/adapter/redis/queue_impl.go\n- High-Level Purpose: Implements the `QueueRepository` interface using Redis Lists, providing a FIFO queue for managing URLs to be crawled.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `QueueRepoImpl`: Struct that implements the `QueueRepository` interface, holding a `redis.Client` instance.\n      - Fields: `client` (*redis.Client).\n  - Functions / Methods:\n    - `NewQueueRepo(client *redis.Client) *QueueRepoImpl`: Public constructor function. Creates and returns a new instance of `QueueRepoImpl`.\n    - `Push(ctx context.Context, url string) error`: Public method on `QueueRepoImpl`. Adds a URL to the left side of the Redis list, effectively pushing it onto the queue.\n    - `Pop(ctx context.Context) (string, error)`: Public method on `QueueRepoImpl`. Removes and returns a URL from the right side of the Redis list, effectively popping it from the queue. Returns `redis.Nil` if the queue is empty.\n    - `Size(ctx context.Context) (int64, error)`: Public method on `QueueRepoImpl`. Returns the current number of elements in the Redis list (queue).\n  - Variables / Constants:\n    - `crawlQueueKey` (string): Constant string \"crawler:queue\" used as the key for the Redis list representing the crawl queue.\n",
  "internal/adapter/redis/visited_impl.go": "- File Path: internal/adapter/redis/visited_impl.go\n- High-Level Purpose: Implements the `VisitedRepository` interface using Redis, providing mechanisms for URL deduplication by marking and checking visited URLs.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `VisitedRepoImpl`: Struct that implements the `VisitedRepository` interface, holding a `redis.Client` instance.\n      - Fields: `client` (*redis.Client).\n  - Functions / Methods:\n    - `NewVisitedRepo(client *redis.Client) *VisitedRepoImpl`: Public constructor function. Creates and returns a new instance of `VisitedRepoImpl`.\n    - `generateKey(url string) string`: Internal method on `VisitedRepoImpl`. Generates a consistent Redis key for a given URL by hashing it and prepending a constant prefix.\n    - `MarkVisited(ctx context.Context, url string, expiry time.Duration) error`: Public method on `VisitedRepoImpl`. Marks a URL as visited in Redis by setting a key with a specified expiry time.\n    - `IsVisited(ctx context.Context, url string) (bool, error)`: Public method on `VisitedRepoImpl`. Checks if a URL has been marked as visited in Redis.\n    - `RemoveVisited(ctx context.Context, url string) error`: Public method on `VisitedRepoImpl`. Deletes the Redis key associated with a URL, effectively removing it from the visited set.\n  - Variables / Constants:\n    - `visitedURLPrefix` (string): Constant string \"visited:\" used as a prefix for Redis keys.\n",
  "internal/delivery/http/handler.go": "- File Path: internal/delivery/http/handler.go\n- High-Level Purpose: Defines HTTP handlers for the API endpoints, responsible for receiving requests, validating input, interacting with the `URLManager` use case, and sending structured JSON responses.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Handler`: Struct holding the `URLManager` use case dependency.\n      - Fields: `urlManager` (usecase.URLManager).\n  - Functions / Methods:\n    - `NewHandler(urlManager usecase.URLManager) *Handler`: Public constructor. Creates and returns a new `Handler` instance.\n    - `HandleSubmitCrawl(w http.ResponseWriter, r *http.Request)`: Public method on `Handler`. Handles `POST /api/crawl` requests. It decodes the `SubmitCrawlRequest`, validates the URL, calls `urlManager.Submit`, and responds with a `SubmitCrawlResponse` or an appropriate HTTP error.\n    - `HandleGetCrawlStatus(w http.ResponseWriter, r *http.Request)`: Public method on `Handler`. Handles `GET /api/status` requests. It extracts the URL from query parameters, validates it, calls `urlManager.GetStatus`, and responds with a `CrawlStatusResponse` or an error (including `404 Not Found` if status is \"not_found\").\n    - `HandleHealthCheck(w http.ResponseWriter, r *http.Request)`: Public method on `Handler`. Handles `GET /api/health` requests, returning a simple \"ok\" status.\n    - `writeJSON(w http.ResponseWriter, status int, data interface{})`: Internal helper method on `Handler`. Writes a JSON response with a given HTTP status code and data.\n    - `writeJSONError(w http.ResponseWriter, message string, status int)`: Internal helper method on `Handler`. Writes a JSON error response with a given message and HTTP status code.\n- Notable Patterns or Logic:\n  - HTTP Handler: Implements standard `http.HandlerFunc` pattern for API endpoints.\n  - Request/Response DTOs: Uses specific request (`request.SubmitCrawlRequest`) and response (`response.SubmitCrawlResponse`, `response.CrawlStatusResponse`) structs for API communication.\n  - Error Handling: Distinguishes between client-side validation errors (e.g., invalid URL, method not allowed) and internal server errors, returning appropriate HTTP status codes.\n",
  "internal/delivery/http/middleware/logging.go": "- File Path: internal/delivery/http/middleware/logging.go\n- High-Level Purpose: Provides an HTTP middleware function for logging incoming requests with details like method, path, duration, and remote address.\n- Definitions in the File:\n  - Functions / Methods:\n    - `Logging(next http.Handler) http.Handler`: Public function. Returns an `http.Handler` middleware that logs details of each HTTP request (method, path, duration, remote address) using `slog` after the request has been processed.\n- Notable Patterns or Logic:\n  - HTTP Middleware: Implements the `http.Handler` interface pattern to wrap other handlers and add cross-cutting concerns (logging).\n",
  "internal/delivery/http/middleware/metrics.go": "- File Path: internal/delivery/http/middleware/metrics.go\n- High-Level Purpose: Provides an HTTP middleware function for collecting Prometheus metrics (request duration and total count) for incoming HTTP requests.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `responseWriter`: Internal struct that wraps `http.ResponseWriter` to capture the HTTP status code written by the downstream handler.\n      - Fields: `ResponseWriter` (http.ResponseWriter), `statusCode` (int).\n  - Functions / Methods:\n    - `newResponseWriter(w http.ResponseWriter) *responseWriter`: Internal constructor function. Creates a new `responseWriter` instance.\n    - `WriteHeader(code int)`: Method on `responseWriter`. Overrides the default `WriteHeader` to store the status code before calling the underlying `ResponseWriter`'s method.\n    - `Metrics(next http.Handler) http.Handler`: Public function. Returns an `http.Handler` middleware that records HTTP request duration and total count using `pkg/metrics` for each request, labeled by method, path, and status code.\n- Notable Patterns or Logic:\n  - HTTP Middleware: Implements the `http.Handler` interface pattern to wrap other handlers and add cross-cutting concerns (metrics).\n  - Response Writer Wrapper: Uses a custom `responseWriter` to capture the HTTP status code, which is not directly accessible from `http.ResponseWriter` after `ServeHTTP` returns.\n",
  "internal/delivery/http/request.go": "- File Path: internal/delivery/http/request.go\n- High-Level Purpose: Defines data transfer objects (DTOs) for HTTP requests, specifically for submitting a URL for crawling.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `SubmitCrawlRequest`: Struct for the request body when submitting a URL for crawling.\n      - Fields: `URL` (string), `ForceCrawl` (bool), `CrawlMode` (string, noted as not currently used).",
  "internal/delivery/http/response.go": "- File Path: internal/delivery/http/response.go\n- High-Level Purpose: Defines data transfer objects (DTOs) for HTTP responses related to submitting crawl requests and retrieving crawl status.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `SubmitCrawlResponse`: Struct for the response after submitting a URL for crawling.\n      - Fields: `Status` (string), `Message` (string), `CrawlRequestID` (string).\n    - `CrawlStatusResponse`: Struct for the response when querying the status of a crawl, mirroring `entity.CrawlStatus`.\n      - Fields: `URL` (string), `CurrentStatus` (string), `LastCrawlTimestamp` (*time.Time, omitempty), `NextRetryAt` (*time.Time, omitempty), `FailureReason` (string, omitempty).\n",
  "internal/delivery/http/router.go": "- File Path: internal/delivery/http/router.go\n- High-Level Purpose: Configures and returns the main HTTP router for the application, mapping API endpoints to their respective handlers and applying global HTTP middlewares.\n- Definitions in the File:\n  - Functions / Methods:\n    - `New(h *handler.Handler) http.Handler`: Public function. Creates a new `http.ServeMux`, registers routes for health checks (`GET /api/health`), submitting crawls (`POST /api/crawl`), and getting crawl status (`GET /api/status`) using the provided `handler.Handler`. It also registers a Prometheus `/metrics` endpoint and applies `middleware.Metrics` and `middleware.Logging` to all API routes.\n- Notable Patterns or Logic:\n  - HTTP Router: Uses `http.NewServeMux` for routing HTTP requests.\n  - Middleware Chaining: Applies `middleware.Metrics` and `middleware.Logging` to all incoming requests, ensuring consistent cross-cutting concerns like metrics collection and request logging.\n  - Metrics Endpoint: Exposes Prometheus metrics via the `/metrics` endpoint, allowing external monitoring.\n",
  "internal/entity/crawl_status.go": "- File Path: internal/entity/crawl_status.go\n- High-Level Purpose: Defines the `CrawlStatus` struct, which represents the current state and relevant metadata of a URL's crawling process.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `CrawlStatus`: Struct representing the status of a URL crawl.\n      - Fields: `URL` (string), `CurrentStatus` (string, e.g., \"pending\", \"crawling\", \"completed\", \"failed\", \"not_found\"), `LastCrawlTimestamp` (*time.Time), `NextRetryAt` (*time.Time), `FailureReason` (string).\n",
  "internal/entity/extracted_data.go": "- File Path: internal/entity/extracted_data.go\n- High-Level Purpose: Defines Go data structures (structs) that represent the entities related to extracted web page content, mirroring the `extracted_data` database table.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ImageInfo`: Struct representing details of an image found on a page.\n      - Fields: `Src` (string), `Alt` (string), `DataSrc` (string, optional for lazy-loaded images).\n    - `ExtractedData`: Struct representing a complete record of data extracted from a crawled URL.\n      - Fields: `ID` (int64), `URL` (string), `Title` (string), `Description` (string), `Keywords` ([]string), `H1Tags` ([]string), `Content` (string), `Images` ([]ImageInfo), `CrawlTimestamp` (time.Time), `HTTPStatusCode` (int), `ResponseTimeMS` (int).\n",
  "internal/entity/failed_url.go": "- File Path: internal/entity/failed_url.go\n- High-Level Purpose: Defines a Go data structure (struct) that represents the entity for URLs that failed processing, mirroring the `failed_urls` database table.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `FailedURL`: Struct representing a record of a URL that failed to be processed.\n      - Fields: `ID` (int64), `URL` (string), `FailureReason` (string), `HTTPStatusCode` (int), `LastAttemptTimestamp` (time.Time), `RetryCount` (int), `NextRetryAt` (time.Time).",
  "internal/repository/crawler_repo.go": "- File Path: internal/repository/crawler_repo.go\n- High-Level Purpose: Defines the interface for the web crawling component, outlining the contract for fetching and extracting data from URLs, and declares specific error types related to crawling operations.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `CrawlerRepository`: Interface defining the contract for a web crawling service.\n      - Methods:\n        - `Crawl(ctx context.Context, url string, sneaky bool) (*entity.ExtractedData, error)`: Fetches a URL, renders it, and extracts structured data. The `sneaky` flag enables anti-bot evasion.\n  - Variables / Constants:\n    - `ErrCrawlTimeout` (error): Public error, indicates a crawl operation timed out.\n    - `ErrNavigationFailed` (error): Public error, indicates navigation to a URL failed.\n    - `ErrExtractionFailed` (error): Public error, indicates data extraction failed.\n    - `ErrContentRestricted` (error): Public error, indicates content is restricted or requires authentication.\n",
  "internal/repository/extracted_data_repo.go": "- File Path: internal/repository/extracted_data_repo.go\n- High-Level Purpose: Defines the interface for a repository responsible for storing and retrieving extracted web page data.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ExtractedDataRepository`: Interface defining the contract for operations related to extracted web page data.\n      - Methods:\n        - `Save(ctx context.Context, data *entity.ExtractedData) error`: Stores or updates extracted data for a URL.\n        - `FindByURL(ctx context.Context, url string) (*entity.ExtractedData, error)`: Retrieves extracted data for a specific URL.",
  "internal/repository/failed_url_repo.go": "- File Path: internal/repository/failed_url_repo.go\n- High-Level Purpose: Defines the interface for a repository that manages URLs which failed to be crawled, including operations for persistence, retrieval of retryable URLs, and deletion.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `FailedURLRepository`: Interface defining the contract for operations related to failed URLs.\n      - Methods:\n        - `SaveOrUpdate(ctx context.Context, failedURL *entity.FailedURL) error`: Creates or updates a record for a failed URL.\n        - `FindRetryable(ctx context.Context, limit int) ([]*entity.FailedURL, error)`: Retrieves a batch of URLs that are due for a retry.\n        - `Delete(ctx context.Context, url string) error`: Removes a failed URL record.\n",
  "internal/repository/queue_repo.go": "- File Path: internal/repository/queue_repo.go\n- High-Level Purpose: Defines the interface for a generic FIFO queue repository, specifically for managing URLs that are awaiting processing.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `QueueRepository`: Interface defining the contract for basic queue operations.\n      - Methods:\n        - `Push(ctx context.Context, url string) error`: Adds an item (URL) to the queue.\n        - `Pop(ctx context.Context) (string, error)`: Removes and returns an item (URL) from the queue.\n        - `Size(ctx context.Context) (int64, error)`: Returns the current number of items in the queue.\n",
  "internal/repository/visited_repo.go": "- File Path: internal/repository/visited_repo.go\n- High-Level Purpose: Defines the interface for a repository that manages the state of visited URLs, primarily for deduplication purposes in a web crawling context.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `VisitedRepository`: Interface defining the contract for operations related to visited URLs.\n      - Methods:\n        - `MarkVisited(ctx context.Context, url string, expiry time.Duration) error`: Marks a URL as visited with a given expiration.\n        - `IsVisited(ctx context.Context, url string) (bool, error)`: Checks if a URL has been previously marked as visited.\n        - `RemoveVisited(ctx context.Context, url string) error`: Removes a URL from the visited set.\n",
  "internal/usecase/crawler_usecase.go": "- File Path: internal/usecase/crawler_usecase.go\n- High-Level Purpose: Implements the business logic for the core crawling process. It orchestrates fetching URLs from a queue, performing the actual crawl, saving successful results, and managing retries for failed attempts.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Crawler`: Interface defining the contract for the core crawling process.\n      - Methods:\n        - `ProcessURLFromQueue(ctx context.Context) error`: Fetches and processes a single URL from the queue.\n    - `crawlerUseCase`: Struct that implements the `Crawler` interface.\n      - Fields: `queueRepo` (repository.QueueRepository), `crawlerRepo` (repository.CrawlerRepository), `extractedDataRepo` (repository.ExtractedDataRepository), `failedURLRepo` (repository.FailedURLRepository).\n  - Functions / Methods:\n    - `NewCrawlerUseCase(queueRepo repository.QueueRepository, crawlerRepo repository.CrawlerRepository, extractedDataRepo repository.ExtractedDataRepository, failedURLRepo repository.FailedURLRepository) Crawler`: Public constructor. Creates and returns a new `crawlerUseCase` instance, injecting all necessary repository dependencies.\n    - `ProcessURLFromQueue(ctx context.Context) error`: Public method on `crawlerUseCase`. Pops a URL from the queue, attempts to crawl it using `crawlerRepo`, records crawl duration metrics, and then delegates to `handleCrawlSuccess` or `handleCrawlFailure` based on the outcome. It returns `nil` if the queue is empty.\n    - `handleCrawlSuccess(ctx context.Context, data *entity.ExtractedData) error`: Internal method on `crawlerUseCase`. Records success metrics, saves the `ExtractedData` to `extractedDataRepo`, and attempts to delete the URL from `failedURLRepo` if it was previously marked as failed.\n    - `handleCrawlFailure(ctx context.Context, url string, crawlErr error) error`: Internal method on `crawlerUseCase`. Records failure metrics (categorized by error type), creates an `entity.FailedURL` record with details about the failure, and calls `failedURLRepo.SaveOrUpdate` to persist it for potential retries.\n- Variables / Constants:\n  - `initialBackoff` (time.Duration): Constant, defines the initial backoff duration for retries (5 seconds).\n  - `maxRetries` (int): Constant, defines the maximum number of retry attempts (5).\n  - `jitterFactor` (float64): Constant, defines the factor for adding randomness to backoff calculations (0.2, i.e., +/- 20%).\n  - `useSneakyMode` (bool): Local constant in `ProcessURLFromQueue`, currently hardcoded to `true` for all crawls.\n- Notable Patterns or Logic:\n  - Use Case / Service Layer: Encapsulates the core business logic for crawling.\n  - Dependency Injection: All external dependencies (repositories) are injected through the constructor.\n  - Metrics Integration: Records `CrawlsTotal` (success/failure, error type) and `CrawlDuration` (per domain).\n  - Error Handling: Distinguishes between different types of crawl errors (timeout, navigation, extraction, restricted content) and handles them by scheduling retries.\n",
  "internal/usecase/url_manager_usecase.go": "- File Path: internal/usecase/url_manager_usecase.go\n- High-Level Purpose: Defines the `URLManager` interface and its implementation, `urlManagerUseCase`, which handles the business logic for submitting URLs for crawling and retrieving their current status. It orchestrates interactions with various repositories (visited, queue, extracted data, failed URLs).\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `URLManager`: Interface for managing URLs.\n      - Methods:\n        - `Submit(ctx context.Context, url string, force bool) (string, error)`: Submits a URL for crawling.\n        - `GetStatus(ctx context.Context, url string) (*entity.CrawlStatus, error)`: Retrieves the current crawl status of a URL.\n    - `urlManagerUseCase`: Struct implementing `URLManager`.\n      - Fields: `visitedRepo` (repository.VisitedRepository), `queueRepo` (repository.QueueRepository), `extractedDataRepo` (repository.ExtractedDataRepository), `failedURLRepo` (repository.FailedURLRepository).\n  - Functions / Methods:\n    - `NewURLManager(visitedRepo repository.VisitedRepository, queueRepo repository.QueueRepository, extractedDataRepo repository.ExtractedDataRepository, failedURLRepo repository.FailedURLRepository) URLManager`: Public constructor. Creates and returns a new `urlManagerUseCase` instance, injecting repository dependencies.\n    - `Submit(ctx context.Context, url string, force bool) (string, error)`: Public method on `urlManagerUseCase`. Handles submitting a URL. It checks if the URL was recently crawled (unless `force` is true), pushes it to the crawl queue, marks it as visited, and returns a hash of the URL.\n    - `GetStatus(ctx context.Context, url string) (*entity.CrawlStatus, error)`: Public method on `urlManagerUseCase`. Determines and returns the crawl status of a URL by checking if it's completed (in `extractedDataRepo`), failed/retrying (in `failedURLRepo`), pending (in `visitedRepo`), or not found.\n  - Variables / Constants:\n    - `ErrURLRecentlyCrawled` (error): Public error indicating a URL has been crawled recently.\n    - `deduplicationExpiry` (time.Duration): Constant representing the duration a URL is considered recently visited (48 hours).\n- Notable Patterns or Logic:\n  - Use Case / Service Layer: Encapsulates business logic and orchestrates interactions with multiple data repositories.\n  - Dependency Injection: Repositories are injected into the use case.\n  - Deduplication: Uses `VisitedRepository` to prevent re-queuing recently processed URLs.\n  - Status Aggregation: Combines information from `extracted_data`, `failed_urls`, and `visited` repositories to provide a comprehensive crawl status.\n",
  "pkg/config/config.go": "- File Path: pkg/config/config.go\n- High-Level Purpose: Provides functionality to load and manage application configuration settings from environment variables, with sensible default values.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Config`: Struct holding all application configuration parameters.\n      - Fields: `ServerPort` (string), `LogLevel` (string), `PostgresHost` (string), `PostgresPort` (string), `PostgresUser` (string), `PostgresPassword` (string), `PostgresDB` (string), `RedisAddr` (string), `RedisPassword` (string), `RedisDB` (int), `MaxConcurrency` (int), `PageLoadTimeout` (time.Duration).\n  - Functions / Methods:\n    - `Load() *Config`: Public function that loads configuration values from environment variables (e.g., `SERVER_PORT`, `POSTGRES_HOST`) and returns a `Config` struct. Provides default values if environment variables are not set.\n    - `getEnv(key, fallback string) string`: Internal helper function to retrieve an environment variable's string value or a fallback.\n    - `getEnvAsInt(key string, fallback int) int`: Internal helper function to retrieve an environment variable's integer value or a fallback.\n    - `getEnvAsDuration(key string, fallback int) time.Duration`: Internal helper function to retrieve an environment variable's integer value and convert it to a `time.Duration` (assuming seconds), or a fallback.\n",
  "pkg/logger/logger.go": "- File Path: pkg/logger/logger.go\n- High-Level Purpose: Provides a utility function to initialize and configure the global structured logger (`slog`).\n- Definitions in the File:\n  - Functions / Methods:\n    - `Init(writer io.Writer, level slog.Level)`: Public function that initializes the default `slog` logger. It configures a JSON handler, sets the logging level, and customizes attribute keys (e.g., `time` to `timestamp`, `level` to `level`, `msg` to `message`).\n",
  "pkg/metrics/metrics.go": "- File Path: pkg/metrics/metrics.go\n- High-Level Purpose: Defines and initializes global Prometheus metrics used throughout the `crawler-service` application for monitoring HTTP requests and crawl operations.\n- Definitions in the File:\n  - Variables / Constants:\n    - `HTTPRequestsTotal` (*prometheus.CounterVec): Global variable, a Prometheus counter vector that tracks the total number of HTTP requests, labeled by method, path, and status code.\n    - `HTTPRequestDuration` (*prometheus.HistogramVec): Global variable, a Prometheus histogram vector that measures the duration of HTTP requests, labeled by method, path, and status code.\n    - `URLsInQueue` (prometheus.Gauge): Global variable, a Prometheus gauge that tracks the current number of URLs awaiting processing in the crawl queue.\n    - `CrawlsTotal` (*prometheus.CounterVec): Global variable, a Prometheus counter vector that tracks the total number of crawl attempts, labeled by status (\"success\", \"failure\") and error type.\n    - `CrawlDuration` (*prometheus.HistogramVec): Global variable, a Prometheus histogram vector that measures the duration of crawl operations, labeled by the domain being crawled.\n  - Functions / Methods:\n    - `Init()`: Public function. Initializes and registers all the global Prometheus metrics (counters, histograms, and gauges) using `promauto` for automatic registration with the default Prometheus registry.\n",
  "pkg/utils/url.go": "- File Path: pkg/utils/url.go\n- High-Level Purpose: Contains utility functions for URL manipulation, specifically for hashing URLs and converting relative URLs to absolute ones.\n- Definitions in the File:\n  - Functions / Methods:\n    - `HashURL(rawURL string) string`: Public function that generates a SHA256 hash of a given URL string, useful for creating consistent keys.\n    - `ToAbsoluteURL(base *url.URL, relative string) (string, error)`: Public function that converts a relative URL string into an absolute URL string, given a base URL.\n",
  "schema/001_init.sql": "- File Path: schema/001_init.sql\n- High-Level Purpose: Defines the initial database schema for the `crawler-service`, including tables for storing extracted web data and tracking failed URL attempts.\n- Definitions in the File:\n  - Tables:\n    - `extracted_data`: Stores detailed information extracted from crawled web pages.\n      - Fields: `id` (BIGSERIAL PK), `url` (TEXT UNIQUE), `title` (TEXT), `description` (TEXT), `keywords` (TEXT[]), `h1_tags` (TEXT[]), `content` (TEXT), `images` (JSONB), `crawl_timestamp` (TIMESTAMPTZ DEFAULT NOW()), `http_status_code` (INT), `response_time_ms` (INT).\n    - `failed_urls`: Stores information about URLs that failed processing, including reasons and retry attempts.\n      - Fields: `id` (BIGSERIAL PK), `url` (TEXT UNIQUE), `failure_reason` (TEXT), `http_status_code` (INT), `last_attempt_timestamp` (TIMESTAMPTZ DEFAULT NOW()), `retry_count` (INT DEFAULT 0), `next_retry_at` (TIMESTAMPTZ).\n  - Indexes:\n    - `idx_extracted_data_url` on `extracted_data(url)`.\n    - `idx_extracted_data_crawl_timestamp` on `extracted_data(crawl_timestamp)`.\n    - `idx_failed_urls_url` on `failed_urls(url)`.\n    - `idx_failed_urls_next_retry_at` on `failed_urls(next_retry_at)`.\n"
}