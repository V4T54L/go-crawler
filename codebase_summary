{
  "Dockerfile": "- File Path: Dockerfile\n- High-Level Purpose: Provides instructions for building a multi-stage Docker image for the `crawler-service` Go application.\n- Definitions in the File:\n  - Variables / Constants:\n    - `builder` (alias): Name for the first build stage using `golang:1.21-alpine`.\n    - `WORKDIR /app`: Sets the working directory inside the builder container.\n    - `CGO_ENABLED=0 GOOS=linux`: Environment variables for cross-compilation.\n    - `WORKDIR /root/`: Sets the working directory in the final `alpine:latest` image.\n    - `EXPOSE 8080`: Declares that the container listens on port 8080.\n  - Notable Patterns or Logic:\n    - Multi-stage build: Separates the build environment from the final runtime environment to create a smaller image.\n",
  "Makefile": "- File Path: Makefile\n- High-Level Purpose: Defines common build, run, and dependency management commands for the Go application.\n- Definitions in the File:\n  - Functions / Methods:\n    - `build`: Builds the Go application binary `crawler-service` and places it in `./bin`.\n    - `run`: Executes the compiled `crawler-service` binary.\n    - `tidy`: Runs `go mod tidy` to clean up Go module dependencies.\n",
  "cmd/api/main.go": "- File Path: cmd/api/main.go\n- High-Level Purpose: The main entry point for the `crawler-service` API application.\n- Definitions in the File:\n  - Functions / Methods:\n    - `main()`: Internal function that serves as the application's entry point. It initializes the global logger and logs a starting message. It also contains a `TODO` for further service initialization.\n",
  "go.mod": "- File Path: go.mod\n- High-Level Purpose: Defines the Go module path and the required Go version for the project.\n- Definitions in the File:\n  - Variables / Constants:\n    - `module github.com/user/crawler-service`: The module path for the project.\n    - `go 1.21`: Specifies the minimum Go version required.\n",
  "internal/entity/extracted_data.go": "- File Path: internal/entity/extracted_data.go\n- High-Level Purpose: Defines Go data structures (structs) that represent the entities related to extracted web page content, mirroring the `extracted_data` database table.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ImageInfo`: Struct representing details of an image found on a page.\n      - Fields: `Src` (string), `Alt` (string), `DataSrc` (string, optional for lazy-loaded images).\n    - `ExtractedData`: Struct representing a complete record of data extracted from a crawled URL.\n      - Fields: `ID` (int64), `URL` (string), `Title` (string), `Description` (string), `Keywords` ([]string), `H1Tags` ([]string), `Content` (string), `Images` ([]ImageInfo), `CrawlTimestamp` (time.Time), `HTTPStatusCode` (int), `ResponseTimeMS` (int).\n",
  "internal/entity/failed_url.go": "- File Path: internal/entity/failed_url.go\n- High-Level Purpose: Defines a Go data structure (struct) that represents the entity for URLs that failed processing, mirroring the `failed_urls` database table.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `FailedURL`: Struct representing a record of a URL that failed to be processed.\n      - Fields: `ID` (int64), `URL` (string), `FailureReason` (string), `HTTPStatusCode` (int), `LastAttemptTimestamp` (time.Time), `RetryCount` (int), `NextRetryAt` (time.Time).",
  "pkg/config/config.go": "- File Path: pkg/config/config.go\n- High-Level Purpose: Provides functionality to load and manage application configuration settings from environment variables, with sensible default values.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Config`: Struct holding all application configuration parameters.\n      - Fields: `ServerPort` (string), `LogLevel` (string), `PostgresHost` (string), `PostgresPort` (string), `PostgresUser` (string), `PostgresPassword` (string), `PostgresDB` (string), `RedisAddr` (string), `RedisPassword` (string), `RedisDB` (int), `MaxConcurrency` (int), `PageLoadTimeout` (time.Duration).\n  - Functions / Methods:\n    - `Load() *Config`: Public function that loads configuration values from environment variables (e.g., `SERVER_PORT`, `POSTGRES_HOST`) and returns a `Config` struct. Provides default values if environment variables are not set.\n    - `getEnv(key, fallback string) string`: Internal helper function to retrieve an environment variable's string value or a fallback.\n    - `getEnvAsInt(key string, fallback int) int`: Internal helper function to retrieve an environment variable's integer value or a fallback.\n    - `getEnvAsDuration(key string, fallback int) time.Duration`: Internal helper function to retrieve an environment variable's integer value and convert it to a `time.Duration` (assuming seconds), or a fallback.\n",
  "pkg/logger/logger.go": "- File Path: pkg/logger/logger.go\n- High-Level Purpose: Provides a utility function to initialize and configure the global structured logger (`slog`).\n- Definitions in the File:\n  - Functions / Methods:\n    - `Init(writer io.Writer, level slog.Level)`: Public function that initializes the default `slog` logger. It configures a JSON handler, sets the logging level, and customizes attribute keys (e.g., `time` to `timestamp`, `level` to `level`, `msg` to `message`).\n",
  "pkg/utils/url.go": "- File Path: pkg/utils/url.go\n- High-Level Purpose: Contains utility functions for URL manipulation, specifically for hashing URLs and converting relative URLs to absolute ones.\n- Definitions in the File:\n  - Functions / Methods:\n    - `HashURL(rawURL string) string`: Public function that generates a SHA256 hash of a given URL string, useful for creating consistent keys.\n    - `ToAbsoluteURL(base *url.URL, relative string) (string, error)`: Public function that converts a relative URL string into an absolute URL string, given a base URL.\n",
  "schema/001_init.sql": "- File Path: schema/001_init.sql\n- High-Level Purpose: Defines the initial database schema for the `crawler-service`, including tables for storing extracted web data and tracking failed URL attempts.\n- Definitions in the File:\n  - Tables:\n    - `extracted_data`: Stores detailed information extracted from crawled web pages.\n      - Fields: `id` (BIGSERIAL PK), `url` (TEXT UNIQUE), `title` (TEXT), `description` (TEXT), `keywords` (TEXT[]), `h1_tags` (TEXT[]), `content` (TEXT), `images` (JSONB), `crawl_timestamp` (TIMESTAMPTZ DEFAULT NOW()), `http_status_code` (INT), `response_time_ms` (INT).\n    - `failed_urls`: Stores information about URLs that failed processing, including reasons and retry attempts.\n      - Fields: `id` (BIGSERIAL PK), `url` (TEXT UNIQUE), `failure_reason` (TEXT), `http_status_code` (INT), `last_attempt_timestamp` (TIMESTAMPTZ DEFAULT NOW()), `retry_count` (INT DEFAULT 0), `next_retry_at` (TIMESTAMPTZ).\n  - Indexes:\n    - `idx_extracted_data_url` on `extracted_data(url)`.\n    - `idx_extracted_data_crawl_timestamp` on `extracted_data(crawl_timestamp)`.\n    - `idx_failed_urls_url` on `failed_urls(url)`.\n    - `idx_failed_urls_next_retry_at` on `failed_urls(next_retry_at)`.\n"
}